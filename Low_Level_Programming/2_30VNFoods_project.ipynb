{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf253ab",
   "metadata": {},
   "source": [
    "# 2-1. 프로젝트: 베트남 음식 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50489533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543673b",
   "metadata": {},
   "source": [
    "손상된 이미지 파일들을 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5863c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training set과 test set의 모든 이미지 파일에 대해서,\n",
    "# jpg image header가 포함되지 않은 (jpg의 파일 구조에 어긋나는) 파일들을 삭제해줍니다.\n",
    "\n",
    "data_path = '/aiffel/aiffel/model-fit/data/30vnfoods/'\n",
    "train_path = data_path + 'Train/'\n",
    "test_path = data_path + 'Test/'\n",
    "\n",
    "for path in [train_path, test_path]:\n",
    "    classes = os.listdir(path)\n",
    "\n",
    "    for food in classes:\n",
    "        food_path = os.path.join(path, food)\n",
    "        images = os.listdir(food_path)\n",
    "        \n",
    "        for image in images:\n",
    "            with open(os.path.join(food_path, image), 'rb') as f:\n",
    "                bytes = f.read()\n",
    "            if bytes[:3] != b'\\xff\\xd8\\xff':\n",
    "                print(os.path.join(food_path, image))\n",
    "                os.remove(os.path.join(food_path, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18119328",
   "metadata": {},
   "source": [
    "삭제되고 난 후의 훈련 데이터 수는 아래와 같이 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051276ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data의 개수: 9775\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(train_path)\n",
    "train_length = 0\n",
    "\n",
    "for food in classes:\n",
    "    food_path = os.path.join(train_path, food)\n",
    "    images = os.listdir(food_path)\n",
    "    \n",
    "    train_length += len(images)\n",
    "\n",
    "print('training data의 개수: '+str(train_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb9621",
   "metadata": {},
   "source": [
    "## 문제1: dataloader 구현하기\n",
    ": 주어진 함수들과 TensorFlow Data API를 이용해서 dataloader를 구현해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0345a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제1: dataloader 구현하기\n",
    "\n",
    "def process_path(file_path, class_names, img_shape=(224, 224)):\n",
    "    '''\n",
    "    file_path로 부터 class label을 만들고, 이미지를 읽는 함수\n",
    "    이미지 크기를 (224, 224)로 맞춰주세요.\n",
    "    '''\n",
    "    label = tf.strings.split(file_path, os.path.sep)\n",
    "    label = label[-2] == class_names\n",
    "\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, img_shape)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "def prepare_for_training(ds, batch_size=32, cache=True, shuffle_buffer_size=1000, n_repeat=1):\n",
    "    '''\n",
    "    TensorFlow Data API를 이용해 data batch를 만드는 함수\n",
    "    '''\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat(n_repeat)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def load_data(data_path, batch_size=32):\n",
    "    '''\n",
    "    데이터를 만들기 위해 필요한 함수들을 호출하고 데이터를 리턴해주는 함수\n",
    "    TensorFlow Dataset 객체를 생성하고 process_path 함수로 이미지와 라벨을 묶은 다음,\n",
    "    prepare_for_training 함수로 batch가 적용된 Dataset 객체를 만들어주세요.\n",
    "    '''\n",
    "    class_names = [cls for cls in os.listdir(data_path) if cls != '.DS_Store']\n",
    "    data_path = pathlib.Path(data_path)\n",
    "\n",
    "    list_ds = tf.data.Dataset.list_files(str(data_path/'*/*'))\n",
    "    labeled_ds = list_ds.map(lambda x: process_path(x, class_names, img_shape=(224, 224)))\n",
    "    ds = prepare_for_training(labeled_ds, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a011bd1",
   "metadata": {},
   "source": [
    "## 문제2: 모델 구현하기\n",
    "- EfficientNetB0 backbone과 Dense 레이어를 결합하여 모델을 구현해야 합니다.\n",
    "\n",
    "- 다른 모델을 사용하여 backbone을 만들어 보아도 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f35546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 1280)              5120      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 multiple                  12810     \n",
      "=================================================================\n",
      "Total params: 4,067,501\n",
      "Trainable params: 15,370\n",
      "Non-trainable params: 4,052,131\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 문제2: 모델 구현하기\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    '''\n",
    "    EfficientNetB0을 백본으로 사용하는 모델을 구성합니다.\n",
    "    Classification 문제로 접근할 것이기 때문에 맨 마지막 Dense 레이어에 \n",
    "    우리가 원하는 클래스 개수만큼을 지정해주어야 합니다.\n",
    "    '''\n",
    "    def __init__(self, num_classes=10, freeze=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "        if freeze:\n",
    "            self.base_model.trainable = False\n",
    "        self.top = tf.keras.Sequential([tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\"),\n",
    "                                       tf.keras.layers.BatchNormalization(),\n",
    "                                       tf.keras.layers.Dropout(0.5, name=\"top_dropout\")])\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.top(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    model = Model(num_classes=10, freeze=True)\n",
    "    model.build(input_shape=(None, 224, 224, 3))\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549376a",
   "metadata": {},
   "source": [
    "## 문제3: custom trainer 구현하기\n",
    "- TensorFlow GradientTape를 이용해서 custom trainer 클래스를 구현해야 합니다.\n",
    "- 학습이 진행되면서 training accuracy가 점차 증가할 수 있도록 만들어 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8acfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제3: custom trainer 구현하기\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, epochs, batch, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "    def train(self, train_dataset, train_metric):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "            # [[YOUR CODE]]\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x_batch_train, training=True)\n",
    "                    loss_value = self.loss_fn(y_batch_train, logits)\n",
    "                grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                # train metric 업데이트\n",
    "                train_metric.update_state(y_batch_train, logits)\n",
    "                # 5 배치마다 로깅\n",
    "                if step % 5 == 0:\n",
    "                    print(\n",
    "                        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                        % (step, float(loss_value))\n",
    "                    )\n",
    "                    print(\"Seen so far: %d samples\" % ((step + 1) * self.batch))\n",
    "                    print(train_metric.result().numpy())            \n",
    "            # 마지막 epoch 학습이 끝나면 train 결과를 보여줌\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa29fed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 4.0263\n",
      "Seen so far: 32 samples\n",
      "0.0625\n",
      "Training loss (for one batch) at step 5: 2.6789\n",
      "Seen so far: 192 samples\n",
      "0.14583333\n",
      "Training loss (for one batch) at step 10: 2.3200\n",
      "Seen so far: 352 samples\n",
      "0.23579545\n",
      "Training loss (for one batch) at step 15: 1.4417\n",
      "Seen so far: 512 samples\n",
      "0.29492188\n",
      "Training loss (for one batch) at step 20: 1.5462\n",
      "Seen so far: 672 samples\n",
      "0.36011904\n",
      "Training loss (for one batch) at step 25: 0.8715\n",
      "Seen so far: 832 samples\n",
      "0.40144232\n",
      "Training loss (for one batch) at step 30: 1.3830\n",
      "Seen so far: 992 samples\n",
      "0.4375\n",
      "Training loss (for one batch) at step 35: 1.0832\n",
      "Seen so far: 1152 samples\n",
      "0.47743055\n",
      "Training loss (for one batch) at step 40: 1.4412\n",
      "Seen so far: 1312 samples\n",
      "0.49618903\n",
      "Training loss (for one batch) at step 45: 0.7839\n",
      "Seen so far: 1472 samples\n",
      "0.515625\n",
      "Training loss (for one batch) at step 50: 1.2562\n",
      "Seen so far: 1632 samples\n",
      "0.5275735\n",
      "Training loss (for one batch) at step 55: 0.9102\n",
      "Seen so far: 1792 samples\n",
      "0.5401786\n",
      "Training loss (for one batch) at step 60: 1.6326\n",
      "Seen so far: 1952 samples\n",
      "0.548668\n",
      "Training loss (for one batch) at step 65: 0.9925\n",
      "Seen so far: 2112 samples\n",
      "0.5568182\n",
      "Training loss (for one batch) at step 70: 0.5300\n",
      "Seen so far: 2272 samples\n",
      "0.5682218\n",
      "Training loss (for one batch) at step 75: 0.6636\n",
      "Seen so far: 2432 samples\n",
      "0.5740132\n",
      "Training loss (for one batch) at step 80: 1.1082\n",
      "Seen so far: 2592 samples\n",
      "0.58487654\n",
      "Training loss (for one batch) at step 85: 0.8110\n",
      "Seen so far: 2752 samples\n",
      "0.58938956\n",
      "Training loss (for one batch) at step 90: 0.7316\n",
      "Seen so far: 2912 samples\n",
      "0.59649724\n",
      "Training loss (for one batch) at step 95: 0.6075\n",
      "Seen so far: 3072 samples\n",
      "0.6041667\n",
      "Training loss (for one batch) at step 100: 0.7837\n",
      "Seen so far: 3232 samples\n",
      "0.61262375\n",
      "Training loss (for one batch) at step 105: 1.2102\n",
      "Seen so far: 3392 samples\n",
      "0.618809\n",
      "Training loss (for one batch) at step 110: 1.1356\n",
      "Seen so far: 3552 samples\n",
      "0.6219032\n",
      "Training loss (for one batch) at step 115: 1.0822\n",
      "Seen so far: 3712 samples\n",
      "0.626347\n",
      "Training loss (for one batch) at step 120: 0.8028\n",
      "Seen so far: 3872 samples\n",
      "0.6322314\n",
      "Training loss (for one batch) at step 125: 0.7340\n",
      "Seen so far: 4032 samples\n",
      "0.6356647\n",
      "Training loss (for one batch) at step 130: 1.0209\n",
      "Seen so far: 4192 samples\n",
      "0.64002866\n",
      "Training loss (for one batch) at step 135: 0.2838\n",
      "Seen so far: 4352 samples\n",
      "0.64269304\n",
      "Training loss (for one batch) at step 140: 1.4886\n",
      "Seen so far: 4512 samples\n",
      "0.6469415\n",
      "Training loss (for one batch) at step 145: 0.5331\n",
      "Seen so far: 4672 samples\n",
      "0.65496576\n",
      "Training loss (for one batch) at step 150: 1.1190\n",
      "Seen so far: 4832 samples\n",
      "0.6585265\n",
      "Training loss (for one batch) at step 155: 0.4866\n",
      "Seen so far: 4992 samples\n",
      "0.66426283\n",
      "Training loss (for one batch) at step 160: 0.9524\n",
      "Seen so far: 5152 samples\n",
      "0.6686724\n",
      "Training loss (for one batch) at step 165: 0.7420\n",
      "Seen so far: 5312 samples\n",
      "0.67338103\n",
      "Training loss (for one batch) at step 170: 0.4589\n",
      "Seen so far: 5472 samples\n",
      "0.6763523\n",
      "Training loss (for one batch) at step 175: 0.4495\n",
      "Seen so far: 5632 samples\n",
      "0.6807528\n",
      "Training loss (for one batch) at step 180: 0.5482\n",
      "Seen so far: 5792 samples\n",
      "0.68370163\n",
      "Training loss (for one batch) at step 185: 0.4774\n",
      "Seen so far: 5952 samples\n",
      "0.6875\n",
      "Training loss (for one batch) at step 190: 0.5722\n",
      "Seen so far: 6112 samples\n",
      "0.69077224\n",
      "Training loss (for one batch) at step 195: 0.5207\n",
      "Seen so far: 6272 samples\n",
      "0.6948342\n",
      "Training loss (for one batch) at step 200: 0.6520\n",
      "Seen so far: 6432 samples\n",
      "0.6979167\n",
      "Training loss (for one batch) at step 205: 0.2952\n",
      "Seen so far: 6592 samples\n",
      "0.7020631\n",
      "Training loss (for one batch) at step 210: 0.4044\n",
      "Seen so far: 6752 samples\n",
      "0.7051244\n",
      "Training loss (for one batch) at step 215: 0.3387\n",
      "Seen so far: 6912 samples\n",
      "0.7074653\n",
      "Training loss (for one batch) at step 220: 0.4903\n",
      "Seen so far: 7072 samples\n",
      "0.71069\n",
      "Training loss (for one batch) at step 225: 0.2379\n",
      "Seen so far: 7232 samples\n",
      "0.7140487\n",
      "Training loss (for one batch) at step 230: 0.4781\n",
      "Seen so far: 7392 samples\n",
      "0.7173972\n",
      "Training loss (for one batch) at step 235: 0.3991\n",
      "Seen so far: 7552 samples\n",
      "0.7204714\n",
      "Training loss (for one batch) at step 240: 0.4547\n",
      "Seen so far: 7712 samples\n",
      "0.7231587\n",
      "Training loss (for one batch) at step 245: 0.5068\n",
      "Seen so far: 7872 samples\n",
      "0.7257368\n",
      "Training loss (for one batch) at step 250: 0.4730\n",
      "Seen so far: 8032 samples\n",
      "0.72758967\n",
      "Training loss (for one batch) at step 255: 0.4288\n",
      "Seen so far: 8192 samples\n",
      "0.7285156\n",
      "Training loss (for one batch) at step 260: 0.5596\n",
      "Seen so far: 8352 samples\n",
      "0.73096263\n",
      "Training loss (for one batch) at step 265: 0.8197\n",
      "Seen so far: 8512 samples\n",
      "0.7319079\n",
      "Training loss (for one batch) at step 270: 0.6017\n",
      "Seen so far: 8672 samples\n",
      "0.7338561\n",
      "Training loss (for one batch) at step 275: 0.8948\n",
      "Seen so far: 8832 samples\n",
      "0.7348279\n",
      "Training loss (for one batch) at step 280: 0.5777\n",
      "Seen so far: 8992 samples\n",
      "0.7365436\n",
      "Training loss (for one batch) at step 285: 0.6433\n",
      "Seen so far: 9152 samples\n",
      "0.73765296\n",
      "Training loss (for one batch) at step 290: 0.3925\n",
      "Seen so far: 9312 samples\n",
      "0.7390464\n",
      "Training loss (for one batch) at step 295: 0.6635\n",
      "Seen so far: 9472 samples\n",
      "0.7404983\n",
      "Training loss (for one batch) at step 300: 0.7574\n",
      "Seen so far: 9632 samples\n",
      "0.741902\n",
      "Training loss (for one batch) at step 305: 0.1972\n",
      "Seen so far: 9792 samples\n",
      "0.7436683\n",
      "Training loss (for one batch) at step 310: 0.3936\n",
      "Seen so far: 9952 samples\n",
      "0.74588025\n",
      "Training loss (for one batch) at step 315: 0.4813\n",
      "Seen so far: 10112 samples\n",
      "0.7471321\n",
      "Training loss (for one batch) at step 320: 0.2643\n",
      "Seen so far: 10272 samples\n",
      "0.7496106\n",
      "Training loss (for one batch) at step 325: 0.3385\n",
      "Seen so far: 10432 samples\n",
      "0.7508627\n",
      "Training loss (for one batch) at step 330: 0.8287\n",
      "Seen so far: 10592 samples\n",
      "0.75273794\n",
      "Training loss (for one batch) at step 335: 0.5682\n",
      "Seen so far: 10752 samples\n",
      "0.75381327\n",
      "Training loss (for one batch) at step 340: 0.3809\n",
      "Seen so far: 10912 samples\n",
      "0.7555902\n",
      "Training loss (for one batch) at step 345: 0.4818\n",
      "Seen so far: 11072 samples\n",
      "0.7574964\n",
      "Training loss (for one batch) at step 350: 0.6491\n",
      "Seen so far: 11232 samples\n",
      "0.75890315\n",
      "Training loss (for one batch) at step 355: 0.5466\n",
      "Seen so far: 11392 samples\n",
      "0.76044595\n",
      "Training loss (for one batch) at step 360: 0.2068\n",
      "Seen so far: 11552 samples\n",
      "0.7618594\n",
      "Training loss (for one batch) at step 365: 0.2800\n",
      "Seen so far: 11712 samples\n",
      "0.76306355\n",
      "Training loss (for one batch) at step 370: 0.8709\n",
      "Seen so far: 11872 samples\n",
      "0.7634771\n",
      "Training loss (for one batch) at step 375: 0.5994\n",
      "Seen so far: 12032 samples\n",
      "0.76462764\n",
      "Training loss (for one batch) at step 380: 0.6254\n",
      "Seen so far: 12192 samples\n",
      "0.76541996\n",
      "Training loss (for one batch) at step 385: 0.4177\n",
      "Seen so far: 12352 samples\n",
      "0.7661917\n",
      "Training loss (for one batch) at step 390: 0.4586\n",
      "Seen so far: 12512 samples\n",
      "0.76758313\n",
      "Training loss (for one batch) at step 395: 0.4538\n",
      "Seen so far: 12672 samples\n",
      "0.7689394\n",
      "Training loss (for one batch) at step 400: 0.5508\n",
      "Seen so far: 12832 samples\n",
      "0.76948255\n",
      "Training loss (for one batch) at step 405: 0.3477\n",
      "Seen so far: 12992 samples\n",
      "0.7705511\n",
      "Training loss (for one batch) at step 410: 0.2627\n",
      "Seen so far: 13152 samples\n",
      "0.77220196\n",
      "Training loss (for one batch) at step 415: 0.0689\n",
      "Seen so far: 13312 samples\n",
      "0.77358776\n",
      "Training loss (for one batch) at step 420: 0.5197\n",
      "Seen so far: 13472 samples\n",
      "0.7751633\n",
      "Training loss (for one batch) at step 425: 0.3770\n",
      "Seen so far: 13632 samples\n",
      "0.7759683\n",
      "Training loss (for one batch) at step 430: 0.4505\n",
      "Seen so far: 13792 samples\n",
      "0.7771172\n",
      "Training loss (for one batch) at step 435: 0.3148\n",
      "Seen so far: 13952 samples\n",
      "0.77852637\n",
      "Training loss (for one batch) at step 440: 0.3298\n",
      "Seen so far: 14112 samples\n",
      "0.77947843\n",
      "Training loss (for one batch) at step 445: 0.6178\n",
      "Seen so far: 14272 samples\n",
      "0.7806895\n",
      "Training loss (for one batch) at step 450: 0.3005\n",
      "Seen so far: 14432 samples\n",
      "0.7819429\n",
      "Training loss (for one batch) at step 455: 0.3212\n",
      "Seen so far: 14592 samples\n",
      "0.78316885\n",
      "Training loss (for one batch) at step 460: 0.1400\n",
      "Seen so far: 14752 samples\n",
      "0.7843682\n",
      "Training loss (for one batch) at step 465: 0.5908\n",
      "Seen so far: 14912 samples\n",
      "0.7854077\n",
      "Training loss (for one batch) at step 470: 0.3878\n",
      "Seen so far: 15072 samples\n",
      "0.7862261\n",
      "Training loss (for one batch) at step 475: 0.4825\n",
      "Seen so far: 15232 samples\n",
      "0.78735554\n",
      "Training loss (for one batch) at step 480: 0.4249\n",
      "Seen so far: 15392 samples\n",
      "0.7883966\n",
      "Training loss (for one batch) at step 485: 0.1603\n",
      "Seen so far: 15552 samples\n",
      "0.78928757\n",
      "Training loss (for one batch) at step 490: 0.1976\n",
      "Seen so far: 15712 samples\n",
      "0.79035133\n",
      "Training loss (for one batch) at step 495: 0.5613\n",
      "Seen so far: 15872 samples\n",
      "0.7911416\n",
      "Training loss (for one batch) at step 500: 0.1472\n",
      "Seen so far: 16032 samples\n",
      "0.7925399\n",
      "Training loss (for one batch) at step 505: 0.2507\n",
      "Seen so far: 16192 samples\n",
      "0.79366356\n",
      "Training loss (for one batch) at step 510: 0.1971\n",
      "Seen so far: 16352 samples\n",
      "0.7940925\n",
      "Training loss (for one batch) at step 515: 0.3327\n",
      "Seen so far: 16512 samples\n",
      "0.79475534\n",
      "Training loss (for one batch) at step 520: 0.3479\n",
      "Seen so far: 16672 samples\n",
      "0.7959453\n",
      "Training loss (for one batch) at step 525: 1.0686\n",
      "Seen so far: 16832 samples\n",
      "0.7963403\n",
      "Training loss (for one batch) at step 530: 0.6365\n",
      "Seen so far: 16992 samples\n",
      "0.7969633\n",
      "Training loss (for one batch) at step 535: 0.1430\n",
      "Seen so far: 17152 samples\n",
      "0.79792446\n",
      "Training loss (for one batch) at step 540: 0.4166\n",
      "Seen so far: 17312 samples\n",
      "0.7988678\n",
      "Training loss (for one batch) at step 545: 0.4018\n",
      "Seen so far: 17472 samples\n",
      "0.7998512\n",
      "Training loss (for one batch) at step 550: 0.5600\n",
      "Seen so far: 17632 samples\n",
      "0.8000794\n",
      "Training loss (for one batch) at step 555: 0.4410\n",
      "Seen so far: 17792 samples\n",
      "0.80097795\n",
      "Training loss (for one batch) at step 560: 0.1882\n",
      "Seen so far: 17952 samples\n",
      "0.8013592\n",
      "Training loss (for one batch) at step 565: 0.3183\n",
      "Seen so far: 18112 samples\n",
      "0.80223054\n",
      "Training loss (for one batch) at step 570: 0.6961\n",
      "Seen so far: 18272 samples\n",
      "0.80286777\n",
      "Training loss (for one batch) at step 575: 0.4761\n",
      "Seen so far: 18432 samples\n",
      "0.8034397\n",
      "Training loss (for one batch) at step 580: 0.2824\n",
      "Seen so far: 18592 samples\n",
      "0.80384034\n",
      "Training loss (for one batch) at step 585: 1.0544\n",
      "Seen so far: 18752 samples\n",
      "0.8045009\n",
      "Training loss (for one batch) at step 590: 0.1376\n",
      "Seen so far: 18912 samples\n",
      "0.80541456\n",
      "Training loss (for one batch) at step 595: 0.8802\n",
      "Seen so far: 19072 samples\n",
      "0.80568373\n",
      "Training loss (for one batch) at step 600: 0.4306\n",
      "Seen so far: 19232 samples\n",
      "0.8063644\n",
      "Training loss (for one batch) at step 605: 0.4482\n",
      "Seen so far: 19392 samples\n",
      "0.80703384\n",
      "Training loss (for one batch) at step 610: 0.2036\n",
      "Seen so far: 19552 samples\n",
      "0.80764115\n",
      "Training loss (for one batch) at step 615: 0.3132\n",
      "Seen so far: 19712 samples\n",
      "0.80839086\n",
      "Training loss (for one batch) at step 620: 0.4593\n",
      "Seen so far: 19872 samples\n",
      "0.80872583\n",
      "Training loss (for one batch) at step 625: 0.4547\n",
      "Seen so far: 20032 samples\n",
      "0.8090056\n",
      "Training loss (for one batch) at step 630: 0.1681\n",
      "Seen so far: 20192 samples\n",
      "0.80982566\n",
      "Training loss (for one batch) at step 635: 0.3042\n",
      "Seen so far: 20352 samples\n",
      "0.8105837\n",
      "Training loss (for one batch) at step 640: 0.8119\n",
      "Seen so far: 20512 samples\n",
      "0.81113493\n",
      "Training loss (for one batch) at step 645: 0.1676\n",
      "Seen so far: 20672 samples\n",
      "0.8117744\n",
      "Training loss (for one batch) at step 650: 0.4291\n",
      "Seen so far: 20832 samples\n",
      "0.81226\n",
      "Training loss (for one batch) at step 655: 0.4921\n",
      "Seen so far: 20992 samples\n",
      "0.81297636\n",
      "Training loss (for one batch) at step 660: 0.1759\n",
      "Seen so far: 21152 samples\n",
      "0.81382376\n",
      "Training loss (for one batch) at step 665: 0.2433\n",
      "Seen so far: 21312 samples\n",
      "0.8143769\n",
      "Training loss (for one batch) at step 670: 0.3818\n",
      "Seen so far: 21472 samples\n",
      "0.81510806\n",
      "Training loss (for one batch) at step 675: 0.2720\n",
      "Seen so far: 21632 samples\n",
      "0.81573594\n",
      "Training loss (for one batch) at step 680: 0.3644\n",
      "Seen so far: 21792 samples\n",
      "0.81621695\n",
      "Training loss (for one batch) at step 685: 0.2004\n",
      "Seen so far: 21952 samples\n",
      "0.816691\n",
      "Training loss (for one batch) at step 690: 0.5876\n",
      "Seen so far: 22112 samples\n",
      "0.8169772\n",
      "Training loss (for one batch) at step 695: 0.4832\n",
      "Seen so far: 22272 samples\n",
      "0.8167654\n",
      "Training loss (for one batch) at step 700: 1.2354\n",
      "Seen so far: 22432 samples\n",
      "0.81709164\n",
      "Training loss (for one batch) at step 705: 0.5526\n",
      "Seen so far: 22592 samples\n",
      "0.8170149\n",
      "Training loss (for one batch) at step 710: 0.5093\n",
      "Seen so far: 22752 samples\n",
      "0.8173787\n",
      "Training loss (for one batch) at step 715: 0.3886\n",
      "Seen so far: 22912 samples\n",
      "0.81786835\n",
      "Training loss (for one batch) at step 720: 0.2962\n",
      "Seen so far: 23072 samples\n",
      "0.8181779\n",
      "Training loss (for one batch) at step 725: 0.1650\n",
      "Seen so far: 23232 samples\n",
      "0.8187414\n",
      "Training loss (for one batch) at step 730: 0.5217\n",
      "Seen so far: 23392 samples\n",
      "0.81908345\n",
      "Training loss (for one batch) at step 735: 0.3383\n",
      "Seen so far: 23552 samples\n",
      "0.8194633\n",
      "Training loss (for one batch) at step 740: 0.2601\n",
      "Seen so far: 23712 samples\n",
      "0.8199646\n",
      "Training loss (for one batch) at step 745: 0.2204\n",
      "Seen so far: 23872 samples\n",
      "0.8203334\n",
      "Training loss (for one batch) at step 750: 0.1581\n",
      "Seen so far: 24032 samples\n",
      "0.8210303\n",
      "Training loss (for one batch) at step 755: 0.3650\n",
      "Seen so far: 24192 samples\n",
      "0.82138723\n",
      "Training loss (for one batch) at step 760: 0.2155\n",
      "Seen so far: 24352 samples\n",
      "0.82202697\n",
      "Training loss (for one batch) at step 765: 0.2196\n",
      "Seen so far: 24512 samples\n",
      "0.8225767\n",
      "Training loss (for one batch) at step 770: 0.7114\n",
      "Seen so far: 24672 samples\n",
      "0.8225519\n",
      "Training loss (for one batch) at step 775: 0.1212\n",
      "Seen so far: 24832 samples\n",
      "0.822769\n",
      "Training loss (for one batch) at step 780: 0.0325\n",
      "Seen so far: 24992 samples\n",
      "0.8230634\n",
      "Training loss (for one batch) at step 785: 0.6134\n",
      "Seen so far: 25152 samples\n",
      "0.82339376\n",
      "Training loss (for one batch) at step 790: 0.3287\n",
      "Seen so far: 25312 samples\n",
      "0.823878\n",
      "Training loss (for one batch) at step 795: 0.4142\n",
      "Seen so far: 25472 samples\n",
      "0.82423836\n",
      "Training loss (for one batch) at step 800: 0.2642\n",
      "Seen so far: 25632 samples\n",
      "0.8249844\n",
      "Training loss (for one batch) at step 805: 0.6404\n",
      "Seen so far: 25792 samples\n",
      "0.8253722\n",
      "Training loss (for one batch) at step 810: 0.2361\n",
      "Seen so far: 25952 samples\n",
      "0.8259094\n",
      "Training loss (for one batch) at step 815: 0.1551\n",
      "Seen so far: 26112 samples\n",
      "0.82655483\n",
      "Training loss (for one batch) at step 820: 0.1865\n",
      "Seen so far: 26272 samples\n",
      "0.8270783\n",
      "Training loss (for one batch) at step 825: 0.5155\n",
      "Seen so far: 26432 samples\n",
      "0.82748187\n",
      "Training loss (for one batch) at step 830: 0.1545\n",
      "Seen so far: 26592 samples\n",
      "0.82769257\n",
      "Training loss (for one batch) at step 835: 0.1310\n",
      "Seen so far: 26752 samples\n",
      "0.828125\n",
      "Training loss (for one batch) at step 840: 0.5410\n",
      "Seen so far: 26912 samples\n",
      "0.828478\n",
      "Training loss (for one batch) at step 845: 0.3207\n",
      "Seen so far: 27072 samples\n",
      "0.8288638\n",
      "Training loss (for one batch) at step 850: 0.3994\n",
      "Seen so far: 27232 samples\n",
      "0.8293552\n",
      "Training loss (for one batch) at step 855: 0.2602\n",
      "Seen so far: 27392 samples\n",
      "0.8296948\n",
      "Training loss (for one batch) at step 860: 0.6693\n",
      "Seen so far: 27552 samples\n",
      "0.8301031\n",
      "Training loss (for one batch) at step 865: 0.5699\n",
      "Seen so far: 27712 samples\n",
      "0.8303623\n",
      "Training loss (for one batch) at step 870: 0.2660\n",
      "Seen so far: 27872 samples\n",
      "0.830762\n",
      "Training loss (for one batch) at step 875: 0.1704\n",
      "Seen so far: 28032 samples\n",
      "0.8310859\n",
      "Training loss (for one batch) at step 880: 0.2355\n",
      "Seen so far: 28192 samples\n",
      "0.83140606\n",
      "Training loss (for one batch) at step 885: 0.2896\n",
      "Seen so far: 28352 samples\n",
      "0.8316521\n",
      "Training loss (for one batch) at step 890: 0.3126\n",
      "Seen so far: 28512 samples\n",
      "0.8320707\n",
      "Training loss (for one batch) at step 895: 0.3541\n",
      "Seen so far: 28672 samples\n",
      "0.8324498\n",
      "Training loss (for one batch) at step 900: 0.1913\n",
      "Seen so far: 28832 samples\n",
      "0.83278996\n",
      "Training loss (for one batch) at step 905: 0.2300\n",
      "Seen so far: 28992 samples\n",
      "0.8329884\n",
      "Training loss (for one batch) at step 910: 0.2650\n",
      "Seen so far: 29152 samples\n",
      "0.83311605\n",
      "Training loss (for one batch) at step 915: 0.1053\n",
      "Seen so far: 29312 samples\n",
      "0.8336176\n",
      "Training acc over epoch: 0.8337\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.8415\n",
      "Seen so far: 32 samples\n",
      "0.8335661\n",
      "Training loss (for one batch) at step 5: 0.5179\n",
      "Seen so far: 192 samples\n",
      "0.83416337\n",
      "Training loss (for one batch) at step 10: 0.2263\n",
      "Seen so far: 352 samples\n",
      "0.8345183\n",
      "Training loss (for one batch) at step 15: 0.2097\n",
      "Seen so far: 512 samples\n",
      "0.834903\n",
      "Training loss (for one batch) at step 20: 0.0996\n",
      "Seen so far: 672 samples\n",
      "0.83531684\n",
      "Training loss (for one batch) at step 25: 0.0204\n",
      "Seen so far: 832 samples\n",
      "0.8359916\n",
      "Training loss (for one batch) at step 30: 0.2569\n",
      "Seen so far: 992 samples\n",
      "0.8363624\n",
      "Training loss (for one batch) at step 35: 0.1375\n",
      "Seen so far: 1152 samples\n",
      "0.836795\n",
      "Training loss (for one batch) at step 40: 0.2790\n",
      "Seen so far: 1312 samples\n",
      "0.83719033\n",
      "Training loss (for one batch) at step 45: 0.5678\n",
      "Seen so far: 1472 samples\n",
      "0.83767897\n",
      "Training loss (for one batch) at step 50: 0.3038\n",
      "Seen so far: 1632 samples\n",
      "0.8381949\n",
      "Training loss (for one batch) at step 55: 0.4791\n",
      "Seen so far: 1792 samples\n",
      "0.8384806\n",
      "Training loss (for one batch) at step 60: 0.2949\n",
      "Seen so far: 1952 samples\n",
      "0.8388912\n",
      "Training loss (for one batch) at step 65: 0.0876\n",
      "Seen so far: 2112 samples\n",
      "0.8392658\n",
      "Training loss (for one batch) at step 70: 0.6273\n",
      "Seen so far: 2272 samples\n",
      "0.8395734\n",
      "Training loss (for one batch) at step 75: 0.2504\n",
      "Seen so far: 2432 samples\n",
      "0.84003526\n",
      "Training loss (for one batch) at step 80: 0.4887\n",
      "Seen so far: 2592 samples\n",
      "0.84030455\n",
      "Training loss (for one batch) at step 85: 0.3836\n",
      "Seen so far: 2752 samples\n",
      "0.84053993\n",
      "Training loss (for one batch) at step 90: 0.0981\n",
      "Seen so far: 2912 samples\n",
      "0.84095913\n",
      "Training loss (for one batch) at step 95: 0.4494\n",
      "Seen so far: 3072 samples\n",
      "0.8412816\n",
      "Training loss (for one batch) at step 100: 0.1467\n",
      "Seen so far: 3232 samples\n",
      "0.84153944\n",
      "Training loss (for one batch) at step 105: 0.4661\n",
      "Seen so far: 3392 samples\n",
      "0.84191704\n",
      "Training loss (for one batch) at step 110: 0.3397\n",
      "Seen so far: 3552 samples\n",
      "0.8421389\n",
      "Training loss (for one batch) at step 115: 0.4751\n",
      "Seen so far: 3712 samples\n",
      "0.8423586\n",
      "Training loss (for one batch) at step 120: 0.4470\n",
      "Seen so far: 3872 samples\n",
      "0.84260625\n",
      "Training loss (for one batch) at step 125: 0.2321\n",
      "Seen so far: 4032 samples\n",
      "0.8428516\n",
      "Training loss (for one batch) at step 130: 0.2862\n",
      "Seen so far: 4192 samples\n",
      "0.8431542\n",
      "Training loss (for one batch) at step 135: 0.2906\n",
      "Seen so far: 4352 samples\n",
      "0.8434243\n",
      "Training loss (for one batch) at step 140: 0.1332\n",
      "Seen so far: 4512 samples\n",
      "0.84366226\n",
      "Training loss (for one batch) at step 145: 0.1254\n",
      "Seen so far: 4672 samples\n",
      "0.8439274\n",
      "Training loss (for one batch) at step 150: 0.4060\n",
      "Seen so far: 4832 samples\n",
      "0.84407294\n",
      "Training loss (for one batch) at step 155: 0.3227\n",
      "Seen so far: 4992 samples\n",
      "0.8444503\n",
      "Training loss (for one batch) at step 160: 0.3274\n",
      "Seen so far: 5152 samples\n",
      "0.844505\n",
      "Training loss (for one batch) at step 165: 0.2091\n",
      "Seen so far: 5312 samples\n",
      "0.844848\n",
      "Training loss (for one batch) at step 170: 0.3484\n",
      "Seen so far: 5472 samples\n",
      "0.8452453\n",
      "Training loss (for one batch) at step 175: 0.2895\n",
      "Seen so far: 5632 samples\n",
      "0.84563893\n",
      "Training loss (for one batch) at step 180: 0.1903\n",
      "Seen so far: 5792 samples\n",
      "0.8458866\n",
      "Training loss (for one batch) at step 185: 0.0816\n",
      "Seen so far: 5952 samples\n",
      "0.84633046\n",
      "Training loss (for one batch) at step 190: 0.1468\n",
      "Seen so far: 6112 samples\n",
      "0.84665745\n",
      "Training loss (for one batch) at step 195: 0.2315\n",
      "Seen so far: 6272 samples\n",
      "0.8470377\n",
      "Training loss (for one batch) at step 200: 0.4667\n",
      "Seen so far: 6432 samples\n",
      "0.84727466\n",
      "Training loss (for one batch) at step 205: 0.3587\n",
      "Seen so far: 6592 samples\n",
      "0.84748167\n",
      "Training loss (for one batch) at step 210: 0.1792\n",
      "Seen so far: 6752 samples\n",
      "0.8477146\n",
      "Training loss (for one batch) at step 215: 0.5784\n",
      "Seen so far: 6912 samples\n",
      "0.84786266\n",
      "Training loss (for one batch) at step 220: 0.0910\n",
      "Seen so far: 7072 samples\n",
      "0.8480644\n",
      "Training loss (for one batch) at step 225: 0.2024\n",
      "Seen so far: 7232 samples\n",
      "0.8484011\n",
      "Training loss (for one batch) at step 230: 0.2818\n",
      "Seen so far: 7392 samples\n",
      "0.84876215\n",
      "Training loss (for one batch) at step 235: 0.2790\n",
      "Seen so far: 7552 samples\n",
      "0.84912\n",
      "Training loss (for one batch) at step 240: 0.4309\n",
      "Seen so far: 7712 samples\n",
      "0.84931284\n",
      "Training loss (for one batch) at step 245: 0.4129\n",
      "Seen so far: 7872 samples\n",
      "0.8496653\n",
      "Training loss (for one batch) at step 250: 0.3714\n",
      "Seen so far: 8032 samples\n",
      "0.8499344\n",
      "Training loss (for one batch) at step 255: 0.5382\n",
      "Seen so far: 8192 samples\n",
      "0.8503079\n",
      "Training loss (for one batch) at step 260: 0.3349\n",
      "Seen so far: 8352 samples\n",
      "0.85067815\n",
      "Training loss (for one batch) at step 265: 0.2357\n",
      "Seen so far: 8512 samples\n",
      "0.85101885\n",
      "Training loss (for one batch) at step 270: 0.1938\n",
      "Seen so far: 8672 samples\n",
      "0.8512251\n",
      "Training loss (for one batch) at step 275: 0.3471\n",
      "Seen so far: 8832 samples\n",
      "0.8514034\n",
      "Training loss (for one batch) at step 280: 0.3284\n",
      "Seen so far: 8992 samples\n",
      "0.8518151\n",
      "Training loss (for one batch) at step 285: 0.4632\n",
      "Seen so far: 9152 samples\n",
      "0.85214543\n",
      "Training loss (for one batch) at step 290: 0.1344\n",
      "Seen so far: 9312 samples\n",
      "0.85252476\n",
      "Training loss (for one batch) at step 295: 0.3504\n",
      "Seen so far: 9472 samples\n",
      "0.85287523\n",
      "Training loss (for one batch) at step 300: 0.0667\n",
      "Seen so far: 9632 samples\n",
      "0.85317147\n",
      "Training loss (for one batch) at step 305: 0.1173\n",
      "Seen so far: 9792 samples\n",
      "0.8534397\n",
      "Training loss (for one batch) at step 310: 0.1326\n",
      "Seen so far: 9952 samples\n",
      "0.8538585\n",
      "Training loss (for one batch) at step 315: 0.0316\n",
      "Seen so far: 10112 samples\n",
      "0.85419786\n",
      "Training loss (for one batch) at step 320: 0.2508\n",
      "Seen so far: 10272 samples\n",
      "0.8544839\n",
      "Training loss (for one batch) at step 325: 0.2753\n",
      "Seen so far: 10432 samples\n",
      "0.85459167\n",
      "Training loss (for one batch) at step 330: 0.1141\n",
      "Seen so far: 10592 samples\n",
      "0.8550743\n",
      "Training loss (for one batch) at step 335: 0.3391\n",
      "Seen so far: 10752 samples\n",
      "0.8553035\n",
      "Training loss (for one batch) at step 340: 0.3432\n",
      "Seen so far: 10912 samples\n",
      "0.8555807\n",
      "Training loss (for one batch) at step 345: 0.4580\n",
      "Seen so far: 11072 samples\n",
      "0.8558309\n",
      "Training loss (for one batch) at step 350: 0.2961\n",
      "Seen so far: 11232 samples\n",
      "0.8560791\n",
      "Training loss (for one batch) at step 355: 0.0738\n",
      "Seen so far: 11392 samples\n",
      "0.8562763\n",
      "Training loss (for one batch) at step 360: 0.0573\n",
      "Seen so far: 11552 samples\n",
      "0.8566676\n",
      "Training loss (for one batch) at step 365: 0.1678\n",
      "Seen so far: 11712 samples\n",
      "0.85688525\n",
      "Training loss (for one batch) at step 370: 0.3195\n",
      "Seen so far: 11872 samples\n",
      "0.8571255\n",
      "Training loss (for one batch) at step 375: 0.1644\n",
      "Seen so far: 12032 samples\n",
      "0.85736394\n",
      "Training loss (for one batch) at step 380: 0.2339\n",
      "Seen so far: 12192 samples\n",
      "0.85767275\n",
      "Training loss (for one batch) at step 385: 0.0566\n",
      "Seen so far: 12352 samples\n",
      "0.8580512\n",
      "Training loss (for one batch) at step 390: 0.2495\n",
      "Seen so far: 12512 samples\n",
      "0.85840285\n",
      "Training loss (for one batch) at step 395: 0.0871\n",
      "Seen so far: 12672 samples\n",
      "0.8587042\n",
      "Training loss (for one batch) at step 400: 0.3134\n",
      "Seen so far: 12832 samples\n",
      "0.85888463\n",
      "Training loss (for one batch) at step 405: 0.0354\n",
      "Seen so far: 12992 samples\n",
      "0.8592528\n",
      "Training loss (for one batch) at step 410: 0.1072\n",
      "Seen so far: 13152 samples\n",
      "0.8595946\n",
      "Training loss (for one batch) at step 415: 0.0215\n",
      "Seen so far: 13312 samples\n",
      "0.85995734\n",
      "Training loss (for one batch) at step 420: 0.0894\n",
      "Seen so far: 13472 samples\n",
      "0.86027056\n",
      "Training loss (for one batch) at step 425: 0.1211\n",
      "Seen so far: 13632 samples\n",
      "0.8605582\n",
      "Training loss (for one batch) at step 430: 0.2135\n",
      "Seen so far: 13792 samples\n",
      "0.86091334\n",
      "Training loss (for one batch) at step 435: 0.3253\n",
      "Seen so far: 13952 samples\n",
      "0.8612196\n",
      "Training loss (for one batch) at step 440: 0.1439\n",
      "Seen so far: 14112 samples\n",
      "0.86156964\n",
      "Training loss (for one batch) at step 445: 0.0829\n",
      "Seen so far: 14272 samples\n",
      "0.8618024\n",
      "Training loss (for one batch) at step 450: 0.0098\n",
      "Seen so far: 14432 samples\n",
      "0.86219347\n",
      "Training loss (for one batch) at step 455: 0.5088\n",
      "Seen so far: 14592 samples\n",
      "0.86253613\n",
      "Training loss (for one batch) at step 460: 0.2905\n",
      "Seen so far: 14752 samples\n",
      "0.8628083\n",
      "Training loss (for one batch) at step 465: 0.0210\n",
      "Seen so far: 14912 samples\n",
      "0.863101\n",
      "Training loss (for one batch) at step 470: 0.1929\n",
      "Seen so far: 15072 samples\n",
      "0.86348176\n",
      "Training loss (for one batch) at step 475: 0.2455\n",
      "Seen so far: 15232 samples\n",
      "0.8636578\n",
      "Training loss (for one batch) at step 480: 0.3188\n",
      "Seen so far: 15392 samples\n",
      "0.86394435\n",
      "Training loss (for one batch) at step 485: 0.0581\n",
      "Seen so far: 15552 samples\n",
      "0.8642957\n",
      "Training loss (for one batch) at step 490: 0.2433\n",
      "Seen so far: 15712 samples\n",
      "0.8645336\n",
      "Training loss (for one batch) at step 495: 0.2028\n",
      "Seen so far: 15872 samples\n",
      "0.86485827\n",
      "Training loss (for one batch) at step 500: 0.2262\n",
      "Seen so far: 16032 samples\n",
      "0.8649602\n",
      "Training loss (for one batch) at step 505: 0.0678\n",
      "Seen so far: 16192 samples\n",
      "0.8652372\n",
      "Training loss (for one batch) at step 510: 0.4438\n",
      "Seen so far: 16352 samples\n",
      "0.8654465\n",
      "Training loss (for one batch) at step 515: 0.1917\n",
      "Seen so far: 16512 samples\n",
      "0.8656108\n",
      "Training loss (for one batch) at step 520: 0.0904\n",
      "Seen so far: 16672 samples\n",
      "0.86583906\n",
      "Training loss (for one batch) at step 525: 0.3046\n",
      "Seen so far: 16832 samples\n",
      "0.86610913\n",
      "Training loss (for one batch) at step 530: 0.0970\n",
      "Seen so far: 16992 samples\n",
      "0.8663558\n",
      "Training loss (for one batch) at step 535: 0.3434\n",
      "Seen so far: 17152 samples\n",
      "0.8665792\n",
      "Training loss (for one batch) at step 540: 0.2763\n",
      "Seen so far: 17312 samples\n",
      "0.86671525\n",
      "Training loss (for one batch) at step 545: 0.0641\n",
      "Seen so far: 17472 samples\n",
      "0.86697865\n",
      "Training loss (for one batch) at step 550: 0.0484\n",
      "Seen so far: 17632 samples\n",
      "0.86730415\n",
      "Training loss (for one batch) at step 555: 0.4290\n",
      "Seen so far: 17792 samples\n",
      "0.86750007\n",
      "Training loss (for one batch) at step 560: 0.0137\n",
      "Seen so far: 17952 samples\n",
      "0.8678639\n",
      "Training loss (for one batch) at step 565: 0.0384\n",
      "Seen so far: 18112 samples\n",
      "0.8682041\n",
      "Training loss (for one batch) at step 570: 0.0561\n",
      "Seen so far: 18272 samples\n",
      "0.8685001\n",
      "Training loss (for one batch) at step 575: 0.6027\n",
      "Seen so far: 18432 samples\n",
      "0.86873126\n",
      "Training loss (for one batch) at step 580: 0.0800\n",
      "Seen so far: 18592 samples\n",
      "0.86904436\n",
      "Training loss (for one batch) at step 585: 0.0386\n",
      "Seen so far: 18752 samples\n",
      "0.86927223\n",
      "Training loss (for one batch) at step 590: 0.0836\n",
      "Seen so far: 18912 samples\n",
      "0.8696229\n",
      "Training loss (for one batch) at step 595: 0.1997\n",
      "Seen so far: 19072 samples\n",
      "0.8699713\n",
      "Training loss (for one batch) at step 600: 0.0848\n",
      "Seen so far: 19232 samples\n",
      "0.8701938\n",
      "Training loss (for one batch) at step 605: 0.5552\n",
      "Seen so far: 19392 samples\n",
      "0.87041485\n",
      "Training loss (for one batch) at step 610: 0.0774\n",
      "Seen so far: 19552 samples\n",
      "0.8706958\n",
      "Training loss (for one batch) at step 615: 0.0704\n",
      "Seen so far: 19712 samples\n",
      "0.8709342\n",
      "Training loss (for one batch) at step 620: 0.0711\n",
      "Seen so far: 19872 samples\n",
      "0.87129295\n",
      "Training loss (for one batch) at step 625: 0.2945\n",
      "Seen so far: 20032 samples\n",
      "0.8715481\n",
      "Training loss (for one batch) at step 630: 0.0169\n",
      "Seen so far: 20192 samples\n",
      "0.8718824\n",
      "Training loss (for one batch) at step 635: 0.0409\n",
      "Seen so far: 20352 samples\n",
      "0.872134\n",
      "Training loss (for one batch) at step 640: 0.0481\n",
      "Seen so far: 20512 samples\n",
      "0.8723639\n",
      "Training loss (for one batch) at step 645: 0.0708\n",
      "Seen so far: 20672 samples\n",
      "0.87265235\n",
      "Training loss (for one batch) at step 650: 0.1058\n",
      "Seen so far: 20832 samples\n",
      "0.8728991\n",
      "Training loss (for one batch) at step 655: 0.1858\n",
      "Seen so far: 20992 samples\n",
      "0.8730449\n",
      "Training loss (for one batch) at step 660: 0.0729\n",
      "Seen so far: 21152 samples\n",
      "0.87316996\n",
      "Training loss (for one batch) at step 665: 0.0573\n",
      "Seen so far: 21312 samples\n",
      "0.8733535\n",
      "Training loss (for one batch) at step 670: 0.0069\n",
      "Seen so far: 21472 samples\n",
      "0.8736343\n",
      "Training loss (for one batch) at step 675: 0.2739\n",
      "Seen so far: 21632 samples\n",
      "0.87387407\n",
      "Training loss (for one batch) at step 680: 0.3050\n",
      "Seen so far: 21792 samples\n",
      "0.8741123\n",
      "Training loss (for one batch) at step 685: 0.0228\n",
      "Seen so far: 21952 samples\n",
      "0.8743296\n",
      "Training loss (for one batch) at step 690: 0.0684\n",
      "Seen so far: 22112 samples\n",
      "0.87450665\n",
      "Training loss (for one batch) at step 695: 0.2545\n",
      "Seen so far: 22272 samples\n",
      "0.8747795\n",
      "Training loss (for one batch) at step 700: 0.3115\n",
      "Seen so far: 22432 samples\n",
      "0.8750314\n",
      "Training loss (for one batch) at step 705: 0.0668\n",
      "Seen so far: 22592 samples\n",
      "0.87522393\n",
      "Training loss (for one batch) at step 710: 0.1658\n",
      "Seen so far: 22752 samples\n",
      "0.87543446\n",
      "Training loss (for one batch) at step 715: 0.0272\n",
      "Seen so far: 22912 samples\n",
      "0.875682\n",
      "Training loss (for one batch) at step 720: 0.1380\n",
      "Seen so far: 23072 samples\n",
      "0.875928\n",
      "Training loss (for one batch) at step 725: 0.0886\n",
      "Seen so far: 23232 samples\n",
      "0.8762106\n",
      "Training loss (for one batch) at step 730: 0.0981\n",
      "Seen so far: 23392 samples\n",
      "0.8764535\n",
      "Training loss (for one batch) at step 735: 0.0633\n",
      "Seen so far: 23552 samples\n",
      "0.87673277\n",
      "Training loss (for one batch) at step 740: 0.0895\n",
      "Seen so far: 23712 samples\n",
      "0.8768973\n",
      "Training loss (for one batch) at step 745: 0.3806\n",
      "Seen so far: 23872 samples\n",
      "0.8770607\n",
      "Training loss (for one batch) at step 750: 0.1843\n",
      "Seen so far: 24032 samples\n",
      "0.877167\n",
      "Training loss (for one batch) at step 755: 0.0548\n",
      "Seen so far: 24192 samples\n",
      "0.87742215\n",
      "Training loss (for one batch) at step 760: 0.1360\n",
      "Seen so far: 24352 samples\n",
      "0.8776012\n",
      "Training loss (for one batch) at step 765: 0.2294\n",
      "Seen so far: 24512 samples\n",
      "0.87776065\n",
      "Training loss (for one batch) at step 770: 0.0174\n",
      "Seen so far: 24672 samples\n",
      "0.87791914\n",
      "Training loss (for one batch) at step 775: 0.0816\n",
      "Seen so far: 24832 samples\n",
      "0.8781136\n",
      "Training loss (for one batch) at step 780: 0.1350\n",
      "Seen so far: 24992 samples\n",
      "0.87825173\n",
      "Training loss (for one batch) at step 785: 0.0991\n",
      "Seen so far: 25152 samples\n",
      "0.8784258\n",
      "Training loss (for one batch) at step 790: 0.0917\n",
      "Seen so far: 25312 samples\n",
      "0.87869024\n",
      "Training loss (for one batch) at step 795: 0.0963\n",
      "Seen so far: 25472 samples\n",
      "0.8788985\n",
      "Training loss (for one batch) at step 800: 0.0503\n",
      "Seen so far: 25632 samples\n",
      "0.87916005\n",
      "Training loss (for one batch) at step 805: 0.1385\n",
      "Seen so far: 25792 samples\n",
      "0.87925684\n",
      "Training loss (for one batch) at step 810: 0.0271\n",
      "Seen so far: 25952 samples\n",
      "0.8794797\n",
      "Training loss (for one batch) at step 815: 0.1679\n",
      "Seen so far: 26112 samples\n",
      "0.8797193\n",
      "Training loss (for one batch) at step 820: 0.0532\n",
      "Seen so far: 26272 samples\n",
      "0.8800115\n",
      "Training loss (for one batch) at step 825: 0.2060\n",
      "Seen so far: 26432 samples\n",
      "0.8801406\n",
      "Training loss (for one batch) at step 830: 0.0730\n",
      "Seen so far: 26592 samples\n",
      "0.8803763\n",
      "Training loss (for one batch) at step 835: 0.0146\n",
      "Seen so far: 26752 samples\n",
      "0.88059276\n",
      "Training loss (for one batch) at step 840: 0.1150\n",
      "Seen so far: 26912 samples\n",
      "0.88079023\n",
      "Training loss (for one batch) at step 845: 0.1658\n",
      "Seen so far: 27072 samples\n",
      "0.88096887\n",
      "Training loss (for one batch) at step 850: 0.0933\n",
      "Seen so far: 27232 samples\n",
      "0.8811111\n",
      "Training loss (for one batch) at step 855: 0.1409\n",
      "Seen so far: 27392 samples\n",
      "0.8813936\n",
      "Training loss (for one batch) at step 860: 0.0332\n",
      "Seen so far: 27552 samples\n",
      "0.8816393\n",
      "Training loss (for one batch) at step 865: 0.0897\n",
      "Seen so far: 27712 samples\n",
      "0.88174343\n",
      "Training loss (for one batch) at step 870: 0.3489\n",
      "Seen so far: 27872 samples\n",
      "0.88184696\n",
      "Training loss (for one batch) at step 875: 0.2399\n",
      "Seen so far: 28032 samples\n",
      "0.88193244\n",
      "Training loss (for one batch) at step 880: 0.1477\n",
      "Seen so far: 28192 samples\n",
      "0.8821044\n",
      "Training loss (for one batch) at step 885: 0.0246\n",
      "Seen so far: 28352 samples\n",
      "0.8822581\n",
      "Training loss (for one batch) at step 890: 0.1269\n",
      "Seen so far: 28512 samples\n",
      "0.8824109\n",
      "Training loss (for one batch) at step 895: 0.1231\n",
      "Seen so far: 28672 samples\n",
      "0.88254565\n",
      "Training loss (for one batch) at step 900: 0.2777\n",
      "Seen so far: 28832 samples\n",
      "0.8825937\n",
      "Training loss (for one batch) at step 905: 0.1223\n",
      "Seen so far: 28992 samples\n",
      "0.88272715\n",
      "Training loss (for one batch) at step 910: 0.1390\n",
      "Seen so far: 29152 samples\n",
      "0.8829112\n",
      "Training loss (for one batch) at step 915: 0.1160\n",
      "Seen so far: 29312 samples\n",
      "0.88307726\n",
      "Training acc over epoch: 0.8831\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.5474\n",
      "Seen so far: 32 samples\n",
      "0.8830646\n",
      "Training loss (for one batch) at step 5: 0.1870\n",
      "Seen so far: 192 samples\n",
      "0.8831447\n",
      "Training loss (for one batch) at step 10: 0.1418\n",
      "Seen so far: 352 samples\n",
      "0.8832243\n",
      "Training loss (for one batch) at step 15: 0.3451\n",
      "Seen so far: 512 samples\n",
      "0.88333726\n",
      "Training loss (for one batch) at step 20: 0.1103\n",
      "Seen so far: 672 samples\n",
      "0.88348335\n",
      "Training loss (for one batch) at step 25: 0.6255\n",
      "Seen so far: 832 samples\n",
      "0.8835446\n",
      "Training loss (for one batch) at step 30: 0.3326\n",
      "Seen so far: 992 samples\n",
      "0.8836726\n",
      "Training loss (for one batch) at step 35: 0.2372\n",
      "Seen so far: 1152 samples\n",
      "0.8837664\n",
      "Training loss (for one batch) at step 40: 0.3271\n",
      "Seen so far: 1312 samples\n",
      "0.8839098\n",
      "Training loss (for one batch) at step 45: 0.1581\n",
      "Seen so far: 1472 samples\n",
      "0.88405246\n",
      "Training loss (for one batch) at step 50: 0.1166\n",
      "Seen so far: 1632 samples\n",
      "0.8841777\n",
      "Training loss (for one batch) at step 55: 0.5505\n",
      "Seen so far: 1792 samples\n",
      "0.8841865\n",
      "Training loss (for one batch) at step 60: 0.3826\n",
      "Seen so far: 1952 samples\n",
      "0.88427776\n",
      "Training loss (for one batch) at step 65: 0.0529\n",
      "Seen so far: 2112 samples\n",
      "0.88440144\n",
      "Training loss (for one batch) at step 70: 0.1137\n",
      "Seen so far: 2272 samples\n",
      "0.88452446\n",
      "Training loss (for one batch) at step 75: 0.3714\n",
      "Seen so far: 2432 samples\n",
      "0.884565\n",
      "Training loss (for one batch) at step 80: 0.1767\n",
      "Seen so far: 2592 samples\n",
      "0.88465434\n",
      "Training loss (for one batch) at step 85: 0.1403\n",
      "Seen so far: 2752 samples\n",
      "0.88479203\n",
      "Training loss (for one batch) at step 90: 0.0733\n",
      "Seen so far: 2912 samples\n",
      "0.8848803\n",
      "Training loss (for one batch) at step 95: 0.0553\n",
      "Seen so far: 3072 samples\n",
      "0.88500047\n",
      "Training loss (for one batch) at step 100: 0.0379\n",
      "Seen so far: 3232 samples\n",
      "0.88516855\n",
      "Training loss (for one batch) at step 105: 0.0211\n",
      "Seen so far: 3392 samples\n",
      "0.88531965\n",
      "Training loss (for one batch) at step 110: 0.1133\n",
      "Seen so far: 3552 samples\n",
      "0.885486\n",
      "Training loss (for one batch) at step 115: 0.1226\n",
      "Seen so far: 3712 samples\n",
      "0.88561946\n",
      "Training loss (for one batch) at step 120: 0.2620\n",
      "Seen so far: 3872 samples\n",
      "0.8857842\n",
      "Training loss (for one batch) at step 125: 0.3445\n",
      "Seen so far: 4032 samples\n",
      "0.88590026\n",
      "Training loss (for one batch) at step 130: 0.1385\n",
      "Seen so far: 4192 samples\n",
      "0.88606346\n",
      "Training loss (for one batch) at step 135: 0.2287\n",
      "Seen so far: 4352 samples\n",
      "0.88624173\n",
      "Training loss (for one batch) at step 140: 0.0998\n",
      "Seen so far: 4512 samples\n",
      "0.88630825\n",
      "Training loss (for one batch) at step 145: 0.1529\n",
      "Seen so far: 4672 samples\n",
      "0.8865481\n",
      "Training loss (for one batch) at step 150: 0.1022\n",
      "Seen so far: 4832 samples\n",
      "0.8866923\n",
      "Training loss (for one batch) at step 155: 0.0497\n",
      "Seen so far: 4992 samples\n",
      "0.88686717\n",
      "Training loss (for one batch) at step 160: 0.2208\n",
      "Seen so far: 5152 samples\n",
      "0.88697845\n",
      "Training loss (for one batch) at step 165: 0.4491\n",
      "Seen so far: 5312 samples\n",
      "0.8871674\n",
      "Training loss (for one batch) at step 170: 0.4432\n",
      "Seen so far: 5472 samples\n",
      "0.887293\n",
      "Training loss (for one batch) at step 175: 0.5014\n",
      "Seen so far: 5632 samples\n",
      "0.88737124\n",
      "Training loss (for one batch) at step 180: 0.1862\n",
      "Seen so far: 5792 samples\n",
      "0.88752675\n",
      "Training loss (for one batch) at step 185: 0.2308\n",
      "Seen so far: 5952 samples\n",
      "0.8875886\n",
      "Training loss (for one batch) at step 190: 0.1412\n",
      "Seen so far: 6112 samples\n",
      "0.8876965\n",
      "Training loss (for one batch) at step 195: 0.1078\n",
      "Seen so far: 6272 samples\n",
      "0.88785005\n",
      "Training loss (for one batch) at step 200: 0.0488\n",
      "Seen so far: 6432 samples\n",
      "0.88791066\n",
      "Training loss (for one batch) at step 205: 0.3673\n",
      "Seen so far: 6592 samples\n",
      "0.8880016\n",
      "Training loss (for one batch) at step 210: 0.1578\n",
      "Seen so far: 6752 samples\n",
      "0.8881227\n",
      "Training loss (for one batch) at step 215: 0.3629\n",
      "Seen so far: 6912 samples\n",
      "0.88825846\n",
      "Training loss (for one batch) at step 220: 0.0726\n",
      "Seen so far: 7072 samples\n",
      "0.8884088\n",
      "Training loss (for one batch) at step 225: 0.1287\n",
      "Seen so far: 7232 samples\n",
      "0.8886342\n",
      "Training loss (for one batch) at step 230: 0.5102\n",
      "Seen so far: 7392 samples\n",
      "0.8887829\n",
      "Training loss (for one batch) at step 235: 0.0711\n",
      "Seen so far: 7552 samples\n",
      "0.88894594\n",
      "Training loss (for one batch) at step 240: 0.0079\n",
      "Seen so far: 7712 samples\n",
      "0.8890479\n",
      "Training loss (for one batch) at step 245: 0.2843\n",
      "Seen so far: 7872 samples\n",
      "0.8891795\n",
      "Training loss (for one batch) at step 250: 0.0400\n",
      "Seen so far: 8032 samples\n",
      "0.88925046\n",
      "Training loss (for one batch) at step 255: 0.0545\n",
      "Seen so far: 8192 samples\n",
      "0.8894408\n",
      "Training loss (for one batch) at step 260: 0.1141\n",
      "Seen so far: 8352 samples\n",
      "0.88963014\n",
      "Training loss (for one batch) at step 265: 0.0523\n",
      "Seen so far: 8512 samples\n",
      "0.88977396\n",
      "Training loss (for one batch) at step 270: 0.1718\n",
      "Seen so far: 8672 samples\n",
      "0.88984287\n",
      "Training loss (for one batch) at step 275: 0.0143\n",
      "Seen so far: 8832 samples\n",
      "0.8899558\n",
      "Training loss (for one batch) at step 280: 0.2922\n",
      "Seen so far: 8992 samples\n",
      "0.8900831\n",
      "Training loss (for one batch) at step 285: 0.0843\n",
      "Seen so far: 9152 samples\n",
      "0.89026874\n",
      "Training loss (for one batch) at step 290: 0.0715\n",
      "Seen so far: 9312 samples\n",
      "0.8904535\n",
      "Training loss (for one batch) at step 295: 0.0953\n",
      "Seen so far: 9472 samples\n",
      "0.8906374\n",
      "Training loss (for one batch) at step 300: 0.0238\n",
      "Seen so far: 9632 samples\n",
      "0.890879\n",
      "Training loss (for one batch) at step 305: 0.2142\n",
      "Seen so far: 9792 samples\n",
      "0.8910172\n",
      "Training loss (for one batch) at step 310: 0.2117\n",
      "Seen so far: 9952 samples\n",
      "0.89116937\n",
      "Training loss (for one batch) at step 315: 0.3038\n",
      "Seen so far: 10112 samples\n",
      "0.8912626\n",
      "Training loss (for one batch) at step 320: 0.0552\n",
      "Seen so far: 10272 samples\n",
      "0.89139897\n",
      "Training loss (for one batch) at step 325: 0.2488\n",
      "Seen so far: 10432 samples\n",
      "0.89143336\n",
      "Training loss (for one batch) at step 330: 0.0431\n",
      "Seen so far: 10592 samples\n",
      "0.89161205\n",
      "Training loss (for one batch) at step 335: 0.1016\n",
      "Seen so far: 10752 samples\n",
      "0.8918187\n",
      "Training loss (for one batch) at step 340: 0.0612\n",
      "Seen so far: 10912 samples\n",
      "0.89201003\n",
      "Training loss (for one batch) at step 345: 0.0635\n",
      "Seen so far: 11072 samples\n",
      "0.89217174\n",
      "Training loss (for one batch) at step 350: 0.3113\n",
      "Seen so far: 11232 samples\n",
      "0.8923185\n",
      "Training loss (for one batch) at step 355: 0.2354\n",
      "Seen so far: 11392 samples\n",
      "0.89249307\n",
      "Training loss (for one batch) at step 360: 0.0168\n",
      "Seen so far: 11552 samples\n",
      "0.8926669\n",
      "Training loss (for one batch) at step 365: 0.1202\n",
      "Seen so far: 11712 samples\n",
      "0.8928541\n",
      "Training loss (for one batch) at step 370: 0.0654\n",
      "Seen so far: 11872 samples\n",
      "0.8930263\n",
      "Training loss (for one batch) at step 375: 0.2056\n",
      "Seen so far: 12032 samples\n",
      "0.8930704\n",
      "Training loss (for one batch) at step 380: 0.1096\n",
      "Seen so far: 12192 samples\n",
      "0.8932413\n",
      "Training loss (for one batch) at step 385: 0.1365\n",
      "Seen so far: 12352 samples\n",
      "0.89335513\n",
      "Training loss (for one batch) at step 390: 0.0554\n",
      "Seen so far: 12512 samples\n",
      "0.8935387\n",
      "Training loss (for one batch) at step 395: 0.2507\n",
      "Seen so far: 12672 samples\n",
      "0.8935952\n",
      "Training loss (for one batch) at step 400: 0.2269\n",
      "Seen so far: 12832 samples\n",
      "0.8937075\n",
      "Training loss (for one batch) at step 405: 0.0947\n",
      "Seen so far: 12992 samples\n",
      "0.8937774\n",
      "Training loss (for one batch) at step 410: 0.0800\n",
      "Seen so far: 13152 samples\n",
      "0.89393055\n",
      "Training loss (for one batch) at step 415: 0.1072\n",
      "Seen so far: 13312 samples\n",
      "0.8940413\n",
      "Training loss (for one batch) at step 420: 0.2713\n",
      "Seen so far: 13472 samples\n",
      "0.89408225\n",
      "Training loss (for one batch) at step 425: 1.3839\n",
      "Seen so far: 13632 samples\n",
      "0.89409536\n",
      "Training loss (for one batch) at step 430: 0.3323\n",
      "Seen so far: 13792 samples\n",
      "0.8941498\n",
      "Training loss (for one batch) at step 435: 0.0988\n",
      "Seen so far: 13952 samples\n",
      "0.8943142\n",
      "Training loss (for one batch) at step 440: 0.0633\n",
      "Seen so far: 14112 samples\n",
      "0.8943954\n",
      "Training loss (for one batch) at step 445: 0.5297\n",
      "Seen so far: 14272 samples\n",
      "0.8944626\n",
      "Training loss (for one batch) at step 450: 0.0295\n",
      "Seen so far: 14432 samples\n",
      "0.8946252\n",
      "Training loss (for one batch) at step 455: 0.0501\n",
      "Seen so far: 14592 samples\n",
      "0.8947189\n",
      "Training loss (for one batch) at step 460: 0.0384\n",
      "Seen so far: 14752 samples\n",
      "0.89486665\n",
      "Training loss (for one batch) at step 465: 0.2823\n",
      "Seen so far: 14912 samples\n",
      "0.89491856\n",
      "Training loss (for one batch) at step 470: 0.1297\n",
      "Seen so far: 15072 samples\n",
      "0.8949703\n",
      "Training loss (for one batch) at step 475: 0.1563\n",
      "Seen so far: 15232 samples\n",
      "0.89508945\n",
      "Training loss (for one batch) at step 480: 0.1010\n",
      "Seen so far: 15392 samples\n",
      "0.8952351\n",
      "Training loss (for one batch) at step 485: 0.1292\n",
      "Seen so far: 15552 samples\n",
      "0.89539367\n",
      "Training loss (for one batch) at step 490: 0.0694\n",
      "Seen so far: 15712 samples\n",
      "0.8954977\n",
      "Training loss (for one batch) at step 495: 0.1045\n",
      "Seen so far: 15872 samples\n",
      "0.895655\n",
      "Training loss (for one batch) at step 500: 0.0853\n",
      "Seen so far: 16032 samples\n",
      "0.8957714\n",
      "Training loss (for one batch) at step 505: 0.3247\n",
      "Seen so far: 16192 samples\n",
      "0.8958339\n",
      "Training loss (for one batch) at step 510: 0.0880\n",
      "Seen so far: 16352 samples\n",
      "0.8959494\n",
      "Training loss (for one batch) at step 515: 0.0645\n",
      "Seen so far: 16512 samples\n",
      "0.8960778\n",
      "Training loss (for one batch) at step 520: 0.3780\n",
      "Seen so far: 16672 samples\n",
      "0.89608616\n",
      "Training loss (for one batch) at step 525: 0.0329\n",
      "Seen so far: 16832 samples\n",
      "0.89626664\n",
      "Training loss (for one batch) at step 530: 0.1018\n",
      "Seen so far: 16992 samples\n",
      "0.8963671\n",
      "Training loss (for one batch) at step 535: 0.0709\n",
      "Seen so far: 17152 samples\n",
      "0.8964539\n",
      "Training loss (for one batch) at step 540: 0.0483\n",
      "Seen so far: 17312 samples\n",
      "0.89657986\n",
      "Training loss (for one batch) at step 545: 0.1879\n",
      "Seen so far: 17472 samples\n",
      "0.89671844\n",
      "Training loss (for one batch) at step 550: 0.0822\n",
      "Seen so far: 17632 samples\n",
      "0.8969088\n",
      "Training loss (for one batch) at step 555: 0.0975\n",
      "Seen so far: 17792 samples\n",
      "0.8970723\n",
      "Training loss (for one batch) at step 560: 0.4200\n",
      "Seen so far: 17952 samples\n",
      "0.8971828\n",
      "Training loss (for one batch) at step 565: 0.2147\n",
      "Seen so far: 18112 samples\n",
      "0.8972929\n",
      "Training loss (for one batch) at step 570: 0.2872\n",
      "Seen so far: 18272 samples\n",
      "0.89744157\n",
      "Training loss (for one batch) at step 575: 0.1271\n",
      "Seen so far: 18432 samples\n",
      "0.8976285\n",
      "Training loss (for one batch) at step 580: 0.0119\n",
      "Seen so far: 18592 samples\n",
      "0.8978017\n",
      "Training loss (for one batch) at step 585: 0.2551\n",
      "Seen so far: 18752 samples\n",
      "0.8978838\n",
      "Training loss (for one batch) at step 590: 0.2275\n",
      "Seen so far: 18912 samples\n",
      "0.8979655\n",
      "Training loss (for one batch) at step 595: 0.3098\n",
      "Seen so far: 19072 samples\n",
      "0.8980597\n",
      "Training loss (for one batch) at step 600: 0.2891\n",
      "Seen so far: 19232 samples\n",
      "0.8981408\n",
      "Training loss (for one batch) at step 605: 0.0740\n",
      "Seen so far: 19392 samples\n",
      "0.89825994\n",
      "Training loss (for one batch) at step 610: 0.1019\n",
      "Seen so far: 19552 samples\n",
      "0.89837855\n",
      "Training loss (for one batch) at step 615: 0.0202\n",
      "Seen so far: 19712 samples\n",
      "0.8984712\n",
      "Training loss (for one batch) at step 620: 0.2608\n",
      "Seen so far: 19872 samples\n",
      "0.8985762\n",
      "Training loss (for one batch) at step 625: 0.0952\n",
      "Seen so far: 20032 samples\n",
      "0.89868075\n",
      "Training loss (for one batch) at step 630: 0.1316\n",
      "Seen so far: 20192 samples\n",
      "0.89877224\n",
      "Training loss (for one batch) at step 635: 0.0589\n",
      "Seen so far: 20352 samples\n",
      "0.8988633\n",
      "Training loss (for one batch) at step 640: 0.2756\n",
      "Seen so far: 20512 samples\n",
      "0.89892876\n",
      "Training loss (for one batch) at step 645: 0.2800\n",
      "Seen so far: 20672 samples\n",
      "0.8990444\n",
      "Training loss (for one batch) at step 650: 0.0359\n",
      "Seen so far: 20832 samples\n",
      "0.8991973\n",
      "Training loss (for one batch) at step 655: 0.0480\n",
      "Seen so far: 20992 samples\n",
      "0.8993119\n",
      "Training loss (for one batch) at step 660: 0.2143\n",
      "Seen so far: 21152 samples\n",
      "0.8994135\n",
      "Training loss (for one batch) at step 665: 0.1958\n",
      "Seen so far: 21312 samples\n",
      "0.89952725\n",
      "Training loss (for one batch) at step 670: 0.0436\n",
      "Seen so far: 21472 samples\n",
      "0.8996156\n",
      "Training loss (for one batch) at step 675: 0.1244\n",
      "Seen so far: 21632 samples\n",
      "0.899716\n",
      "Training loss (for one batch) at step 680: 0.2983\n",
      "Seen so far: 21792 samples\n",
      "0.89982843\n",
      "Training loss (for one batch) at step 685: 0.0880\n",
      "Seen so far: 21952 samples\n",
      "0.8999528\n",
      "Training loss (for one batch) at step 690: 0.0853\n",
      "Seen so far: 22112 samples\n",
      "0.90007675\n",
      "Training loss (for one batch) at step 695: 0.0092\n",
      "Seen so far: 22272 samples\n",
      "0.9002125\n",
      "Training loss (for one batch) at step 700: 0.1167\n",
      "Seen so far: 22432 samples\n",
      "0.9002985\n",
      "Training loss (for one batch) at step 705: 0.1052\n",
      "Seen so far: 22592 samples\n",
      "0.900384\n",
      "Training loss (for one batch) at step 710: 0.1129\n",
      "Seen so far: 22752 samples\n",
      "0.9005184\n",
      "Training loss (for one batch) at step 715: 0.1856\n",
      "Seen so far: 22912 samples\n",
      "0.90064\n",
      "Training loss (for one batch) at step 720: 0.2359\n",
      "Seen so far: 23072 samples\n",
      "0.9006999\n",
      "Training loss (for one batch) at step 725: 0.1360\n",
      "Seen so far: 23232 samples\n",
      "0.90086955\n",
      "Training loss (for one batch) at step 730: 0.0922\n",
      "Seen so far: 23392 samples\n",
      "0.9009288\n",
      "Training loss (for one batch) at step 735: 0.0539\n",
      "Seen so far: 23552 samples\n",
      "0.90104866\n",
      "Training loss (for one batch) at step 740: 0.1628\n",
      "Seen so far: 23712 samples\n",
      "0.9011559\n",
      "Training loss (for one batch) at step 745: 0.0097\n",
      "Seen so far: 23872 samples\n",
      "0.9012627\n",
      "Training loss (for one batch) at step 750: 0.0078\n",
      "Seen so far: 24032 samples\n",
      "0.9014417\n",
      "Training loss (for one batch) at step 755: 0.1737\n",
      "Seen so far: 24192 samples\n",
      "0.9014751\n",
      "Training loss (for one batch) at step 760: 0.0043\n",
      "Seen so far: 24352 samples\n",
      "0.9015566\n",
      "Training loss (for one batch) at step 765: 0.1284\n",
      "Seen so far: 24512 samples\n",
      "0.9016618\n",
      "Training loss (for one batch) at step 770: 0.0358\n",
      "Seen so far: 24672 samples\n",
      "0.9017906\n",
      "Training loss (for one batch) at step 775: 0.0358\n",
      "Seen so far: 24832 samples\n",
      "0.90191895\n",
      "Training loss (for one batch) at step 780: 0.0787\n",
      "Seen so far: 24992 samples\n",
      "0.90192723\n",
      "Training loss (for one batch) at step 785: 0.0181\n",
      "Seen so far: 25152 samples\n",
      "0.90197134\n",
      "Training loss (for one batch) at step 790: 0.1263\n",
      "Seen so far: 25312 samples\n",
      "0.9020986\n",
      "Training loss (for one batch) at step 795: 0.0580\n",
      "Seen so far: 25472 samples\n",
      "0.9021778\n",
      "Training loss (for one batch) at step 800: 0.0133\n",
      "Seen so far: 25632 samples\n",
      "0.9023042\n",
      "Training loss (for one batch) at step 805: 0.0683\n",
      "Seen so far: 25792 samples\n",
      "0.9023827\n",
      "Training loss (for one batch) at step 810: 0.0991\n",
      "Seen so far: 25952 samples\n",
      "0.9025082\n",
      "Training loss (for one batch) at step 815: 0.0528\n",
      "Seen so far: 26112 samples\n",
      "0.90262145\n",
      "Training loss (for one batch) at step 820: 0.0007\n",
      "Seen so far: 26272 samples\n",
      "0.9027225\n",
      "Training loss (for one batch) at step 825: 0.0123\n",
      "Seen so far: 26432 samples\n",
      "0.90282315\n",
      "Training loss (for one batch) at step 830: 0.0177\n",
      "Seen so far: 26592 samples\n",
      "0.9029704\n",
      "Training loss (for one batch) at step 835: 0.0310\n",
      "Seen so far: 26752 samples\n",
      "0.9030702\n",
      "Training loss (for one batch) at step 840: 0.0199\n",
      "Seen so far: 26912 samples\n",
      "0.90322804\n",
      "Training loss (for one batch) at step 845: 0.1167\n",
      "Seen so far: 27072 samples\n",
      "0.90331537\n",
      "Training loss (for one batch) at step 850: 0.1210\n",
      "Seen so far: 27232 samples\n",
      "0.9033907\n",
      "Training loss (for one batch) at step 855: 0.0774\n",
      "Seen so far: 27392 samples\n",
      "0.903489\n",
      "Training loss (for one batch) at step 860: 0.0765\n",
      "Seen so far: 27552 samples\n",
      "0.90362173\n",
      "Training loss (for one batch) at step 865: 0.0424\n",
      "Seen so far: 27712 samples\n",
      "0.90376556\n",
      "Training loss (for one batch) at step 870: 0.2946\n",
      "Seen so far: 27872 samples\n",
      "0.9038279\n",
      "Training loss (for one batch) at step 875: 0.2888\n",
      "Seen so far: 28032 samples\n",
      "0.90390164\n",
      "Training loss (for one batch) at step 880: 0.0496\n",
      "Seen so far: 28192 samples\n",
      "0.90404415\n",
      "Training loss (for one batch) at step 885: 0.0038\n",
      "Seen so far: 28352 samples\n",
      "0.9041401\n",
      "Training loss (for one batch) at step 890: 0.0457\n",
      "Seen so far: 28512 samples\n",
      "0.9042587\n",
      "Training loss (for one batch) at step 895: 0.0293\n",
      "Seen so far: 28672 samples\n",
      "0.9043998\n",
      "Training loss (for one batch) at step 900: 0.0618\n",
      "Seen so far: 28832 samples\n",
      "0.90449464\n",
      "Training loss (for one batch) at step 905: 0.2034\n",
      "Seen so far: 28992 samples\n",
      "0.9046348\n",
      "Training loss (for one batch) at step 910: 0.3936\n",
      "Seen so far: 29152 samples\n",
      "0.90472883\n",
      "Training loss (for one batch) at step 915: 0.0800\n",
      "Seen so far: 29312 samples\n",
      "0.90481114\n",
      "Training acc over epoch: 0.9048\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.0975\n",
      "Seen so far: 32 samples\n",
      "0.90484846\n",
      "Training loss (for one batch) at step 5: 0.0278\n",
      "Seen so far: 192 samples\n",
      "0.9049985\n",
      "Training loss (for one batch) at step 10: 0.0185\n",
      "Seen so far: 352 samples\n",
      "0.90511394\n",
      "Training loss (for one batch) at step 15: 0.4583\n",
      "Seen so far: 512 samples\n",
      "0.9052177\n",
      "Training loss (for one batch) at step 20: 0.4252\n",
      "Seen so far: 672 samples\n",
      "0.9052647\n",
      "Training loss (for one batch) at step 25: 0.0086\n",
      "Seen so far: 832 samples\n",
      "0.90541285\n",
      "Training loss (for one batch) at step 30: 0.1782\n",
      "Seen so far: 992 samples\n",
      "0.905538\n",
      "Training loss (for one batch) at step 35: 0.0465\n",
      "Seen so far: 1152 samples\n",
      "0.9056851\n",
      "Training loss (for one batch) at step 40: 0.0095\n",
      "Seen so far: 1312 samples\n",
      "0.9057534\n",
      "Training loss (for one batch) at step 45: 0.0413\n",
      "Seen so far: 1472 samples\n",
      "0.9058884\n",
      "Training loss (for one batch) at step 50: 0.2545\n",
      "Seen so far: 1632 samples\n",
      "0.9059672\n",
      "Training loss (for one batch) at step 55: 0.0412\n",
      "Seen so far: 1792 samples\n",
      "0.9060345\n",
      "Training loss (for one batch) at step 60: 0.0392\n",
      "Seen so far: 1952 samples\n",
      "0.9061461\n",
      "Training loss (for one batch) at step 65: 0.0717\n",
      "Seen so far: 2112 samples\n",
      "0.9062351\n",
      "Training loss (for one batch) at step 70: 0.1374\n",
      "Seen so far: 2272 samples\n",
      "0.9063016\n",
      "Training loss (for one batch) at step 75: 0.0493\n",
      "Seen so far: 2432 samples\n",
      "0.90643424\n",
      "Training loss (for one batch) at step 80: 0.1569\n",
      "Seen so far: 2592 samples\n",
      "0.9065664\n",
      "Training loss (for one batch) at step 85: 0.0784\n",
      "Seen so far: 2752 samples\n",
      "0.90667605\n",
      "Training loss (for one batch) at step 90: 0.0560\n",
      "Seen so far: 2912 samples\n",
      "0.90678537\n",
      "Training loss (for one batch) at step 95: 0.0034\n",
      "Seen so far: 3072 samples\n",
      "0.9068613\n",
      "Training loss (for one batch) at step 100: 0.2469\n",
      "Seen so far: 3232 samples\n",
      "0.9069479\n",
      "Training loss (for one batch) at step 105: 0.0588\n",
      "Seen so far: 3392 samples\n",
      "0.9070452\n",
      "Training loss (for one batch) at step 110: 0.1569\n",
      "Seen so far: 3552 samples\n",
      "0.90717494\n",
      "Training loss (for one batch) at step 115: 0.0280\n",
      "Seen so far: 3712 samples\n",
      "0.9072933\n",
      "Training loss (for one batch) at step 120: 0.0373\n",
      "Seen so far: 3872 samples\n",
      "0.9073568\n",
      "Training loss (for one batch) at step 125: 0.0979\n",
      "Seen so far: 4032 samples\n",
      "0.90739834\n",
      "Training loss (for one batch) at step 130: 0.0102\n",
      "Seen so far: 4192 samples\n",
      "0.90752655\n",
      "Training loss (for one batch) at step 135: 0.0326\n",
      "Seen so far: 4352 samples\n",
      "0.9076218\n",
      "Training loss (for one batch) at step 140: 0.0885\n",
      "Seen so far: 4512 samples\n",
      "0.9077276\n",
      "Training loss (for one batch) at step 145: 0.0258\n",
      "Seen so far: 4672 samples\n",
      "0.90784377\n",
      "Training loss (for one batch) at step 150: 0.0562\n",
      "Seen so far: 4832 samples\n",
      "0.907938\n",
      "Training loss (for one batch) at step 155: 0.0505\n",
      "Seen so far: 4992 samples\n",
      "0.90802115\n",
      "Training loss (for one batch) at step 160: 0.2841\n",
      "Seen so far: 5152 samples\n",
      "0.90811473\n",
      "Training loss (for one batch) at step 165: 0.0906\n",
      "Seen so far: 5312 samples\n",
      "0.90818655\n",
      "Training loss (for one batch) at step 170: 0.2999\n",
      "Seen so far: 5472 samples\n",
      "0.9082475\n",
      "Training loss (for one batch) at step 175: 0.1953\n",
      "Seen so far: 5632 samples\n",
      "0.9082975\n",
      "Training loss (for one batch) at step 180: 0.2943\n",
      "Seen so far: 5792 samples\n",
      "0.90829396\n",
      "Training loss (for one batch) at step 185: 0.0209\n",
      "Seen so far: 5952 samples\n",
      "0.908365\n",
      "Training loss (for one batch) at step 190: 0.0947\n",
      "Seen so far: 6112 samples\n",
      "0.9083933\n",
      "Training loss (for one batch) at step 195: 0.1336\n",
      "Seen so far: 6272 samples\n",
      "0.9084321\n",
      "Training loss (for one batch) at step 200: 0.1059\n",
      "Seen so far: 6432 samples\n",
      "0.9085025\n",
      "Training loss (for one batch) at step 205: 0.0824\n",
      "Seen so far: 6592 samples\n",
      "0.90858334\n",
      "Training loss (for one batch) at step 210: 0.1070\n",
      "Seen so far: 6752 samples\n",
      "0.90865326\n",
      "Training loss (for one batch) at step 215: 0.0562\n",
      "Seen so far: 6912 samples\n",
      "0.90873355\n",
      "Training loss (for one batch) at step 220: 0.0474\n",
      "Seen so far: 7072 samples\n",
      "0.90881354\n",
      "Training loss (for one batch) at step 225: 0.2352\n",
      "Seen so far: 7232 samples\n",
      "0.9088932\n",
      "Training loss (for one batch) at step 230: 0.3029\n",
      "Seen so far: 7392 samples\n",
      "0.9089727\n",
      "Training loss (for one batch) at step 235: 0.4386\n",
      "Seen so far: 7552 samples\n",
      "0.9089891\n",
      "Training loss (for one batch) at step 240: 0.2517\n",
      "Seen so far: 7712 samples\n",
      "0.90903676\n",
      "Training loss (for one batch) at step 245: 0.1123\n",
      "Seen so far: 7872 samples\n",
      "0.9090947\n",
      "Training loss (for one batch) at step 250: 0.0177\n",
      "Seen so far: 8032 samples\n",
      "0.9091733\n",
      "Training loss (for one batch) at step 255: 0.1443\n",
      "Seen so far: 8192 samples\n",
      "0.9091996\n",
      "Training loss (for one batch) at step 260: 0.2111\n",
      "Seen so far: 8352 samples\n",
      "0.9092466\n",
      "Training loss (for one batch) at step 265: 0.0405\n",
      "Seen so far: 8512 samples\n",
      "0.90930384\n",
      "Training loss (for one batch) at step 270: 0.0124\n",
      "Seen so far: 8672 samples\n",
      "0.9093712\n",
      "Training loss (for one batch) at step 275: 0.1707\n",
      "Seen so far: 8832 samples\n",
      "0.9094384\n",
      "Training loss (for one batch) at step 280: 0.1749\n",
      "Seen so far: 8992 samples\n",
      "0.9095053\n",
      "Training loss (for one batch) at step 285: 0.3424\n",
      "Seen so far: 9152 samples\n",
      "0.90959257\n",
      "Training loss (for one batch) at step 290: 0.0919\n",
      "Seen so far: 9312 samples\n",
      "0.90966934\n",
      "Training loss (for one batch) at step 295: 0.6498\n",
      "Seen so far: 9472 samples\n",
      "0.9096329\n",
      "Training loss (for one batch) at step 300: 0.4088\n",
      "Seen so far: 9632 samples\n",
      "0.9097196\n",
      "Training loss (for one batch) at step 305: 0.1005\n",
      "Seen so far: 9792 samples\n",
      "0.90977526\n",
      "Training loss (for one batch) at step 310: 0.0473\n",
      "Seen so far: 9952 samples\n",
      "0.9098308\n",
      "Training loss (for one batch) at step 315: 0.0259\n",
      "Seen so far: 10112 samples\n",
      "0.9099065\n",
      "Training loss (for one batch) at step 320: 0.0241\n",
      "Seen so far: 10272 samples\n",
      "0.90998197\n",
      "Training loss (for one batch) at step 325: 0.3909\n",
      "Seen so far: 10432 samples\n",
      "0.9099962\n",
      "Training loss (for one batch) at step 330: 0.0073\n",
      "Seen so far: 10592 samples\n",
      "0.91008145\n",
      "Training loss (for one batch) at step 335: 0.0529\n",
      "Seen so far: 10752 samples\n",
      "0.91021705\n",
      "Training loss (for one batch) at step 340: 0.0516\n",
      "Seen so far: 10912 samples\n",
      "0.9102612\n",
      "Training loss (for one batch) at step 345: 0.3754\n",
      "Seen so far: 11072 samples\n",
      "0.9103254\n",
      "Training loss (for one batch) at step 350: 0.1887\n",
      "Seen so far: 11232 samples\n",
      "0.91038936\n",
      "Training loss (for one batch) at step 355: 0.1350\n",
      "Seen so far: 11392 samples\n",
      "0.9104733\n",
      "Training loss (for one batch) at step 360: 0.5146\n",
      "Seen so far: 11552 samples\n",
      "0.91051674\n",
      "Training loss (for one batch) at step 365: 0.1411\n",
      "Seen so far: 11712 samples\n",
      "0.91055\n",
      "Training loss (for one batch) at step 370: 0.1584\n",
      "Seen so far: 11872 samples\n",
      "0.91061324\n",
      "Training loss (for one batch) at step 375: 0.0252\n",
      "Seen so far: 12032 samples\n",
      "0.9106662\n",
      "Training loss (for one batch) at step 380: 0.1605\n",
      "Seen so far: 12192 samples\n",
      "0.9107491\n",
      "Training loss (for one batch) at step 385: 0.2392\n",
      "Seen so far: 12352 samples\n",
      "0.91081166\n",
      "Training loss (for one batch) at step 390: 0.1238\n",
      "Seen so far: 12512 samples\n",
      "0.91087407\n",
      "Training loss (for one batch) at step 395: 0.0724\n",
      "Seen so far: 12672 samples\n",
      "0.91098595\n",
      "Training loss (for one batch) at step 400: 0.1334\n",
      "Seen so far: 12832 samples\n",
      "0.9110478\n",
      "Training loss (for one batch) at step 405: 0.3310\n",
      "Seen so far: 12992 samples\n",
      "0.9110997\n",
      "Training loss (for one batch) at step 410: 0.1354\n",
      "Seen so far: 13152 samples\n",
      "0.9111711\n",
      "Training loss (for one batch) at step 415: 0.0806\n",
      "Seen so far: 13312 samples\n",
      "0.9112719\n",
      "Training loss (for one batch) at step 420: 0.0215\n",
      "Seen so far: 13472 samples\n",
      "0.91135275\n",
      "Training loss (for one batch) at step 425: 0.1173\n",
      "Seen so far: 13632 samples\n",
      "0.91145295\n",
      "Training loss (for one batch) at step 430: 0.1464\n",
      "Seen so far: 13792 samples\n",
      "0.91153324\n",
      "Training loss (for one batch) at step 435: 0.0142\n",
      "Seen so far: 13952 samples\n",
      "0.9116427\n",
      "Training loss (for one batch) at step 440: 0.3133\n",
      "Seen so far: 14112 samples\n",
      "0.91173214\n",
      "Training loss (for one batch) at step 445: 0.0573\n",
      "Seen so far: 14272 samples\n",
      "0.9118116\n",
      "Training loss (for one batch) at step 450: 0.0127\n",
      "Seen so far: 14432 samples\n",
      "0.9119006\n",
      "Training loss (for one batch) at step 455: 0.0330\n",
      "Seen so far: 14592 samples\n",
      "0.9119892\n",
      "Training loss (for one batch) at step 460: 0.0874\n",
      "Seen so far: 14752 samples\n",
      "0.91202897\n",
      "Training loss (for one batch) at step 465: 0.0326\n",
      "Seen so far: 14912 samples\n",
      "0.91209775\n",
      "Training loss (for one batch) at step 470: 0.6029\n",
      "Seen so far: 15072 samples\n",
      "0.9121566\n",
      "Training loss (for one batch) at step 475: 0.0095\n",
      "Seen so far: 15232 samples\n",
      "0.91223466\n",
      "Training loss (for one batch) at step 480: 0.0125\n",
      "Seen so far: 15392 samples\n",
      "0.9123415\n",
      "Training loss (for one batch) at step 485: 0.0118\n",
      "Seen so far: 15552 samples\n",
      "0.9124286\n",
      "Training loss (for one batch) at step 490: 0.0141\n",
      "Seen so far: 15712 samples\n",
      "0.9125059\n",
      "Training loss (for one batch) at step 495: 0.0234\n",
      "Seen so far: 15872 samples\n",
      "0.912554\n",
      "Training loss (for one batch) at step 500: 0.1226\n",
      "Seen so far: 16032 samples\n",
      "0.91262126\n",
      "Training loss (for one batch) at step 505: 0.6831\n",
      "Seen so far: 16192 samples\n",
      "0.9126019\n",
      "Training loss (for one batch) at step 510: 0.0468\n",
      "Seen so far: 16352 samples\n",
      "0.91264004\n",
      "Training loss (for one batch) at step 515: 0.0093\n",
      "Seen so far: 16512 samples\n",
      "0.9127356\n",
      "Training loss (for one batch) at step 520: 0.0094\n",
      "Seen so far: 16672 samples\n",
      "0.91283077\n",
      "Training loss (for one batch) at step 525: 0.1135\n",
      "Seen so far: 16832 samples\n",
      "0.9128875\n",
      "Training loss (for one batch) at step 530: 0.1958\n",
      "Seen so far: 16992 samples\n",
      "0.9129536\n",
      "Training loss (for one batch) at step 535: 0.1217\n",
      "Seen so far: 17152 samples\n",
      "0.913029\n",
      "Training loss (for one batch) at step 540: 0.0348\n",
      "Seen so far: 17312 samples\n",
      "0.91311365\n",
      "Training loss (for one batch) at step 545: 0.1128\n",
      "Seen so far: 17472 samples\n",
      "0.91321707\n",
      "Training loss (for one batch) at step 550: 0.0527\n",
      "Seen so far: 17632 samples\n",
      "0.9132728\n",
      "Training loss (for one batch) at step 555: 0.3019\n",
      "Seen so far: 17792 samples\n",
      "0.9133378\n",
      "Training loss (for one batch) at step 560: 0.0159\n",
      "Seen so far: 17952 samples\n",
      "0.9134309\n",
      "Training loss (for one batch) at step 565: 0.0401\n",
      "Seen so far: 18112 samples\n",
      "0.9135238\n",
      "Training loss (for one batch) at step 570: 0.0183\n",
      "Seen so far: 18272 samples\n",
      "0.9136352\n",
      "Training loss (for one batch) at step 575: 0.0277\n",
      "Seen so far: 18432 samples\n",
      "0.91375566\n",
      "Training loss (for one batch) at step 580: 0.0999\n",
      "Seen so far: 18592 samples\n",
      "0.913857\n",
      "Training loss (for one batch) at step 585: 0.0689\n",
      "Seen so far: 18752 samples\n",
      "0.913958\n",
      "Training loss (for one batch) at step 590: 0.0116\n",
      "Seen so far: 18912 samples\n",
      "0.91403073\n",
      "Training loss (for one batch) at step 595: 0.1333\n",
      "Seen so far: 19072 samples\n",
      "0.91410315\n",
      "Training loss (for one batch) at step 600: 0.0253\n",
      "Seen so far: 19232 samples\n",
      "0.91419405\n",
      "Training loss (for one batch) at step 605: 0.2416\n",
      "Seen so far: 19392 samples\n",
      "0.9142474\n",
      "Training loss (for one batch) at step 610: 0.3003\n",
      "Seen so far: 19552 samples\n",
      "0.9143378\n",
      "Training loss (for one batch) at step 615: 0.0109\n",
      "Seen so far: 19712 samples\n",
      "0.91442794\n",
      "Training loss (for one batch) at step 620: 0.1837\n",
      "Seen so far: 19872 samples\n",
      "0.91446215\n",
      "Training loss (for one batch) at step 625: 0.0252\n",
      "Seen so far: 20032 samples\n",
      "0.91454256\n",
      "Training loss (for one batch) at step 630: 0.1307\n",
      "Seen so far: 20192 samples\n",
      "0.91460425\n",
      "Training loss (for one batch) at step 635: 0.0219\n",
      "Seen so far: 20352 samples\n",
      "0.91466576\n",
      "Training loss (for one batch) at step 640: 0.0351\n",
      "Seen so far: 20512 samples\n",
      "0.9147179\n",
      "Training loss (for one batch) at step 645: 0.1727\n",
      "Seen so far: 20672 samples\n",
      "0.91481584\n",
      "Training loss (for one batch) at step 650: 0.0538\n",
      "Seen so far: 20832 samples\n",
      "0.9148492\n",
      "Training loss (for one batch) at step 655: 0.0597\n",
      "Seen so far: 20992 samples\n",
      "0.91490084\n",
      "Training loss (for one batch) at step 660: 0.0150\n",
      "Seen so far: 21152 samples\n",
      "0.91498893\n",
      "Training loss (for one batch) at step 665: 0.0081\n",
      "Seen so far: 21312 samples\n",
      "0.9150494\n",
      "Training loss (for one batch) at step 670: 0.0163\n",
      "Seen so far: 21472 samples\n",
      "0.9151462\n",
      "Training loss (for one batch) at step 675: 0.1800\n",
      "Seen so far: 21632 samples\n",
      "0.91516054\n",
      "Training loss (for one batch) at step 680: 0.0560\n",
      "Seen so far: 21792 samples\n",
      "0.9151931\n",
      "Training loss (for one batch) at step 685: 0.0096\n",
      "Seen so far: 21952 samples\n",
      "0.91528016\n",
      "Training loss (for one batch) at step 690: 0.3308\n",
      "Seen so far: 22112 samples\n",
      "0.9153124\n",
      "Training loss (for one batch) at step 695: 0.0624\n",
      "Seen so far: 22272 samples\n",
      "0.9153991\n",
      "Training loss (for one batch) at step 700: 0.2219\n",
      "Seen so far: 22432 samples\n",
      "0.91545826\n",
      "Training loss (for one batch) at step 705: 0.1155\n",
      "Seen so far: 22592 samples\n",
      "0.91555345\n",
      "Training loss (for one batch) at step 710: 0.0170\n",
      "Seen so far: 22752 samples\n",
      "0.91563934\n",
      "Training loss (for one batch) at step 715: 0.1327\n",
      "Seen so far: 22912 samples\n",
      "0.915725\n",
      "Training loss (for one batch) at step 720: 0.0060\n",
      "Seen so far: 23072 samples\n",
      "0.9158014\n",
      "Training loss (for one batch) at step 725: 0.4559\n",
      "Seen so far: 23232 samples\n",
      "0.91581464\n",
      "Training loss (for one batch) at step 730: 0.1913\n",
      "Seen so far: 23392 samples\n",
      "0.91587275\n",
      "Training loss (for one batch) at step 735: 0.0703\n",
      "Seen so far: 23552 samples\n",
      "0.9159217\n",
      "Training loss (for one batch) at step 740: 0.0149\n",
      "Seen so far: 23712 samples\n",
      "0.9159974\n",
      "Training loss (for one batch) at step 745: 0.0698\n",
      "Seen so far: 23872 samples\n",
      "0.9160639\n",
      "Training loss (for one batch) at step 750: 0.0134\n",
      "Seen so far: 24032 samples\n",
      "0.9161392\n",
      "Training loss (for one batch) at step 755: 0.2421\n",
      "Seen so far: 24192 samples\n",
      "0.9162142\n",
      "Training loss (for one batch) at step 760: 0.0737\n",
      "Seen so far: 24352 samples\n",
      "0.91630685\n",
      "Training loss (for one batch) at step 765: 0.0338\n",
      "Seen so far: 24512 samples\n",
      "0.9163815\n",
      "Training loss (for one batch) at step 770: 0.0278\n",
      "Seen so far: 24672 samples\n",
      "0.9164469\n",
      "Training loss (for one batch) at step 775: 0.0295\n",
      "Seen so far: 24832 samples\n",
      "0.9165389\n",
      "Training loss (for one batch) at step 780: 0.3109\n",
      "Seen so far: 24992 samples\n",
      "0.9165686\n",
      "Training loss (for one batch) at step 785: 0.0370\n",
      "Seen so far: 25152 samples\n",
      "0.91661584\n",
      "Training loss (for one batch) at step 790: 0.1477\n",
      "Seen so far: 25312 samples\n",
      "0.9166718\n",
      "Training loss (for one batch) at step 795: 0.4447\n",
      "Seen so far: 25472 samples\n",
      "0.91675407\n",
      "Training loss (for one batch) at step 800: 0.0416\n",
      "Seen so far: 25632 samples\n",
      "0.9168273\n",
      "Training loss (for one batch) at step 805: 0.0367\n",
      "Seen so far: 25792 samples\n",
      "0.9168915\n",
      "Training loss (for one batch) at step 810: 0.0233\n",
      "Seen so far: 25952 samples\n",
      "0.9170083\n",
      "Training loss (for one batch) at step 815: 0.0695\n",
      "Seen so far: 26112 samples\n",
      "0.91707206\n",
      "Training loss (for one batch) at step 820: 0.0350\n",
      "Seen so far: 26272 samples\n",
      "0.91713566\n",
      "Training loss (for one batch) at step 825: 0.0897\n",
      "Seen so far: 26432 samples\n",
      "0.91722536\n",
      "Training loss (for one batch) at step 830: 0.0831\n",
      "Seen so far: 26592 samples\n",
      "0.91732347\n",
      "Training loss (for one batch) at step 835: 0.0849\n",
      "Seen so far: 26752 samples\n",
      "0.91741264\n",
      "Training loss (for one batch) at step 840: 0.0441\n",
      "Seen so far: 26912 samples\n",
      "0.91750157\n",
      "Training loss (for one batch) at step 845: 0.0146\n",
      "Seen so far: 27072 samples\n",
      "0.91756415\n",
      "Training loss (for one batch) at step 850: 0.0345\n",
      "Seen so far: 27232 samples\n",
      "0.91761786\n",
      "Training loss (for one batch) at step 855: 0.0253\n",
      "Seen so far: 27392 samples\n",
      "0.9177148\n",
      "Training loss (for one batch) at step 860: 0.0753\n",
      "Seen so far: 27552 samples\n",
      "0.91780275\n",
      "Training loss (for one batch) at step 865: 0.0267\n",
      "Seen so far: 27712 samples\n",
      "0.9178473\n",
      "Training loss (for one batch) at step 870: 0.0236\n",
      "Seen so far: 27872 samples\n",
      "0.9179003\n",
      "Training loss (for one batch) at step 875: 0.0634\n",
      "Seen so far: 28032 samples\n",
      "0.91795325\n",
      "Training loss (for one batch) at step 880: 0.0977\n",
      "Seen so far: 28192 samples\n",
      "0.9180318\n",
      "Training loss (for one batch) at step 885: 0.1054\n",
      "Seen so far: 28352 samples\n",
      "0.91809297\n",
      "Training loss (for one batch) at step 890: 0.0291\n",
      "Seen so far: 28512 samples\n",
      "0.9181625\n",
      "Training loss (for one batch) at step 895: 0.0358\n",
      "Seen so far: 28672 samples\n",
      "0.9182491\n",
      "Training loss (for one batch) at step 900: 0.0209\n",
      "Seen so far: 28832 samples\n",
      "0.9183268\n",
      "Training loss (for one batch) at step 905: 0.0356\n",
      "Seen so far: 28992 samples\n",
      "0.91841286\n",
      "Training loss (for one batch) at step 910: 0.1114\n",
      "Seen so far: 29152 samples\n",
      "0.9184731\n",
      "Training loss (for one batch) at step 915: 0.0297\n",
      "Seen so far: 29312 samples\n",
      "0.9185502\n",
      "Training acc over epoch: 0.9186\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.0788\n",
      "Seen so far: 32 samples\n",
      "0.9185559\n",
      "Training loss (for one batch) at step 5: 0.1192\n",
      "Seen so far: 192 samples\n",
      "0.91863275\n",
      "Training loss (for one batch) at step 10: 0.0305\n",
      "Seen so far: 352 samples\n",
      "0.9186924\n",
      "Training loss (for one batch) at step 15: 0.0061\n",
      "Seen so far: 512 samples\n",
      "0.91874343\n",
      "Training loss (for one batch) at step 20: 0.0912\n",
      "Seen so far: 672 samples\n",
      "0.91877735\n",
      "Training loss (for one batch) at step 25: 0.0537\n",
      "Seen so far: 832 samples\n",
      "0.91885346\n",
      "Training loss (for one batch) at step 30: 0.1214\n",
      "Seen so far: 992 samples\n",
      "0.918921\n",
      "Training loss (for one batch) at step 35: 0.0427\n",
      "Seen so far: 1152 samples\n",
      "0.91900516\n",
      "Training loss (for one batch) at step 40: 0.0195\n",
      "Seen so far: 1312 samples\n",
      "0.91906387\n",
      "Training loss (for one batch) at step 45: 0.3387\n",
      "Seen so far: 1472 samples\n",
      "0.9191055\n",
      "Training loss (for one batch) at step 50: 0.1061\n",
      "Seen so far: 1632 samples\n",
      "0.9191471\n",
      "Training loss (for one batch) at step 55: 0.0552\n",
      "Seen so far: 1792 samples\n",
      "0.9192053\n",
      "Training loss (for one batch) at step 60: 0.1726\n",
      "Seen so far: 1952 samples\n",
      "0.919255\n",
      "Training loss (for one batch) at step 65: 0.1501\n",
      "Seen so far: 2112 samples\n",
      "0.91927946\n",
      "Training loss (for one batch) at step 70: 0.0388\n",
      "Seen so far: 2272 samples\n",
      "0.91933733\n",
      "Training loss (for one batch) at step 75: 0.5150\n",
      "Seen so far: 2432 samples\n",
      "0.9193783\n",
      "Training loss (for one batch) at step 80: 0.0585\n",
      "Seen so far: 2592 samples\n",
      "0.9194358\n",
      "Training loss (for one batch) at step 85: 0.1096\n",
      "Seen so far: 2752 samples\n",
      "0.9194682\n",
      "Training loss (for one batch) at step 90: 0.2214\n",
      "Seen so far: 2912 samples\n",
      "0.9194756\n",
      "Training loss (for one batch) at step 95: 0.0652\n",
      "Seen so far: 3072 samples\n",
      "0.91954106\n",
      "Training loss (for one batch) at step 100: 0.0626\n",
      "Seen so far: 3232 samples\n",
      "0.9195898\n",
      "Training loss (for one batch) at step 105: 0.1493\n",
      "Seen so far: 3392 samples\n",
      "0.91966325\n",
      "Training loss (for one batch) at step 110: 0.1985\n",
      "Seen so far: 3552 samples\n",
      "0.91969514\n",
      "Training loss (for one batch) at step 115: 0.1953\n",
      "Seen so far: 3712 samples\n",
      "0.9197435\n",
      "Training loss (for one batch) at step 120: 0.5798\n",
      "Seen so far: 3872 samples\n",
      "0.9197752\n",
      "Training loss (for one batch) at step 125: 0.0299\n",
      "Seen so far: 4032 samples\n",
      "0.919848\n",
      "Training loss (for one batch) at step 130: 0.0698\n",
      "Seen so far: 4192 samples\n",
      "0.91989595\n",
      "Training loss (for one batch) at step 135: 0.0898\n",
      "Seen so far: 4352 samples\n",
      "0.9199602\n",
      "Training loss (for one batch) at step 140: 0.0352\n",
      "Seen so far: 4512 samples\n",
      "0.9200407\n",
      "Training loss (for one batch) at step 145: 0.1441\n",
      "Seen so far: 4672 samples\n",
      "0.9201128\n",
      "Training loss (for one batch) at step 150: 0.0413\n",
      "Seen so far: 4832 samples\n",
      "0.9201765\n",
      "Training loss (for one batch) at step 155: 0.0135\n",
      "Seen so far: 4992 samples\n",
      "0.92024827\n",
      "Training loss (for one batch) at step 160: 0.1001\n",
      "Seen so far: 5152 samples\n",
      "0.92032796\n",
      "Training loss (for one batch) at step 165: 0.1531\n",
      "Seen so far: 5312 samples\n",
      "0.92035854\n",
      "Training loss (for one batch) at step 170: 0.3122\n",
      "Seen so far: 5472 samples\n",
      "0.92041343\n",
      "Training loss (for one batch) at step 175: 0.0522\n",
      "Seen so far: 5632 samples\n",
      "0.9204682\n",
      "Training loss (for one batch) at step 180: 0.0446\n",
      "Seen so far: 5792 samples\n",
      "0.92054725\n",
      "Training loss (for one batch) at step 185: 0.1822\n",
      "Seen so far: 5952 samples\n",
      "0.92063415\n",
      "Training loss (for one batch) at step 190: 0.4037\n",
      "Seen so far: 6112 samples\n",
      "0.920656\n",
      "Training loss (for one batch) at step 195: 0.1992\n",
      "Seen so far: 6272 samples\n",
      "0.9207102\n",
      "Training loss (for one batch) at step 200: 0.0638\n",
      "Seen so far: 6432 samples\n",
      "0.9207804\n",
      "Training loss (for one batch) at step 205: 0.0926\n",
      "Seen so far: 6592 samples\n",
      "0.9208343\n",
      "Training loss (for one batch) at step 210: 0.1743\n",
      "Seen so far: 6752 samples\n",
      "0.9208719\n",
      "Training loss (for one batch) at step 215: 0.1262\n",
      "Seen so far: 6912 samples\n",
      "0.9209658\n",
      "Training loss (for one batch) at step 220: 0.0253\n",
      "Seen so far: 7072 samples\n",
      "0.9210192\n",
      "Training loss (for one batch) at step 225: 0.0669\n",
      "Seen so far: 7232 samples\n",
      "0.92106444\n",
      "Training loss (for one batch) at step 230: 0.2012\n",
      "Seen so far: 7392 samples\n",
      "0.92111766\n",
      "Training loss (for one batch) at step 235: 0.1149\n",
      "Seen so far: 7552 samples\n",
      "0.9211867\n",
      "Training loss (for one batch) at step 240: 0.0506\n",
      "Seen so far: 7712 samples\n",
      "0.92124754\n",
      "Training loss (for one batch) at step 245: 0.0500\n",
      "Seen so far: 7872 samples\n",
      "0.9212843\n",
      "Training loss (for one batch) at step 250: 0.0126\n",
      "Seen so far: 8032 samples\n",
      "0.92132896\n",
      "Training loss (for one batch) at step 255: 0.2632\n",
      "Seen so far: 8192 samples\n",
      "0.9213575\n",
      "Training loss (for one batch) at step 260: 0.0921\n",
      "Seen so far: 8352 samples\n",
      "0.9214258\n",
      "Training loss (for one batch) at step 265: 0.1107\n",
      "Seen so far: 8512 samples\n",
      "0.9214622\n",
      "Training loss (for one batch) at step 270: 0.1615\n",
      "Seen so far: 8672 samples\n",
      "0.92150635\n",
      "Training loss (for one batch) at step 275: 0.0794\n",
      "Seen so far: 8832 samples\n",
      "0.9215346\n",
      "Training loss (for one batch) at step 280: 0.0045\n",
      "Seen so far: 8992 samples\n",
      "0.92161024\n",
      "Training loss (for one batch) at step 285: 0.1470\n",
      "Seen so far: 9152 samples\n",
      "0.92167777\n",
      "Training loss (for one batch) at step 290: 0.2821\n",
      "Seen so far: 9312 samples\n",
      "0.9217294\n",
      "Training loss (for one batch) at step 295: 0.0288\n",
      "Seen so far: 9472 samples\n",
      "0.9218045\n",
      "Training loss (for one batch) at step 300: 0.2007\n",
      "Seen so far: 9632 samples\n",
      "0.9218794\n",
      "Training loss (for one batch) at step 305: 0.0653\n",
      "Seen so far: 9792 samples\n",
      "0.9219227\n",
      "Training loss (for one batch) at step 310: 0.3527\n",
      "Seen so far: 9952 samples\n",
      "0.92196584\n",
      "Training loss (for one batch) at step 315: 0.0385\n",
      "Seen so far: 10112 samples\n",
      "0.9220325\n",
      "Training loss (for one batch) at step 320: 0.0214\n",
      "Seen so far: 10272 samples\n",
      "0.92210674\n",
      "Training loss (for one batch) at step 325: 0.1855\n",
      "Seen so far: 10432 samples\n",
      "0.92216516\n",
      "Training loss (for one batch) at step 330: 0.3637\n",
      "Seen so far: 10592 samples\n",
      "0.92223907\n",
      "Training loss (for one batch) at step 335: 0.0185\n",
      "Seen so far: 10752 samples\n",
      "0.9223206\n",
      "Training loss (for one batch) at step 340: 0.0656\n",
      "Seen so far: 10912 samples\n",
      "0.92238635\n",
      "Training loss (for one batch) at step 345: 0.3039\n",
      "Seen so far: 11072 samples\n",
      "0.92244416\n",
      "Training loss (for one batch) at step 350: 0.0135\n",
      "Seen so far: 11232 samples\n",
      "0.9225018\n",
      "Training loss (for one batch) at step 355: 0.2951\n",
      "Seen so far: 11392 samples\n",
      "0.92253596\n",
      "Training loss (for one batch) at step 360: 0.2041\n",
      "Seen so far: 11552 samples\n",
      "0.9226089\n",
      "Training loss (for one batch) at step 365: 0.0119\n",
      "Seen so far: 11712 samples\n",
      "0.9226584\n",
      "Training loss (for one batch) at step 370: 0.0229\n",
      "Seen so far: 11872 samples\n",
      "0.9227154\n",
      "Training loss (for one batch) at step 375: 0.0462\n",
      "Seen so far: 12032 samples\n",
      "0.92278016\n",
      "Training loss (for one batch) at step 380: 0.1596\n",
      "Seen so far: 12192 samples\n",
      "0.9228292\n",
      "Training loss (for one batch) at step 385: 0.5232\n",
      "Seen so far: 12352 samples\n",
      "0.92287815\n",
      "Training loss (for one batch) at step 390: 0.0356\n",
      "Seen so far: 12512 samples\n",
      "0.922927\n",
      "Training loss (for one batch) at step 395: 0.0166\n",
      "Seen so far: 12672 samples\n",
      "0.92295265\n",
      "Training loss (for one batch) at step 400: 0.0371\n",
      "Seen so far: 12832 samples\n",
      "0.9229859\n",
      "Training loss (for one batch) at step 405: 0.0346\n",
      "Seen so far: 12992 samples\n",
      "0.92302674\n",
      "Training loss (for one batch) at step 410: 0.0828\n",
      "Seen so far: 13152 samples\n",
      "0.9230675\n",
      "Training loss (for one batch) at step 415: 0.3693\n",
      "Seen so far: 13312 samples\n",
      "0.9230775\n",
      "Training loss (for one batch) at step 420: 0.0737\n",
      "Seen so far: 13472 samples\n",
      "0.9231487\n",
      "Training loss (for one batch) at step 425: 0.3394\n",
      "Seen so far: 13632 samples\n",
      "0.9231662\n",
      "Training loss (for one batch) at step 430: 0.0053\n",
      "Seen so far: 13792 samples\n",
      "0.9232295\n",
      "Training loss (for one batch) at step 435: 0.1367\n",
      "Seen so far: 13952 samples\n",
      "0.923285\n",
      "Training loss (for one batch) at step 440: 0.0216\n",
      "Seen so far: 14112 samples\n",
      "0.92334795\n",
      "Training loss (for one batch) at step 445: 0.1670\n",
      "Seen so far: 14272 samples\n",
      "0.92338794\n",
      "Training loss (for one batch) at step 450: 0.0852\n",
      "Seen so far: 14432 samples\n",
      "0.9234431\n",
      "Training loss (for one batch) at step 455: 0.0138\n",
      "Seen so far: 14592 samples\n",
      "0.9235056\n",
      "Training loss (for one batch) at step 460: 0.1137\n",
      "Seen so far: 14752 samples\n",
      "0.92356044\n",
      "Training loss (for one batch) at step 465: 0.0201\n",
      "Seen so far: 14912 samples\n",
      "0.9236454\n",
      "Training loss (for one batch) at step 470: 0.0355\n",
      "Seen so far: 15072 samples\n",
      "0.92369235\n",
      "Training loss (for one batch) at step 475: 0.0982\n",
      "Seen so far: 15232 samples\n",
      "0.9237316\n",
      "Training loss (for one batch) at step 480: 0.3522\n",
      "Seen so far: 15392 samples\n",
      "0.92377836\n",
      "Training loss (for one batch) at step 485: 0.2543\n",
      "Seen so far: 15552 samples\n",
      "0.92380995\n",
      "Training loss (for one batch) at step 490: 0.0331\n",
      "Seen so far: 15712 samples\n",
      "0.923879\n",
      "Training loss (for one batch) at step 495: 0.0090\n",
      "Seen so far: 15872 samples\n",
      "0.92393297\n",
      "Training loss (for one batch) at step 500: 0.0203\n",
      "Seen so far: 16032 samples\n",
      "0.92400175\n",
      "Training loss (for one batch) at step 505: 0.0222\n",
      "Seen so far: 16192 samples\n",
      "0.92407036\n",
      "Training loss (for one batch) at step 510: 0.0308\n",
      "Seen so far: 16352 samples\n",
      "0.9241463\n",
      "Training loss (for one batch) at step 515: 0.0515\n",
      "Seen so far: 16512 samples\n",
      "0.9242295\n",
      "Training loss (for one batch) at step 520: 0.0097\n",
      "Seen so far: 16672 samples\n",
      "0.92429763\n",
      "Training loss (for one batch) at step 525: 0.1299\n",
      "Seen so far: 16832 samples\n",
      "0.92434317\n",
      "Training loss (for one batch) at step 530: 0.1057\n",
      "Seen so far: 16992 samples\n",
      "0.9243663\n",
      "Training loss (for one batch) at step 535: 0.2878\n",
      "Seen so far: 17152 samples\n",
      "0.9244117\n",
      "Training loss (for one batch) at step 540: 0.0845\n",
      "Seen so far: 17312 samples\n",
      "0.9244867\n",
      "Training loss (for one batch) at step 545: 0.1428\n",
      "Seen so far: 17472 samples\n",
      "0.9245392\n",
      "Training loss (for one batch) at step 550: 0.0104\n",
      "Seen so far: 17632 samples\n",
      "0.92459166\n",
      "Training loss (for one batch) at step 555: 0.0204\n",
      "Seen so far: 17792 samples\n",
      "0.9246588\n",
      "Training loss (for one batch) at step 560: 0.1989\n",
      "Seen so far: 17952 samples\n",
      "0.9247109\n",
      "Training loss (for one batch) at step 565: 0.1733\n",
      "Seen so far: 18112 samples\n",
      "0.92476296\n",
      "Training loss (for one batch) at step 570: 0.0493\n",
      "Seen so far: 18272 samples\n",
      "0.9248075\n",
      "Training loss (for one batch) at step 575: 0.0424\n",
      "Seen so far: 18432 samples\n",
      "0.92484456\n",
      "Training loss (for one batch) at step 580: 0.0768\n",
      "Seen so far: 18592 samples\n",
      "0.92489624\n",
      "Training loss (for one batch) at step 585: 0.1217\n",
      "Seen so far: 18752 samples\n",
      "0.92493314\n",
      "Training loss (for one batch) at step 590: 0.1741\n",
      "Seen so far: 18912 samples\n",
      "0.92497724\n",
      "Training loss (for one batch) at step 595: 0.0270\n",
      "Seen so far: 19072 samples\n",
      "0.9250506\n",
      "Training loss (for one batch) at step 600: 0.0137\n",
      "Seen so far: 19232 samples\n",
      "0.9250945\n",
      "Training loss (for one batch) at step 605: 0.0110\n",
      "Seen so far: 19392 samples\n",
      "0.9251383\n",
      "Training loss (for one batch) at step 610: 0.3632\n",
      "Seen so far: 19552 samples\n",
      "0.9251673\n",
      "Training loss (for one batch) at step 615: 0.1658\n",
      "Seen so far: 19712 samples\n",
      "0.92516714\n",
      "Training loss (for one batch) at step 620: 0.0166\n",
      "Seen so far: 19872 samples\n",
      "0.9251961\n",
      "Training loss (for one batch) at step 625: 0.5451\n",
      "Seen so far: 20032 samples\n",
      "0.92524683\n",
      "Training loss (for one batch) at step 630: 0.2858\n",
      "Seen so far: 20192 samples\n",
      "0.9253048\n",
      "Training loss (for one batch) at step 635: 0.0075\n",
      "Seen so far: 20352 samples\n",
      "0.925377\n",
      "Training loss (for one batch) at step 640: 0.0087\n",
      "Seen so far: 20512 samples\n",
      "0.92543465\n",
      "Training loss (for one batch) at step 645: 0.0090\n",
      "Seen so far: 20672 samples\n",
      "0.9254776\n",
      "Training loss (for one batch) at step 650: 0.3302\n",
      "Seen so far: 20832 samples\n",
      "0.92552775\n",
      "Training loss (for one batch) at step 655: 0.0583\n",
      "Seen so far: 20992 samples\n",
      "0.9255633\n",
      "Training loss (for one batch) at step 660: 0.0272\n",
      "Seen so far: 21152 samples\n",
      "0.92563486\n",
      "Training loss (for one batch) at step 665: 0.0645\n",
      "Seen so far: 21312 samples\n",
      "0.92569184\n",
      "Training loss (for one batch) at step 670: 0.0241\n",
      "Seen so far: 21472 samples\n",
      "0.9257559\n",
      "Training loss (for one batch) at step 675: 0.1020\n",
      "Seen so far: 21632 samples\n",
      "0.925791\n",
      "Training loss (for one batch) at step 680: 0.0801\n",
      "Seen so far: 21792 samples\n",
      "0.925862\n",
      "Training loss (for one batch) at step 685: 0.0044\n",
      "Seen so far: 21952 samples\n",
      "0.92591846\n",
      "Training loss (for one batch) at step 690: 0.0595\n",
      "Seen so far: 22112 samples\n",
      "0.92595327\n",
      "Training loss (for one batch) at step 695: 0.0824\n",
      "Seen so far: 22272 samples\n",
      "0.9260167\n",
      "Training loss (for one batch) at step 700: 0.0536\n",
      "Seen so far: 22432 samples\n",
      "0.9260585\n",
      "Training loss (for one batch) at step 705: 0.0050\n",
      "Seen so far: 22592 samples\n",
      "0.9261073\n",
      "Training loss (for one batch) at step 710: 0.1867\n",
      "Seen so far: 22752 samples\n",
      "0.92614883\n",
      "Training loss (for one batch) at step 715: 0.2534\n",
      "Seen so far: 22912 samples\n",
      "0.92619747\n",
      "Training loss (for one batch) at step 720: 0.4278\n",
      "Seen so far: 23072 samples\n",
      "0.9262175\n",
      "Training loss (for one batch) at step 725: 0.1099\n",
      "Seen so far: 23232 samples\n",
      "0.92625165\n",
      "Training loss (for one batch) at step 730: 0.0023\n",
      "Seen so far: 23392 samples\n",
      "0.92631423\n",
      "Training loss (for one batch) at step 735: 0.0335\n",
      "Seen so far: 23552 samples\n",
      "0.9263624\n",
      "Training loss (for one batch) at step 740: 0.0519\n",
      "Seen so far: 23712 samples\n",
      "0.92643887\n",
      "Training loss (for one batch) at step 745: 0.0023\n",
      "Seen so far: 23872 samples\n",
      "0.92647976\n",
      "Training loss (for one batch) at step 750: 0.0002\n",
      "Seen so far: 24032 samples\n",
      "0.92654175\n",
      "Training loss (for one batch) at step 755: 0.0019\n",
      "Seen so far: 24192 samples\n",
      "0.9265824\n",
      "Training loss (for one batch) at step 760: 0.0202\n",
      "Seen so far: 24352 samples\n",
      "0.92664415\n",
      "Training loss (for one batch) at step 765: 0.0092\n",
      "Seen so far: 24512 samples\n",
      "0.9266776\n",
      "Training loss (for one batch) at step 770: 0.0949\n",
      "Seen so far: 24672 samples\n",
      "0.9267391\n",
      "Training loss (for one batch) at step 775: 0.0755\n",
      "Seen so far: 24832 samples\n",
      "0.9268145\n",
      "Training loss (for one batch) at step 780: 0.0011\n",
      "Seen so far: 24992 samples\n",
      "0.9268757\n",
      "Training loss (for one batch) at step 785: 0.1154\n",
      "Seen so far: 25152 samples\n",
      "0.92692274\n",
      "Training loss (for one batch) at step 790: 0.0195\n",
      "Seen so far: 25312 samples\n",
      "0.9269767\n",
      "Training loss (for one batch) at step 795: 0.2946\n",
      "Seen so far: 25472 samples\n",
      "0.9270025\n",
      "Training loss (for one batch) at step 800: 0.0598\n",
      "Seen so far: 25632 samples\n",
      "0.9270492\n",
      "Training loss (for one batch) at step 805: 0.3771\n",
      "Seen so far: 25792 samples\n",
      "0.92710286\n",
      "Training loss (for one batch) at step 810: 0.2424\n",
      "Seen so far: 25952 samples\n",
      "0.9271424\n",
      "Training loss (for one batch) at step 815: 0.0960\n",
      "Seen so far: 26112 samples\n",
      "0.9271888\n",
      "Training loss (for one batch) at step 820: 0.0332\n",
      "Seen so far: 26272 samples\n",
      "0.9272421\n",
      "Training loss (for one batch) at step 825: 0.2580\n",
      "Seen so far: 26432 samples\n",
      "0.9272883\n",
      "Training loss (for one batch) at step 830: 0.0048\n",
      "Seen so far: 26592 samples\n",
      "0.92733437\n",
      "Training loss (for one batch) at step 835: 0.0773\n",
      "Seen so far: 26752 samples\n",
      "0.92738736\n",
      "Training loss (for one batch) at step 840: 0.0455\n",
      "Seen so far: 26912 samples\n",
      "0.9274263\n",
      "Training loss (for one batch) at step 845: 0.2807\n",
      "Seen so far: 27072 samples\n",
      "0.92746514\n",
      "Training loss (for one batch) at step 850: 0.0952\n",
      "Seen so far: 27232 samples\n",
      "0.92751086\n",
      "Training loss (for one batch) at step 855: 0.1634\n",
      "Seen so far: 27392 samples\n",
      "0.9275703\n",
      "Training loss (for one batch) at step 860: 0.1989\n",
      "Seen so far: 27552 samples\n",
      "0.92761576\n",
      "Training loss (for one batch) at step 865: 0.1204\n",
      "Seen so far: 27712 samples\n",
      "0.9276612\n",
      "Training loss (for one batch) at step 870: 0.0394\n",
      "Seen so far: 27872 samples\n",
      "0.92772025\n",
      "Training loss (for one batch) at step 875: 0.0593\n",
      "Seen so far: 28032 samples\n",
      "0.9277723\n",
      "Training loss (for one batch) at step 880: 0.2665\n",
      "Seen so far: 28192 samples\n",
      "0.9277967\n",
      "Training loss (for one batch) at step 885: 0.0938\n",
      "Seen so far: 28352 samples\n",
      "0.9278486\n",
      "Training loss (for one batch) at step 890: 0.1733\n",
      "Seen so far: 28512 samples\n",
      "0.9279003\n",
      "Training loss (for one batch) at step 895: 0.0031\n",
      "Seen so far: 28672 samples\n",
      "0.92795193\n",
      "Training loss (for one batch) at step 900: 0.0022\n",
      "Seen so far: 28832 samples\n",
      "0.9280103\n",
      "Training loss (for one batch) at step 905: 0.0351\n",
      "Seen so far: 28992 samples\n",
      "0.92805487\n",
      "Training loss (for one batch) at step 910: 0.0037\n",
      "Seen so far: 29152 samples\n",
      "0.92812663\n",
      "Training loss (for one batch) at step 915: 0.0054\n",
      "Seen so far: 29312 samples\n",
      "0.9281778\n",
      "Training acc over epoch: 0.9282\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 코드\n",
    "\n",
    "train_path = \"/aiffel/aiffel/model-fit/data/30vnfoods/Train\"\n",
    "\n",
    "epoch = 5\n",
    "batch = 32\n",
    "\n",
    "model = Model(num_classes=10)\n",
    "dataset = load_data(data_path=train_path, batch_size=batch)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "trainer = Trainer(model=model,\n",
    "                epochs=epoch,\n",
    "                batch=batch,\n",
    "                #ds_length=train_length,\n",
    "                loss_fn=loss_function,\n",
    "                optimizer=optimizer)\n",
    "\n",
    "trainer.train(train_dataset=dataset,\n",
    "            train_metric=train_acc_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9d22a",
   "metadata": {},
   "source": [
    "### ㄴ 결과\n",
    "\n",
    "TensorFlow GradientTape를 이용해서 custom trainer 클래스를 구현하였고,\n",
    "\n",
    "학습과정에서 training accuracy가 점차 증가하는 것을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ef607",
   "metadata": {},
   "source": [
    "### Test 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a4ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/32\n",
      "29/32\n",
      "27/32\n",
      "30/32\n",
      "30/32\n",
      "28/32\n",
      "30/32\n",
      "26/32\n",
      "30/32\n",
      "29/32\n"
     ]
    }
   ],
   "source": [
    "# 모델 테스트 코드\n",
    "\n",
    "test_ds = load_data(data_path=test_path)\n",
    "\n",
    "for step_train, (x_batch_train, y_batch_train) in enumerate(test_ds.take(10)):\n",
    "    prediction = model(x_batch_train)\n",
    "    print(\"{}/{}\".format(np.array(tf.equal(tf.argmax(y_batch_train, axis=1), tf.argmax(prediction, axis=1))).sum(), tf.argmax(y_batch_train, axis=1).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff88b6c",
   "metadata": {},
   "source": [
    "ㄴ 평균적으로 약 90%에 달하는 예측성능을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2826229",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- 배운 점\n",
    "1. low-level 단계에서부터 신경망을 학습하는 과정을 구현하면서 지금까지 이해가 부족했던 코드에 대한 이해도를 보충할 수 있었다.\n",
    "2. tf.GradientTape() 함수를 통해 gradient를 구하고, optimizer.apply_gradients() 함수를 통하여 가중치를 갱신하는 과정을 구현할 수 있었다.\n",
    "3. load_data() 함수와 prepare_for_training() 함수를 직접 코드로 구현하면서 배치단위로 데이터를 가져오는 과정, 셔플링, 학습전 배치단위로의 샘플링, prefetch와 캐싱을 통한 속도 향상, 메모리 문제해결 과정을 이해할 수 있었다. 이전 노드에서 이와 같은 방법으로 코드가 구현된 것을 많이 봤지만, 이번 프로젝트를 수행하면서 부족한 이해를 채워 넣을 수 있었다.\n",
    "\n",
    "- 문제해결\n",
    "1. 학습과정에서 훈련이 무한정 진행되는 문제가 발생하였다. prepare_for_training 함수 안에 repeat() 메서드의 인자를 추가해야 정해진 만큼 반복을 진행하는 것을 파악하였다.\n",
    "2. 이번 태스크에서는 Trainer() 함수 안에 ds_length 인자가 필요하지 않으므로 삭제하였다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
