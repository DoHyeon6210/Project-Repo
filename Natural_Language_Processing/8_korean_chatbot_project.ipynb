{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21567c77",
   "metadata": {},
   "source": [
    "# 8-1. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
    "\n",
    "시작하기 전에 우선 주요 라이브러리 버전을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4c4966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf3c77",
   "metadata": {},
   "source": [
    "### Transformer 구조 생성 함수 및 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00db5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45f8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ed725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93448da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 헤드 어센션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192b319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 마스크\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fef784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩어헤드 마스크\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ef0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a82e2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 생성 함수\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbca79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d93a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 구현 함수\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f1f73",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 수집하기\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "\n",
    "Cloud shell에서 아래 명령어를 입력해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e65ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ~/aiffel/Natural_Language_Processing/8_project/data/\n",
    "# !ln -s ~/data/* ~/aiffel/Natural_Language_Processing/8_project/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40abc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_data = pd.read_csv('data/ChatbotData.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2f5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 184.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data[['Q', 'A']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78d40f",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfae53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문, 답 데이터 전처리 및 나눠서 저장하기\n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9049e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네', 'SD카드 망가졌어', 'SD카드 안돼', 'SNS 맞팔 왜 안하지ㅠㅠ', 'SNS 시간낭비인 거 아는데 매일 하는 중', 'SNS 시간낭비인데 자꾸 보게됨', 'SNS보면 나만 빼고 다 행복해보여', '가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다 .', '가스비 비싼데 감기 걸리겠어', '가스비 장난 아님', '가장 확실한 건 뭘까 ?', '가족 여행 가기로 했어', '가족 여행 고고', '가족 여행 어디로 가지 ?', '가족 있어 ?', '가족관계 알려 줘', '가족끼리 여행간다 .', '가족들 보고 싶어', '가족들이랑 서먹해', '가족들이랑 서먹해졌어', '가족들이랑 어디 가지 ?', '가족들이랑 여행 갈거야', '가족여행 가야지', '가족이 누구야 ?', '가족이랑 여행 가려고', '가족한테 스트레스 풀었어', '가출할까 ?', '가출해도 갈 데가 없어', '간만에 떨리니까 좋더라', '간만에 쇼핑 중', '간만에 휴식 중', '간식 뭐 먹을까', '간식 추천', '간장치킨 시켜야지', '간접흡연 싫어', '갈까 말까 고민 돼', '갈까 말까 ?', '감 말랭이 먹고 싶다 .', '감 말랭이 먹어야지', '감기 같애', '감기 걸린 것 같아', '감기 기운이 있어', '감기 들 거 같애', '감기가 오려나', '감기약이 없어', '감기인거 같애', '감미로운 목소리 좋아', '감정이 쓰레기통처럼 엉망진창이야', '감정컨트롤을 못하겠어', '감정컨트롤이 안돼', '감히 나를 무시하는 애가 있어', '갑자기 나쁜 생각이 막 들더라', '갑자기 눈물 나', '갑자기 물어봐서 당황했어', '갑자기 불편한 사이가 된 거 같아', '강렬한 첫인상 남겨야 하는데', '강아지 키우고 싶어', '강아지 키우고 싶은데 역시 안돼겠지', '강아지 키울 수 있을까', '강아지 키울까', '강원도 가서 살까 ?', '같이 게임하자고 해도 되나 ?', '같이 놀러갈 친구가 없어', '같이 먹었는데 나만 살찐 거 같아', '같이 수영장 가기로 했어', '같이 있으면 힘든데 붙잡고 싶어', '같이 피씨방 가자고 해볼까 ?', '같이 할 수 있는 취미 생활 뭐 있을까', '개강룩 입어볼까', '개강옷 예쁘게 입어 볼까', '개강이다', '개강이라니', '개같은 상황', '개같이 되버렸어 .', '개기름 꼈어', '개념도 놓고 옴', '개념이 없어', '개당황', '개당황했잖아 갑자기 물어 봐서', '개인적인 업무까지 다 시켜', '개인적인 일도 다 시켜', '개졸려', '개좋아', '개학하니까 좋다', '걔 너무 싫다', '걔는 누굴 닮아서 그런거니 ?', '걔랑 같은 반 됐으면 좋겠다', '거지 같이 일해 놓고 갔어']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .', '다시 새로 사는 게 마음 편해요 .', '다시 새로 사는 게 마음 편해요 .', '잘 모르고 있을 수도 있어요 .', '시간을 정하고 해보세요 .', '시간을 정하고 해보세요 .', '자랑하는 자리니까요 .', '그 사람도 그럴 거예요 .', '그 사람도 그럴 거예요 .', '혼자를 즐기세요 .', '돈은 다시 들어올 거예요 .', '땀을 식혀주세요 .', '어서 잊고 새출발 하세요 .', '빨리 집에 돌아가서 끄고 나오세요 .', '빨리 집에 돌아가서 끄고 나오세요 .', '다음 달에는 더 절약해봐요 .', '따뜻하게 사세요 !', '다음 달에는 더 절약해봐요 .', '가장 확실한 시간은 오늘이에요 .  어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '더 가까워질 기회가 되겠네요 .', '저도요 .', '다들 바빠서 이야기할 시간이 부족했나봐요 .', '다들 바빠서 이야기할 시간이 부족했나봐요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '좋은 생각이에요 .', '더 가까워질 기회가 되겠네요 .', '저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '좋은 생각이에요 .', '정말 후회할 습관이에요 .', '무모한 결정을 내리지 마세요 .', '선생님이나 기관에 연락해보세요 .', '떨리는 감정은 그 자체로 소중해요 .', '득템했길 바라요 .', '휴식도 필요하죠 .', '단짠으로 두 개 사는게 진리죠 .', '단짠으로 두 개 사는게 진리죠 .', '맛있게 드세요 .', '저도 싫어요 .', '가세요 .', '가세요 .', '맛있게 드세요 .', '맛있게 드세요 .', '병원가세요 .', '이럴 때 잘 쉬는 게 중요해요 .', '이럴 때 잘 쉬는 게 중요해요 .', '이럴 때 잘 쉬는 게 중요해요 .', '따뜻하게 관리하세요 .', '병원가세요 .', '병원가세요 .', '저도 듣고 싶네요 .', '자신을 더 사랑해주세요 .', '그건 습관이에요 .', '그건 습관이에요 .', '콕 집어서 물어보세요 .', '좋은 생각만 하세요 .', '마음이 아픈가요 .', '갑작스러웠나봐요 .', '관계의 변화가 왔나봅니다 .', '처음 3초가 중요해요 .  당신의 매력을 어필해보세요 .', '책임질 수 있을 때 키워 보세요 .', '먼저 생활패턴을 살펴 보세요 .', '먼저 생활패턴을 살펴 보세요 .', '책임질 수 있을 때 키워 보세요 .', '아름다운 곳이죠 .', '안 될 것도 없죠 .', '혼자도 좋아요 .', '연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요 .', '즐거운 시간 보내고 오세요 !', '질질 끌지 마세요 .', '말해보세요 .', '함께하면 서로를 더 많이 알게 될 거예요 .', '개시해보세요 .', '개시해보세요 .', '곧 방학이예요 .', '방학이 참 짧죠 .', '벗어나는 게 좋겠네요 .', '벗어나는 게 좋겠네요 .', '세수하고 오세요 .', '그게 제일 중요한 건데요 .', '그게 제일 중요한 건데요 .', '다음부터는 더 많이 아세요 .', '갑작스러웠나봐요 .', '공적인 일부터 하세요 .', '공적인 일부터 하세요 .', '낮잠을 잠깐 자도 괜찮아요 .', '저도 좋아해주세요 .', '친구들이 보고싶었나봐요 .', '되도록 만나지 마세요 .', '당신이요 .', '당신의 운을 믿어보세요 .', '일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠 .']\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(questions[:100])\n",
    "print(answers[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07165cdd",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b4e7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1150921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b540320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8178]\n",
      "END_TOKEN의 번호 : [8179]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27d9cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8180\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff6468",
   "metadata": {},
   "source": [
    "### 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)\n",
    "위에서 tensorflow_datasets의 SubwordTextEncoder를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다면, tokenizer.encode()로 각 단어를 정수로 변환할 수 있고 또는 tokenizer.decode()를 통해 정수 시퀀스를 단어 시퀀스로 변환할 수 있습니다.\n",
    "\n",
    "예를 들어서 22번째 샘플을 tokenizer.encode()의 입력으로 사용해서 변환 결과를 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a089aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5766, 611, 2495, 4167]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2359, 7516, 7, 6279, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4281085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 20\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa9db915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 MAX_LENGTH로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='pre')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='pre')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db5d8ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8180\n",
      "필터링 후의 질문 샘플 개수: 11791\n",
      "필터링 후의 답변 샘플 개수: 11791\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af214f68",
   "metadata": {},
   "source": [
    "질문과 답변의 쌍을 tf.data.Dataset API의 입력으로 사용하여 파이프라인을 구성합니다. 이때, 교사 강요를 위해서 answers[:, :-1]를 디코더의 입력값, answers[:, 1:]를 디코더의 레이블로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9ebe21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92ac09",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d427f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8d81e",
   "metadata": {},
   "source": [
    "### 1. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e47a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3148288     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3675648     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8180)   2102260     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,926,196\n",
      "Trainable params: 8,926,196\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.2 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d3308",
   "metadata": {},
   "source": [
    "### 2. 손실 함수(Loss function)\n",
    "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d01d6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa8a06",
   "metadata": {},
   "source": [
    "### 3. 커스텀 된 학습률(Learning rate) 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "285c3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114e540",
   "metadata": {},
   "source": [
    "### 4. 모델 컴파일\n",
    "손실 함수와 커스텀 된 학습률(learning rate)을 사용하여 모델을 컴파일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c559971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c8b4d",
   "metadata": {},
   "source": [
    "### 5. 훈련하기\n",
    "이제 학습을 진행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c664c132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 3060   41 8179]\n",
      " [   0    0    0 ...  998 1716 8179]\n",
      " [   0    0    0 ... 3652   67 8179]\n",
      " ...\n",
      " [   0    0    0 ...  286    2 8179]\n",
      " [   0    0    0 ... 2106  237 8179]\n",
      " [   0    0    0 ... 1114 3761 8179]]\n",
      "[[   0    0    0 ... 7894    1 8179]\n",
      " [   0    0    0 ... 5502    1 8179]\n",
      " [   0    0    0 ...  131    1 8179]\n",
      " ...\n",
      " [   0    0    0 ...   17    1 8179]\n",
      " [   0    0    0 ... 2321    1 8179]\n",
      " [   0    0    0 ...  753    1 8179]]\n"
     ]
    }
   ],
   "source": [
    "# MAX_LEN에 따른 데이터 크기 상태 확인\n",
    "print(questions[:100])\n",
    "print(answers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d83cb5fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "93/93 [==============================] - 11s 53ms/step - loss: 3.5733 - accuracy: 0.0388\n",
      "Epoch 2/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 3.1825 - accuracy: 0.1118\n",
      "Epoch 3/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 2.7497 - accuracy: 0.1538\n",
      "Epoch 4/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 2.3531 - accuracy: 0.1547\n",
      "Epoch 5/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 2.0752 - accuracy: 0.1574\n",
      "Epoch 6/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.9315 - accuracy: 0.1608\n",
      "Epoch 7/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.8410 - accuracy: 0.1654\n",
      "Epoch 8/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.7686 - accuracy: 0.1703\n",
      "Epoch 9/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.7017 - accuracy: 0.1744\n",
      "Epoch 10/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.6361 - accuracy: 0.1786\n",
      "Epoch 11/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.5661 - accuracy: 0.1837\n",
      "Epoch 12/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.4940 - accuracy: 0.1904\n",
      "Epoch 13/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 1.4137 - accuracy: 0.1987\n",
      "Epoch 14/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 1.3313 - accuracy: 0.2079\n",
      "Epoch 15/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 1.2432 - accuracy: 0.2182\n",
      "Epoch 16/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 1.1532 - accuracy: 0.2284\n",
      "Epoch 17/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 1.0615 - accuracy: 0.2389\n",
      "Epoch 18/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.9681 - accuracy: 0.2512\n",
      "Epoch 19/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.8744 - accuracy: 0.2632\n",
      "Epoch 20/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.7813 - accuracy: 0.2768\n",
      "Epoch 21/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.6878 - accuracy: 0.2903\n",
      "Epoch 22/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.5994 - accuracy: 0.3030\n",
      "Epoch 23/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.5133 - accuracy: 0.3163\n",
      "Epoch 24/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.4347 - accuracy: 0.3292\n",
      "Epoch 25/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.3623 - accuracy: 0.3413\n",
      "Epoch 26/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.2961 - accuracy: 0.3541\n",
      "Epoch 27/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.2404 - accuracy: 0.3644\n",
      "Epoch 28/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.1924 - accuracy: 0.3734\n",
      "Epoch 29/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.1536 - accuracy: 0.3805\n",
      "Epoch 30/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.1225 - accuracy: 0.3861\n",
      "Epoch 31/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.1003 - accuracy: 0.3897\n",
      "Epoch 32/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0822 - accuracy: 0.3932\n",
      "Epoch 33/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0726 - accuracy: 0.3941\n",
      "Epoch 34/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0632 - accuracy: 0.3958\n",
      "Epoch 35/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0560 - accuracy: 0.3972\n",
      "Epoch 36/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0527 - accuracy: 0.3976\n",
      "Epoch 37/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0495 - accuracy: 0.3980\n",
      "Epoch 38/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0457 - accuracy: 0.3988\n",
      "Epoch 39/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0434 - accuracy: 0.3990\n",
      "Epoch 40/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0429 - accuracy: 0.3990\n",
      "Epoch 41/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0436 - accuracy: 0.3988\n",
      "Epoch 42/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0415 - accuracy: 0.3989\n",
      "Epoch 43/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0382 - accuracy: 0.3999\n",
      "Epoch 44/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0381 - accuracy: 0.3999\n",
      "Epoch 45/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0372 - accuracy: 0.4002\n",
      "Epoch 46/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0342 - accuracy: 0.4009\n",
      "Epoch 47/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0309 - accuracy: 0.4016\n",
      "Epoch 48/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0316 - accuracy: 0.4015\n",
      "Epoch 49/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0286 - accuracy: 0.4023\n",
      "Epoch 50/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0260 - accuracy: 0.4029\n",
      "Epoch 51/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0248 - accuracy: 0.4031\n",
      "Epoch 52/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0240 - accuracy: 0.4032\n",
      "Epoch 53/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0230 - accuracy: 0.4037\n",
      "Epoch 54/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0213 - accuracy: 0.4040\n",
      "Epoch 55/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0211 - accuracy: 0.4042\n",
      "Epoch 56/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0193 - accuracy: 0.4046\n",
      "Epoch 57/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0179 - accuracy: 0.4051\n",
      "Epoch 58/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0176 - accuracy: 0.4050\n",
      "Epoch 59/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0168 - accuracy: 0.4053\n",
      "Epoch 60/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0154 - accuracy: 0.4056\n",
      "Epoch 61/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0150 - accuracy: 0.4058\n",
      "Epoch 62/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0155 - accuracy: 0.4056\n",
      "Epoch 63/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0148 - accuracy: 0.4057\n",
      "Epoch 64/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0140 - accuracy: 0.4060\n",
      "Epoch 65/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0134 - accuracy: 0.4062\n",
      "Epoch 66/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0128 - accuracy: 0.4064\n",
      "Epoch 67/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0126 - accuracy: 0.4064\n",
      "Epoch 68/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0133 - accuracy: 0.4062\n",
      "Epoch 69/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0117 - accuracy: 0.4066\n",
      "Epoch 70/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0114 - accuracy: 0.4067\n",
      "Epoch 71/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0109 - accuracy: 0.4068\n",
      "Epoch 72/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0105 - accuracy: 0.4069\n",
      "Epoch 73/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0104 - accuracy: 0.4069\n",
      "Epoch 74/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0102 - accuracy: 0.4070\n",
      "Epoch 75/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0103 - accuracy: 0.4070\n",
      "Epoch 76/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0096 - accuracy: 0.4072\n",
      "Epoch 77/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0097 - accuracy: 0.4071\n",
      "Epoch 78/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0085 - accuracy: 0.4074\n",
      "Epoch 79/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0090 - accuracy: 0.4074\n",
      "Epoch 80/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0090 - accuracy: 0.4072\n",
      "Epoch 81/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0079 - accuracy: 0.4076\n",
      "Epoch 82/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0085 - accuracy: 0.4073\n",
      "Epoch 83/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0079 - accuracy: 0.4074\n",
      "Epoch 84/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0080 - accuracy: 0.4075\n",
      "Epoch 85/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0072 - accuracy: 0.4077\n",
      "Epoch 86/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0078 - accuracy: 0.4075\n",
      "Epoch 87/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0073 - accuracy: 0.4077\n",
      "Epoch 88/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0072 - accuracy: 0.4078\n",
      "Epoch 89/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0074 - accuracy: 0.4076\n",
      "Epoch 90/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0069 - accuracy: 0.4077\n",
      "Epoch 91/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0067 - accuracy: 0.4079\n",
      "Epoch 92/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0066 - accuracy: 0.4079\n",
      "Epoch 93/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0066 - accuracy: 0.4080\n",
      "Epoch 94/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0064 - accuracy: 0.4080\n",
      "Epoch 95/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0060 - accuracy: 0.4080\n",
      "Epoch 96/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0071 - accuracy: 0.4077\n",
      "Epoch 97/100\n",
      "93/93 [==============================] - 5s 53ms/step - loss: 0.0058 - accuracy: 0.4081\n",
      "Epoch 98/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0052 - accuracy: 0.4083\n",
      "Epoch 99/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0060 - accuracy: 0.4080\n",
      "Epoch 100/100\n",
      "93/93 [==============================] - 5s 54ms/step - loss: 0.0054 - accuracy: 0.4082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70947c9220>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d0a2e",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f53b5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 동일한 전처리를 수행하는 함수 정의\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa88511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989571b7",
   "metadata": {},
   "source": [
    "임의의 입력 문장에 대해서 decoder_inference() 함수를 호출하여 챗봇의 대답을 얻는 sentence_generation() 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f13f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e069716",
   "metadata": {},
   "source": [
    "임의의 문장으로부터 챗봇의 대답을 얻어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d564eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요.\n",
      "출력 : 안녕하세요옛 화살은 부메랑이 되어 돌아오기도 하죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요옛 화살은 부메랑이 되어 돌아오기도 하죠 .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('안녕하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bbbabcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 12시 땡!\n",
      "출력 : 하루가 또 가네요 .  안부 생각할 걸 수도 있어요 .  그렇지만 조금 더 상황을 지켜보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'하루가 또 가네요 .  안부 생각할 걸 수도 있어요 .  그렇지만 조금 더 상황을 지켜보세요 .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"12시 땡!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c326f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 1지망 학교 떨어졌어.\n",
      "출력 : 기억 쏜 화살은 부편에 따라 다르겠지만 하는 게 좋겠죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'기억 쏜 화살은 부편에 따라 다르겠지만 하는 게 좋겠죠 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"1지망 학교 떨어졌어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "565c2c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 3박4일 놀러가고 싶다.\n",
      "출력 : 여행은 언제나 좋죠기만 하다가 놓치는 것보다 훨씬 낫습니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'여행은 언제나 좋죠기만 하다가 놓치는 것보다 훨씬 낫습니다 .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"3박4일 놀러가고 싶다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b1fc7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : PPL 심하네.\n",
      "출력 : 눈살이 찌푸려지거나 그런 게 아니라 눈살이 찌푸려지거나 할 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'눈살이 찌푸려지거나 그런 게 아니라 눈살이 찌푸려지거나 할 거예요 .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"PPL 심하네.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5ece196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : SD카드 망가졌어\n",
      "출력 : 다시 새로 사는 게 마음 편해게 마음 편해진 않아요 .  도움이 할 텐데 안타깝네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'다시 새로 사는 게 마음 편해게 마음 편해진 않아요 .  도움이 할 텐데 안타깝네요 .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"SD카드 망가졌어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aaa20158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 가족관계 알려 줘\n",
      "출력 : 저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요 .  그런 것 같습니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요 .  그런 것 같습니다 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"가족관계 알려 줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "928f5db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 결혼식 가기 귀찮아\n",
      "출력 : 경�사는 것보다 자책하지 말아요 .  그런 사람이 아닌 것 보단 연락하고 관심을 거둬보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'경�사는 것보다 자책하지 말아요 .  그런 사람이 아닌 것 보단 연락하고 관심을 거둬보세요 .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"결혼식 가기 귀찮아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58891852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 계속 보고 싶으면 어떡해?\n",
      "출력 : 보러 갈 수 있을 수도 있지만 마음을 확인하연애 하다가 직접 물어보는게 좋겠어요 .  다가가는 건 어떨까요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'보러 갈 수 있을 수도 있지만 마음을 확인하연애 하다가 직접 물어보는게 좋겠어요 .  다가가는 건 어떨까요 .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"계속 보고 싶으면 어떡해?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfa66c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 공부 꼭 해야 할까\n",
      "출력 : 미래의 배우자가 달라져요 .  같이 살은 꼭 그녀가 만큼 갑진 건 없어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'미래의 배우자가 달라져요 .  같이 살은 꼭 그녀가 만큼 갑진 건 없어요 .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"공부 꼭 해야 할까\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecba13d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 금연이 쉽지 않아\n",
      "출력 : 자신을 이겨야해요 .  새로운 관계를 병원 다녀와요 .  힘들 때 비하면 좀 멀리 가리죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'자신을 이겨야해요 .  새로운 관계를 병원 다녀와요 .  힘들 때 비하면 좀 멀리 가리죠 .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"금연이 쉽지 않아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17e63eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나만 없어 고양이\n",
      "출력 : 강가 같이 분위기 이더라고요 .  좋은 기억이 선천이에 따라 얼마든지 바뀔 수 있어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'강가 같이 분위기 이더라고요 .  좋은 기억이 선천이에 따라 얼마든지 바뀔 수 있어요 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"나만 없어 고양이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "749d2b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 갖고 싶어 고양이\n",
      "출력 : 생기는 것보다 하면 함께 말할 수 없는 가며 하면 돼요 .  당신은 위로받을 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'생기는 것보다 하면 함께 말할 수 없는 가며 하면 돼요 .  당신은 위로받을 거예요 .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"갖고 싶어 고양이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2390cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너 고양이 있어?\n",
      "출력 : 가족들과 함께할때 가장 자신다운 모습으로 살 수 있는 사람이랑 하는 게 좋은 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'가족들과 함께할때 가장 자신다운 모습으로 살 수 있는 사람이랑 하는 게 좋은 거예요 .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"너 고양이 있어?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2930fa3",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 배운 점\n",
    "1. transformer 모델의 핵심 아이디어와 구조에 대하여 대략적으로 이해할 수 있었다.\n",
    "2. transformer 모델 구조를 코드로 구현할 수 있었다.\n",
    "3. transformer 모델에 데이터를 입력하기 전 텍스트 데이터의 전처리 단계에 대해 배울 수 있었다. 정규표현식을 활용해서 구두점과 문자를 분리하고, 필요에 따라 특정 문자를 제거하는 방법을 활용하였다. \n",
    "\n",
    "- 문제해결 시도\n",
    "1. MAX_LEN 이 40 인 경우 대부분의 텍스트 데이터에 패딩값이 포함되어 있는 것을 확인하였다. 최대길이를 20으로 제한하여 모델링을 진행할때는 학습 속도에 큰 향상이 있었다.\n",
    "2. batch size를 128, dropout 비율을 0.2로 시도했을 때 정확도와 학습 속도에 향상이 있었다.\n",
    "3. padding 설정을 post에서 pre로 바꿨을 때 성능 향상에 진전이 있었다. transformer는 데이터를 병렬적이게 처리한다고 하였는데, padding 설정에 따른 차이에 대하여 질문을 남겨놓았다.\n",
    "4. 학습 데이터가 11000여개 뿐이기 때문에, 주어진 데이터 외 다른 질문은 답을 못하는 듯 하다. MAX_LEN에 따른 정확도 문제도 있는 것으로 확인했다.\n",
    "5. NUM_LAYERS를 2에서 4로 확장시켜 학습을 시도하였다. MAX_LEN=20인 경우, 모델의 깊이를 추가했을 때 accuracy 향상 폭이 확실히 개선되었다.\n",
    "6. MAX_LEN이 40인 경우 최대 정확도는 20%, 20인 경우 40%로 나왔다. 문장길이가 길다고 해서 좋은 답이 나오지는 않았다. 체감상 문맥에 맞는 답변이 반반 되는 것 같다.\n",
    "7. 가장 양호한 결과는 레이어 2, 드롭아웃 0.2, MAX_LEN=20, batch size=128 정도로 나온다.\n",
    "\n",
    "- 부족한 점\n",
    "1. 노드에서도 transformer과 다른 텍스트 처리 아키텍쳐에 대해 간략하게 설명한 것 같다. 필요에 따라 더 구체적으로 조사하고 공부해야할 필요성을 느꼈다.\n",
    "2. 텍스트 데이터를 불러와 전처리하는 과정에서 코드를 명확하게 이해해야할 필요성을 느꼈다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
