{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fdf4cd",
   "metadata": {},
   "source": [
    "# 8-1. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
    "\n",
    "시작하기 전에 우선 주요 라이브러리 버전을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a550aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a74c22",
   "metadata": {},
   "source": [
    "### Transformer 구조 생성 함수 및 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2076c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d0e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f03545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4693ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 헤드 어센션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b216a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 마스크\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05af81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩어헤드 마스크\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbb7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac09986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 생성 함수\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704178a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8acc8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 구현 함수\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff39040",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 수집하기\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "\n",
    "Cloud shell에서 아래 명령어를 입력해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6050d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ~/aiffel/Natural_Language_Processing/8_project/data/\n",
    "# !ln -s ~/data/* ~/aiffel/Natural_Language_Processing/8_project/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "389f3fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_data = pd.read_csv('data/ChatbotData.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "161db461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 184.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data[['Q', 'A']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abcdc3",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f23f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문, 답 데이터 전처리 및 나눠서 저장하기\n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1be108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네', 'SD카드 망가졌어', 'SD카드 안돼', 'SNS 맞팔 왜 안하지ㅠㅠ', 'SNS 시간낭비인 거 아는데 매일 하는 중', 'SNS 시간낭비인데 자꾸 보게됨', 'SNS보면 나만 빼고 다 행복해보여', '가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다 .', '가스비 비싼데 감기 걸리겠어', '가스비 장난 아님', '가장 확실한 건 뭘까 ?', '가족 여행 가기로 했어', '가족 여행 고고', '가족 여행 어디로 가지 ?', '가족 있어 ?', '가족관계 알려 줘', '가족끼리 여행간다 .', '가족들 보고 싶어', '가족들이랑 서먹해', '가족들이랑 서먹해졌어', '가족들이랑 어디 가지 ?', '가족들이랑 여행 갈거야', '가족여행 가야지', '가족이 누구야 ?', '가족이랑 여행 가려고', '가족한테 스트레스 풀었어', '가출할까 ?', '가출해도 갈 데가 없어', '간만에 떨리니까 좋더라', '간만에 쇼핑 중', '간만에 휴식 중', '간식 뭐 먹을까', '간식 추천', '간장치킨 시켜야지', '간접흡연 싫어', '갈까 말까 고민 돼', '갈까 말까 ?', '감 말랭이 먹고 싶다 .', '감 말랭이 먹어야지', '감기 같애', '감기 걸린 것 같아', '감기 기운이 있어', '감기 들 거 같애', '감기가 오려나', '감기약이 없어', '감기인거 같애', '감미로운 목소리 좋아', '감정이 쓰레기통처럼 엉망진창이야', '감정컨트롤을 못하겠어', '감정컨트롤이 안돼', '감히 나를 무시하는 애가 있어', '갑자기 나쁜 생각이 막 들더라', '갑자기 눈물 나', '갑자기 물어봐서 당황했어', '갑자기 불편한 사이가 된 거 같아', '강렬한 첫인상 남겨야 하는데', '강아지 키우고 싶어', '강아지 키우고 싶은데 역시 안돼겠지', '강아지 키울 수 있을까', '강아지 키울까', '강원도 가서 살까 ?', '같이 게임하자고 해도 되나 ?', '같이 놀러갈 친구가 없어', '같이 먹었는데 나만 살찐 거 같아', '같이 수영장 가기로 했어', '같이 있으면 힘든데 붙잡고 싶어', '같이 피씨방 가자고 해볼까 ?', '같이 할 수 있는 취미 생활 뭐 있을까', '개강룩 입어볼까', '개강옷 예쁘게 입어 볼까', '개강이다', '개강이라니', '개같은 상황', '개같이 되버렸어 .', '개기름 꼈어', '개념도 놓고 옴', '개념이 없어', '개당황', '개당황했잖아 갑자기 물어 봐서', '개인적인 업무까지 다 시켜', '개인적인 일도 다 시켜', '개졸려', '개좋아', '개학하니까 좋다', '걔 너무 싫다', '걔는 누굴 닮아서 그런거니 ?', '걔랑 같은 반 됐으면 좋겠다', '거지 같이 일해 놓고 갔어']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .', '다시 새로 사는 게 마음 편해요 .', '다시 새로 사는 게 마음 편해요 .', '잘 모르고 있을 수도 있어요 .', '시간을 정하고 해보세요 .', '시간을 정하고 해보세요 .', '자랑하는 자리니까요 .', '그 사람도 그럴 거예요 .', '그 사람도 그럴 거예요 .', '혼자를 즐기세요 .', '돈은 다시 들어올 거예요 .', '땀을 식혀주세요 .', '어서 잊고 새출발 하세요 .', '빨리 집에 돌아가서 끄고 나오세요 .', '빨리 집에 돌아가서 끄고 나오세요 .', '다음 달에는 더 절약해봐요 .', '따뜻하게 사세요 !', '다음 달에는 더 절약해봐요 .', '가장 확실한 시간은 오늘이에요 .  어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '더 가까워질 기회가 되겠네요 .', '저도요 .', '다들 바빠서 이야기할 시간이 부족했나봐요 .', '다들 바빠서 이야기할 시간이 부족했나봐요 .', '온 가족이 모두 마음에 드는 곳으로 가보세요 .', '좋은 생각이에요 .', '더 가까워질 기회가 되겠네요 .', '저를 만들어 준 사람을 부모님 ,  저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '좋은 생각이에요 .', '정말 후회할 습관이에요 .', '무모한 결정을 내리지 마세요 .', '선생님이나 기관에 연락해보세요 .', '떨리는 감정은 그 자체로 소중해요 .', '득템했길 바라요 .', '휴식도 필요하죠 .', '단짠으로 두 개 사는게 진리죠 .', '단짠으로 두 개 사는게 진리죠 .', '맛있게 드세요 .', '저도 싫어요 .', '가세요 .', '가세요 .', '맛있게 드세요 .', '맛있게 드세요 .', '병원가세요 .', '이럴 때 잘 쉬는 게 중요해요 .', '이럴 때 잘 쉬는 게 중요해요 .', '이럴 때 잘 쉬는 게 중요해요 .', '따뜻하게 관리하세요 .', '병원가세요 .', '병원가세요 .', '저도 듣고 싶네요 .', '자신을 더 사랑해주세요 .', '그건 습관이에요 .', '그건 습관이에요 .', '콕 집어서 물어보세요 .', '좋은 생각만 하세요 .', '마음이 아픈가요 .', '갑작스러웠나봐요 .', '관계의 변화가 왔나봅니다 .', '처음 3초가 중요해요 .  당신의 매력을 어필해보세요 .', '책임질 수 있을 때 키워 보세요 .', '먼저 생활패턴을 살펴 보세요 .', '먼저 생활패턴을 살펴 보세요 .', '책임질 수 있을 때 키워 보세요 .', '아름다운 곳이죠 .', '안 될 것도 없죠 .', '혼자도 좋아요 .', '연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요 .', '즐거운 시간 보내고 오세요 !', '질질 끌지 마세요 .', '말해보세요 .', '함께하면 서로를 더 많이 알게 될 거예요 .', '개시해보세요 .', '개시해보세요 .', '곧 방학이예요 .', '방학이 참 짧죠 .', '벗어나는 게 좋겠네요 .', '벗어나는 게 좋겠네요 .', '세수하고 오세요 .', '그게 제일 중요한 건데요 .', '그게 제일 중요한 건데요 .', '다음부터는 더 많이 아세요 .', '갑작스러웠나봐요 .', '공적인 일부터 하세요 .', '공적인 일부터 하세요 .', '낮잠을 잠깐 자도 괜찮아요 .', '저도 좋아해주세요 .', '친구들이 보고싶었나봐요 .', '되도록 만나지 마세요 .', '당신이요 .', '당신의 운을 믿어보세요 .', '일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠 .']\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(questions[:100])\n",
    "print(answers[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9abf0f",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fea8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "456802ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8178]\n",
      "END_TOKEN의 번호 : [8179]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71df7612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8180\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf094c7b",
   "metadata": {},
   "source": [
    "### 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)\n",
    "위에서 tensorflow_datasets의 SubwordTextEncoder를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다면, tokenizer.encode()로 각 단어를 정수로 변환할 수 있고 또는 tokenizer.decode()를 통해 정수 시퀀스를 단어 시퀀스로 변환할 수 있습니다.\n",
    "\n",
    "예를 들어서 22번째 샘플을 tokenizer.encode()의 입력으로 사용해서 변환 결과를 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caa78f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5766, 611, 2495, 4167]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2359, 7516, 7, 6279, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b83932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 10\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6455f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 MAX_LENGTH로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d99b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8180\n",
      "필터링 후의 질문 샘플 개수: 9096\n",
      "필터링 후의 답변 샘플 개수: 9096\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2a654",
   "metadata": {},
   "source": [
    "질문과 답변의 쌍을 tf.data.Dataset API의 입력으로 사용하여 파이프라인을 구성합니다. 이때, 교사 강요를 위해서 answers[:, :-1]를 디코더의 입력값, answers[:, 1:]를 디코더의 레이블로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81822e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d09e0",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a114ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d5a2c",
   "metadata": {},
   "source": [
    "### 1. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa86294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3675392     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    4466432     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8180)   2102260     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,244,084\n",
      "Trainable params: 10,244,084\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 3 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.2 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831966a",
   "metadata": {},
   "source": [
    "### 2. 손실 함수(Loss function)\n",
    "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7d51b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98808f",
   "metadata": {},
   "source": [
    "### 3. 커스텀 된 학습률(Learning rate) 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e955bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161da8b",
   "metadata": {},
   "source": [
    "### 4. 모델 컴파일\n",
    "손실 함수와 커스텀 된 학습률(learning rate)을 사용하여 모델을 컴파일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf75c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42279b",
   "metadata": {},
   "source": [
    "### 5. 훈련하기\n",
    "이제 학습을 진행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b5c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0 8178 7915 4207 3060   41 8179]\n",
      " [   0    0 8178 7971   47  919 7954  998 1716 8179]\n",
      " [   0    0 8178 7973 1435 4653 7954 3652   67 8179]\n",
      " [   0 8178 7973 1435 4653 7954 1295 3652   67 8179]\n",
      " [   0    0    0 8178 8002 8002 7998 7954 4190 8179]\n",
      " [   0    0 8178 8005 7990 2192  919   78  821 8179]\n",
      " [   0    0    0    0 8178 8005 7990 2192  199 8179]\n",
      " [   0 8178 2161  645 1129 7954   66 5116  852 8179]\n",
      " [8178 2161  782 7612  204   84  183  352 1258 8179]\n",
      " [   0    0 8178 5779  484  194 2984   45 3818 8179]\n",
      " [   0    0    0    0    0    0 8178  686  747 8179]\n",
      " [   0    0    0    0 8178  686 7203   31  747 8179]\n",
      " [   0    0    0    0 8178 2740 3806    5  767 8179]\n",
      " [   0 8178   78  743   23  148   54  961  951 8179]\n",
      " [   0    0    0 8178   78   62 1146 5487 1574 8179]\n",
      " [   0    0    0 8178 5766 7060 3885   16 5625 8179]\n",
      " [   0 8178 5766 7060 3885 1114 3673    8   37 8179]\n",
      " [   0 8178 5766  611    6   19 3116   55    1 8179]\n",
      " [   0 8178 5766  611 3509  141  685 3747  849 8179]\n",
      " [   0    0    0    0 8178 5766  611 2495 4167 8179]\n",
      " [   0    0    0    0 8178 5758  430 7898  143 8179]\n",
      " [   0    0    0    0    0 8178 5758  430 4604 8179]\n",
      " [   0    0    0 8178 5758  430 4136 1056    2 8179]\n",
      " [   0    0    0 8178 2159 4535 2900  579    1 8179]\n",
      " [   0    0    0    0 8178 7880 7954  140   26 8179]\n",
      " [   0    0    0    0 8178 7879  174  546   36 8179]\n",
      " [   0    0    0 8178 7879  174  546 4706   51 8179]\n",
      " [   0    0    0    0 8178 7879 1157 1056    2 8179]\n",
      " [   0    0    0    0 8178 7879  430 5753  114 8179]\n",
      " [   0    0    0    0    0 8178 2159  430  992 8179]\n",
      " [   0    0    0    0 8178 2159 1398  430 7887 8179]\n",
      " [   0    0    0    0 8178 7878  470 1127  901 8179]\n",
      " [   0    0    0    0 8178   78 1376   98    2 8179]\n",
      " [   0    0 8178   78 6060   14  397 7479   72 8179]\n",
      " [   0    0    0    0 8178 7875 1717  320 4884 8179]\n",
      " [   0    0    0    0    0 8178 7875 4228  203 8179]\n",
      " [   0    0    0    0 8178 7875 3792 7954  203 8179]\n",
      " [   0    0 8178  625  374  418 6012 5169   47 8179]\n",
      " [   0    0    0 8178  625 1392 5802 4112  239 8179]\n",
      " [   0    0 8178  885 7954 2096 7954  931  142 8179]\n",
      " [   0    0    0    0 8178  885 7954 2096    2 8179]\n",
      " [   0 8178 3177  307 8157 8080 8095   11  671 8179]\n",
      " [   0    0    0    0    0    0 8178  685  396 8179]\n",
      " [   0    0    0    0 8178  685 7828   18   37 8179]\n",
      " [   0    0    0    0 8178  685 1734   11   86 8179]\n",
      " [   0    0    0    0 8178  685  521    8  396 8179]\n",
      " [   0    0    0    0    0 8178 2157   27 5040 8179]\n",
      " [   0    0    0    0    0 8178 2157 2915   72 8179]\n",
      " [   0    0    0    0    0 8178 2157 4033  396 8179]\n",
      " [   0    0 8178  601  545  343  863 7236  139 8179]\n",
      " [   0    0    0    0 8178  933 6026   12  644 8179]\n",
      " [   0    0    0    0 8178  933 6026   11  199 8179]\n",
      " [   0    0    0 8178 4637  113 3554  702   86 8179]\n",
      " [   0    0 8178  409  462  214 2331  266 1568 8179]\n",
      " [   0    0    0    0    0 8178  409 2365   59 8179]\n",
      " [   0    0    0 8178  409 2086   52 2111  143 8179]\n",
      " [   0    0 8178  409 7053 1683  285    8   37 8179]\n",
      " [   0    0 8178 3175 3251  402 2027  199  415 8179]\n",
      " [   0    0    0    0 8178 3175 6013    4  103 8179]\n",
      " [   0    0 8178  719  862   14  551 1322    2 8179]\n",
      " [   0    0 8178   80 5717  449  123  407    2 8179]\n",
      " [   0    0    0 8178   80 4505  397  153   72 8179]\n",
      " [   0    0    0 8178   80 4224 1011 7898  143 8179]\n",
      " [   0    0    0 8178   80  158 4663 1552   26 8179]\n",
      " [   0 8178   80  555 1233 1883 1367  300    2 8179]\n",
      " [   0    0 8178  650  719 2334 7954 2865  256 8179]\n",
      " [   0 8178  650  719 1401  776 2865 7954  256 8179]\n",
      " [   0    0    0    0    0    0 8178 4631   55 8179]\n",
      " [   0    0    0    0    0    0 8178 4631 3606 8179]\n",
      " [   0    0    0    0    0 8178  650  201 1424 8179]\n",
      " [   0    0    0 8178  650   80  473 4323    1 8179]\n",
      " [   0    0    0 8178  650 5653 7954 3683   51 8179]\n",
      " [   0    0    0    0 8178 5737   14 1114 1511 8179]\n",
      " [   0    0    0    0    0 8178 5737   11   72 8179]\n",
      " [   0    0    0    0    0    0 8178  650 2111 8179]\n",
      " [   0 8178 7851  172 1092  273  620   45 1416 8179]\n",
      " [   0    0    0 8178 7851  172 1083   45 1416 8179]\n",
      " [   0    0    0    0    0    0 8178  650 1390 8179]\n",
      " [   0    0    0    0    0    0 8178  650  139 8179]\n",
      " [   0    0    0    0 8178  650 1374 3223  767 8179]\n",
      " [   0    0    0    0 8178 3171 7954    6  867 8179]\n",
      " [   0 8178 7848 7569 7515   52 4572  286    2 8179]\n",
      " [   0    0 8178 3171  208  201  667 2106  237 8179]\n",
      " [   0    0    0    0    0    0 8178 1941  460 8179]\n",
      " [   0    0    0    0    0    0 8178 3168  143 8179]\n",
      " [   0    0    0 8178 7841  598 3562   84   36 8179]\n",
      " [   0    0    0    0    0 8178 7841  197  142 8179]\n",
      " [   0 8178 5727 7954 7841 8157 8052 8101  135 8179]\n",
      " [   0    0    0    0 8178 3167  318 6966 1130 8179]\n",
      " [   0    0 8178 3167   24  318  405   67    1 8179]\n",
      " [   0    0    0    0    0    0 8178 2731 3151 8179]\n",
      " [   0    0    0    0 8178 2731  544 3190  757 8179]\n",
      " [   0    0 8178 1940 8156 8100 8050  272  289 8179]\n",
      " [   0 8178 1940 8156 8100 8050  556 1971 1511 8179]\n",
      " [   0    0    0    0    0    0 8178 4621 1609 8179]\n",
      " [   0    0    0 8178 4621 1609  172   18   37 8179]\n",
      " [   0    0    0    0 8178 7833 2114   48  383 8179]\n",
      " [   0    0    0    0 8178 1940   23 5556  872 8179]\n",
      " [   0    0    0    0    0 8178  623  591  337 8179]\n",
      " [   0    0    0    0 8178 3749  110 1322  200 8179]]\n",
      "[[   0    0    0    0 8178 3844   74 7894    1 8179]\n",
      " [   0    0    0    0    0 8178 1830 5502    1 8179]\n",
      " [   0    0    0    0 8178 3400  777  131    1 8179]\n",
      " [   0    0    0    0 8178 3400  777  131    1 8179]\n",
      " [   0 8178  985 2300 1492 2184 5470   50    1 8179]\n",
      " [8178   69 2064  456    5  137 2188   17    1 8179]\n",
      " [8178   69 2064  456    5  137 2188   17    1 8179]\n",
      " [   0    0 8178   13 1892   30   71   25    1 8179]\n",
      " [   0    0    0 8178  350 3985   16   32    1 8179]\n",
      " [   0    0    0 8178 6346   48 1628  334    1 8179]\n",
      " [   0    0    0 8178   63  516  138    3    1 8179]\n",
      " [   0    0    0 8178   63  516  138    3    1 8179]\n",
      " [   0    0    0    0    0 8178 5831 1618    1 8179]\n",
      " [   0    0 8178 5524   69 5497 7954    3    1 8179]\n",
      " [   0    0    0 8178 7400  453 2171  105    1 8179]\n",
      " [8178  544  348 1567   52 2136   16 4529    1 8179]\n",
      " [8178  544  348 1567   52 2136   16 4529    1 8179]\n",
      " [   0    0 8178 2359 7516    7 6279   97    1 8179]\n",
      " [   0    0    0    0    0 8178 2343  514   41 8179]\n",
      " [   0    0 8178 2359 7516    7 6279   97    1 8179]\n",
      " [8178 1088 3178  592  264  878  622  288    1 8179]\n",
      " [8178 1088 3178  592  264  878  622  288    1 8179]\n",
      " [8178 1088 3178  592  264  878  622  288    1 8179]\n",
      " [   0    0 8178    7 5773  231 2140 1039    1 8179]\n",
      " [   0    0    0    0    0    0 8178 1500    1 8179]\n",
      " [8178 1919 5343 6427 7954   70 2076 1205    1 8179]\n",
      " [8178 1919 5343 6427 7954   70 2076 1205    1 8179]\n",
      " [8178 1088 3178  592  264  878  622  288    1 8179]\n",
      " [   0    0    0    0    0 8178   21 2062    1 8179]\n",
      " [   0    0 8178    7 5773  231 2140 1039    1 8179]\n",
      " [   0    0    0    0    0 8178   21 2062    1 8179]\n",
      " [   0    0    0    0 8178   77 1201 2054    1 8179]\n",
      " [   0 8178 7222   23 1743 1454   31   44    1 8179]\n",
      " [   0 8178 6913   28  117 1466   29  339    1 8179]\n",
      " [8178 1717   10 4638   63 3336 5196   83    1 8179]\n",
      " [   0    0    0    0 8178 4440 5875  257    1 8179]\n",
      " [   0    0    0    0    0 8178 5808 1479    1 8179]\n",
      " [   0    0    0    0    0 8178  503  126    1 8179]\n",
      " [   0    0    0    0    0 8178  100 6793    1 8179]\n",
      " [   0    0    0    0    0    0 8178  750    1 8179]\n",
      " [   0    0    0    0    0    0 8178  750    1 8179]\n",
      " [   0    0    0    0    0 8178  503  126    1 8179]\n",
      " [   0    0    0    0    0    0 8178 7113    1 8179]\n",
      " [   0 8178 2521   99   13 1418    5  398    1 8179]\n",
      " [   0 8178 2521   99   13 1418    5  398    1 8179]\n",
      " [   0 8178 2521   99   13 1418    5  398    1 8179]\n",
      " [   0    0    0    0    0 8178 2343 3150    1 8179]\n",
      " [   0    0    0    0    0    0 8178 7113    1 8179]\n",
      " [   0    0    0    0    0    0 8178 7113    1 8179]\n",
      " [   0    0    0    0 8178  100 1718  663    1 8179]\n",
      " [   0    0    0    0    0 8178 1120 2054    1 8179]\n",
      " [   0    0    0    0    0 8178 1120 2054    1 8179]\n",
      " [   0    0 8178 2446 7954 6132   52  312    1 8179]\n",
      " [   0    0    0    0 8178   21 1320   87    1 8179]\n",
      " [   0    0    0    0 8178   58 1850  425    1 8179]\n",
      " [   0    0    0    0 8178 7861 6485   97    1 8179]\n",
      " [   0    0    0    0 8178 7760  789 6525    1 8179]\n",
      " [   0 8178  102 2060 5984   12 5245   49    1 8179]\n",
      " [   0 8178  102 2060 5984   12 5245   49    1 8179]\n",
      " [   0    0    0    0    0 8178 2042 7771    1 8179]\n",
      " [   0    0    0 8178   20   82   73  636    1 8179]\n",
      " [   0    0    0    0    0 8178 2760  118    1 8179]\n",
      " [   0    0    0 8178  820  169 2307  317   41 8179]\n",
      " [   0    0    0 8178 6134 2697   31   44    1 8179]\n",
      " [   0    0    0    0    0    0 8178  146    1 8179]\n",
      " [   0    0    0    0    0    0 8178 7852    1 8179]\n",
      " [   0    0    0    0    0    0 8178 7852    1 8179]\n",
      " [   0    0    0    0 8178  681 4326  177    1 8179]\n",
      " [   0    0    0    0 8178 3002  182 6121    1 8179]\n",
      " [   0    0    0    0 8178 7126    5  222    1 8179]\n",
      " [   0    0    0    0 8178 7126    5  222    1 8179]\n",
      " [   0    0    0 8178  404  179   46  317    1 8179]\n",
      " [   0    0 8178  243  373  381  623 2354    1 8179]\n",
      " [   0    0 8178  243  373  381  623 2354    1 8179]\n",
      " [   0    0 8178 3641    7   19  135   68    1 8179]\n",
      " [   0    0    0 8178  423  559 3350   87    1 8179]\n",
      " [   0    0    0 8178  423  559 3350   87    1 8179]\n",
      " [   0    0 8178 7611   12  895 4933  294    1 8179]\n",
      " [   0    0    0    0 8178  100 6222   68    1 8179]\n",
      " [   0    0    0    0 8178 1782 2623 2905    1 8179]\n",
      " [   0    0    0 8178  473 2353  843   44    1 8179]\n",
      " [   0    0    0    0    0 8178 2676   17    1 8179]\n",
      " [   0    0    0    0 8178  367 4077 2321    1 8179]\n",
      " [   0 8178  501 6977 7954  690 2454   49 8048 8179]\n",
      " [   0 8178  869  375   54 5727   91  257    1 8179]\n",
      " [   0    0    0    0 8178 7842 4715 3094    1 8179]\n",
      " [   0    0    0    0 8178 7842 4715 3094    1 8179]\n",
      " [   0    0    0 8178 6142  290 3219 1572    1 8179]\n",
      " [   0    0    0    0 8178  798 4623   25    1 8179]\n",
      " [   0    0    0    0 8178  798 4623   25    1 8179]\n",
      " [   0    0    0    0    0 8178 1832   32    1 8179]\n",
      " [8178 6894   54 7219  278 4621  373  398    1 8179]\n",
      " [   0    0    0 8178 3310 1966    5  131    1 8179]\n",
      " [   0    0    0 8178 3310 1966    5  131    1 8179]\n",
      " [   0    0    0    0 8178  389  381 7235    1 8179]\n",
      " [   0    0    0    0 8178  389  381 7235    1 8179]\n",
      " [   0    0    0 8178 3329  129   19 6495    1 8179]\n",
      " [   0    0    0 8178 3329  129   19 6495    1 8179]\n",
      " [   0    0    0    0 8178  487 3676 2595    1 8179]\n",
      " [   0    0    0    0 8178  158 1482 2357    1 8179]]\n"
     ]
    }
   ],
   "source": [
    "# MAX_LEN에 따른 데이터 크기 상태 확인\n",
    "print(questions[:100])\n",
    "print(answers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd2b6655",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 12s 47ms/step - loss: 6.7119 - accuracy: 0.0528\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 6.1424 - accuracy: 0.1404\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 5.5646 - accuracy: 0.2899\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 4.8490 - accuracy: 0.3157\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 4.2615 - accuracy: 0.3180\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 3.7992 - accuracy: 0.3192\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 3.5319 - accuracy: 0.3223\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 3.3689 - accuracy: 0.3296\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 3.2445 - accuracy: 0.3355\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 3.1424 - accuracy: 0.3376\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 3.0516 - accuracy: 0.3426\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.9576 - accuracy: 0.3476\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 2.8692 - accuracy: 0.3536\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.7765 - accuracy: 0.3605\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 2.6770 - accuracy: 0.3686\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.5672 - accuracy: 0.3802\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.4534 - accuracy: 0.3926\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.3295 - accuracy: 0.4079\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.1879 - accuracy: 0.4246\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 2.0578 - accuracy: 0.4410\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 1.9088 - accuracy: 0.4594\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 1.7592 - accuracy: 0.4795\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 1.6162 - accuracy: 0.5014\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 1.4645 - accuracy: 0.5232\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 1.3154 - accuracy: 0.5467\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 1.1734 - accuracy: 0.5693\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 1.0359 - accuracy: 0.5883\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.9026 - accuracy: 0.6110\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.7860 - accuracy: 0.6303\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.6729 - accuracy: 0.6480\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.5693 - accuracy: 0.6678\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.4792 - accuracy: 0.6820\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.4026 - accuracy: 0.6970\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.3234 - accuracy: 0.7120\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.2742 - accuracy: 0.7214\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.2333 - accuracy: 0.7269\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.1941 - accuracy: 0.7333\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.1574 - accuracy: 0.7398\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.1469 - accuracy: 0.7412\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.1182 - accuracy: 0.7461\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.1107 - accuracy: 0.7472\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.1007 - accuracy: 0.7480\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0943 - accuracy: 0.7488\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0842 - accuracy: 0.7509\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0827 - accuracy: 0.7503\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0818 - accuracy: 0.7499\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0779 - accuracy: 0.7504\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0753 - accuracy: 0.7507\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0721 - accuracy: 0.7517\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0720 - accuracy: 0.7508\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0714 - accuracy: 0.7510\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0734 - accuracy: 0.7505\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0623 - accuracy: 0.7534\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0674 - accuracy: 0.7516\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0671 - accuracy: 0.7510\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0629 - accuracy: 0.7524\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0621 - accuracy: 0.7526\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0582 - accuracy: 0.7534\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0547 - accuracy: 0.7548\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0509 - accuracy: 0.7549\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0465 - accuracy: 0.7564\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.0505 - accuracy: 0.7552\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.0424 - accuracy: 0.7577\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0486 - accuracy: 0.7563\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0405 - accuracy: 0.7583\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0414 - accuracy: 0.7578\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0393 - accuracy: 0.7583\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0399 - accuracy: 0.7581\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0366 - accuracy: 0.7596\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0341 - accuracy: 0.7598\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0316 - accuracy: 0.7603\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0313 - accuracy: 0.7604\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0345 - accuracy: 0.7595\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0315 - accuracy: 0.7607\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0258 - accuracy: 0.7622\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0272 - accuracy: 0.7615\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0301 - accuracy: 0.7609\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.0241 - accuracy: 0.7622\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0242 - accuracy: 0.7624\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0276 - accuracy: 0.7615\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0256 - accuracy: 0.7619\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0233 - accuracy: 0.7625\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0218 - accuracy: 0.7632\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.0212 - accuracy: 0.7636\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0208 - accuracy: 0.7631\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0203 - accuracy: 0.7633\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0212 - accuracy: 0.7632\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0208 - accuracy: 0.7632\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0210 - accuracy: 0.7631\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0176 - accuracy: 0.7643\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0183 - accuracy: 0.7639\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0195 - accuracy: 0.7635\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0205 - accuracy: 0.7633\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0161 - accuracy: 0.7644\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0176 - accuracy: 0.7639\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0191 - accuracy: 0.7635\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0173 - accuracy: 0.7640\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0141 - accuracy: 0.7651\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.0168 - accuracy: 0.7643\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0173 - accuracy: 0.7640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6a20034760>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885ee7b",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1da3211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 동일한 전처리를 수행하는 함수 정의\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ced19408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62724ecc",
   "metadata": {},
   "source": [
    "임의의 입력 문장에 대해서 decoder_inference() 함수를 호출하여 챗봇의 대답을 얻는 sentence_generation() 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac6667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228e7a1",
   "metadata": {},
   "source": [
    "임의의 문장으로부터 챗봇의 대답을 얻어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f37c8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요.\n",
      "출력 : 안녕하세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요 .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('안녕하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0870734b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 12시 땡!\n",
      "출력 : 하루가 또 다른 사랑이 하실 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'하루가 또 다른 사랑이 하실 거예요 .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"12시 땡!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1df2386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 1지망 학교 떨어졌어.\n",
      "출력 : 위로해 드립니다구박하지는 마세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'위로해 드립니다구박하지는 마세요 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"1지망 학교 떨어졌어.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "433271ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 3박4일 놀러가고 싶다.\n",
      "출력 : 여행은 언제나 좋죠긴여행은 언제나 좋죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'여행은 언제나 좋죠긴여행은 언제나 좋죠 .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"3박4일 놀러가고 싶다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf5ff385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : PPL 심하네.\n",
      "출력 : 눈살이 찌푸려지죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'눈살이 찌푸려지죠 .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"PPL 심하네.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de4f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : SD카드 망가졌어\n",
      "출력 : 다시 새로 사는 게 마음 편해요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'다시 새로 사는 게 마음 편해요 .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"SD카드 망가졌어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f45731f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 가족관계 알려 줘\n",
      "출력 : 그 어떤 어디에서 만난 건 상관없어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그 어떤 어디에서 만난 건 상관없어요 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"가족관계 알려 줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f018c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 결혼식 가기 귀찮아\n",
      "출력 : 경조사는 참석하는게 좋아요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'경조사는 참석하는게 좋아요 .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"결혼식 가기 귀찮아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ec9ff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 계속 보고 싶으면 어떡해?\n",
      "출력 : 지금은 보러 �봐야 알죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'지금은 보러 �봐야 알죠 .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"계속 보고 싶으면 어떡해?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bae70440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 공부 꼭 해야 할까\n",
      "출력 : 공부하면 더 많은 선택을 할 수 있죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'공부하면 더 많은 선택을 할 수 있죠 .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"공부 꼭 해야 할까\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e35c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 금연이 쉽지 않아\n",
      "출력 : 자신을 위한 결정을 내리길 바라요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'자신을 위한 결정을 내리길 바라요 .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"금연이 쉽지 않아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "197d8099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 심심한데 뭐 재미있는거 없을까\n",
      "출력 : 저랑 대화하는 게 위로가 되었으면 합니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저랑 대화하는 게 위로가 되었으면 합니다 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"심심한데 뭐 재미있는거 없을까\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e934c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 아침에 들을 노래 추천해줘\n",
      "출력 : 애애국가요 .  붙잡으세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'애애국가요 .  붙잡으세요 .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"아침에 들을 노래 추천해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10da6d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 운동 다녀왔어\n",
      "출력 : 아름다운 선택을 쨌요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아름다운 선택을 쨌요 .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"운동 다녀왔어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8c857",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 배운 점\n",
    "1. transformer 모델의 핵심 아이디어와 구조에 대하여 대략적으로 이해할 수 있었다.\n",
    "2. transformer 모델 구조를 코드로 구현할 수 있었다.\n",
    "3. transformer 모델에 데이터를 입력하기 전 텍스트 데이터의 전처리 단계에 대해 배울 수 있었다. 정규표현식을 활용해서 구두점과 문자를 분리하고, 필요에 따라 특정 문자를 제거하는 방법을 활용하였다. \n",
    "\n",
    "- 문제해결 시도\n",
    "1. MAX_LEN 이 40 인 경우 대부분의 텍스트 데이터에 패딩값이 포함되어 있는 것을 확인하였다. 최대길이를 20으로 제한하여 모델링을 진행할때는 학습 속도에 큰 향상이 있었다.\n",
    "2. batch size를 128, dropout 비율을 0.2로 시도했을 때 정확도와 학습 속도에 향상이 있었다.\n",
    "3. padding 설정을 post에서 pre로 바꿨을 때 성능 향상에 진전이 있었다. transformer는 데이터를 병렬적이게 처리한다고 하였는데, padding 설정에 따른 차이에 대하여 질문을 남겨놓았다.\n",
    "4. 학습 데이터가 11000여개 뿐이기 때문에, 주어진 데이터 외 다른 질문은 답을 못하는 듯 하다. MAX_LEN에 따른 정확도 문제도 있는 것으로 확인했다.\n",
    "5. NUM_LAYERS를 2에서 4로 확장시켜 학습을 시도하였다. MAX_LEN=20인 경우, 모델의 깊이를 추가했을 때 accuracy 향상 폭이 확실히 개선되었다.\n",
    "6. MAX_LEN이 40인 경우 최대 정확도는 20%, 20인 경우 40%로 나왔다. 문장길이가 길다고 해서 좋은 답이 나오지는 않았다. 체감상 문맥에 맞는 답변이 반반 되는 것 같다.\n",
    "7. 레이어 3, 드롭아웃 0.2, MAX_LEN=10, batch size=128에서 훈련 정확도는 76%정도 나타났다.\n",
    "\n",
    "- 부족한 점\n",
    "1. 노드에서도 transformer과 다른 텍스트 처리 아키텍쳐에 대해 간략하게 설명한 것 같다. 필요에 따라 더 구체적으로 조사하고 공부해야할 필요성을 느꼈다.\n",
    "2. 텍스트 데이터를 불러와 전처리하는 과정에서 코드를 명확하게 이해해야할 필요성을 느꼈다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
