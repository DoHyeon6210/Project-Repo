{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a806466",
   "metadata": {},
   "source": [
    "# 10-1. 프로젝트 : 커스텀 프로젝트 직접 만들기\n",
    "실습 코드에서 수행해 본 내용을 토대로, 이번에는 한국어 데이터셋에 도전해보겠습니다.\n",
    "\n",
    "앞서 본 GLUE benchmark의 한국어 버전 KLUE benchmark를 들어보신 적 있나요?\n",
    "\n",
    "https://klue-benchmark.com/\n",
    "\n",
    "GLUE와 마찬가지로 한국어 자연어처리에 대한 이해도를 높이기 위해 만들어진 데이터셋 benchmark입니다. 총 8가지의 데이터셋이 있습니다. 다만 이번 시간에 진행할 프로젝트는 KLUE의 dataset을 활용하는 것이 아닌, model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task를 도전해보겠습니다.\n",
    "\n",
    "모델과 데이터에 관한 정보는 링크를 참조해주세요.\n",
    "\n",
    "KLUE/Bert-base - https://huggingface.co/klue/bert-base\n",
    "\n",
    "NSMC - https://github.com/e9t/nsmc\n",
    "\n",
    "준비가 되셨다면 아래와 같은 순서로 진행해주세요.\n",
    "\n",
    "### 라이브러리 버전을 확인해 봅니다.\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7626ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871cca3",
   "metadata": {},
   "source": [
    "# STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "데이터셋은 깃허브에서 다운받거나, Huggingface datasets에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!\n",
    "\n",
    "https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51d183",
   "metadata": {},
   "source": [
    "### Huggingface transformers 설치 및 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5973be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360486c6",
   "metadata": {},
   "source": [
    "### nsmc 데이터 불러오기 및 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3266e521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6291c1063b4149edad07c2d34be3a8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087e4ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 150000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_nsmc_dataset['train']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e58612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9d7ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
       " '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.',\n",
       " '원작의 긴장감을 제대로 살려내지못했다.',\n",
       " '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네',\n",
       " '액션이 없는데도 재미 있는 몇안되는 영화',\n",
       " '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?',\n",
       " '걍인피니트가짱이다.진짜짱이다♥',\n",
       " '볼때마다 눈물나서 죽겠다90년대의 향수자극!!허진호는 감성절제멜로의 달인이다~',\n",
       " '울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해',\n",
       " '담백하고 깔끔해서 좋다. 신문기사로만 보다 보면 자꾸 잊어버린다. 그들도 사람이었다는 것을.',\n",
       " '취향은 존중한다지만 진짜 내생에 극장에서 본 영화중 가장 노잼 노감동임 스토리도 어거지고 감동도 어거지',\n",
       " 'ㄱ냥 매번 긴장되고 재밋음ㅠㅠ',\n",
       " '참 사람들 웃긴게 바스코가 이기면 락스코라고 까고바비가 이기면 아이돌이라고 깐다.그냥 까고싶어서 안달난것처럼 보인다',\n",
       " '굿바이 레닌 표절인것은 이해하는데 왜 뒤로 갈수록 재미없어지냐',\n",
       " '이건 정말 깨알 캐스팅과 질퍽하지않은 산뜻한 내용구성이 잘 버무러진 깨알일드!!♥',\n",
       " '약탈자를 위한 변명, 이라. 저놈들은 착한놈들 절대 아닌걸요.',\n",
       " '나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님',\n",
       " '보면서 웃지 않는 건 불가능하다',\n",
       " '재미없다 지루하고. 같은 음식 영화인데도 바베트의 만찬하고 넘 차이남....바베트의 만찬은 이야기도 있고 음식 보는재미도 있는데 ; 이건 볼게없다 음식도 별로 안나오고, 핀란드 풍경이라도 구경할랫는데 그것도 별로 안나옴 ㅡㅡ',\n",
       " '절대 평범한 영화가 아닌 수작이라는걸 말씀드립니다.',\n",
       " '주제는 좋은데 중반부터 지루하다',\n",
       " '다 짤랐을꺼야. 그래서 납득할 수 없었던거야.. 그럴꺼야.. 꼭 그랬던걸꺼야..',\n",
       " 'kl2g 고추를 털어버려야 할텐데',\n",
       " '카밀라벨 발연기',\n",
       " '재밋는뎅',\n",
       " '센스있는 연출력..탁월한 캐스팅..90년대의 향수.. 그래서 9점..',\n",
       " '엄포스의 위력을 다시 한번 깨닫게 해준 적.남 꽃검사님도 연기 정말 좋았어요! 완전 명품드라마!',\n",
       " '졸쓰레기 진부하고말도안됌ㅋㅋ 아..시간아까워',\n",
       " '재밌는데 별점이 왜이리 낮은고',\n",
       " '1%라도 기대했던 내가 죄인입니다 죄인입니다....',\n",
       " '아직도 이 드라마는 내인생의 최고!',\n",
       " '패션에 대한 열정! 안나 윈투어!',\n",
       " '키이라 나이틀리가 연기하고자 했던건 대체 정신장애일까 틱장애일까',\n",
       " '허허...원작가 정신나간 유령이라... 재미있겠네요!',\n",
       " '포스터는 있어보이는데 관객은 114명이네',\n",
       " '이 영화가 왜 이렇게 저평가 받는지 모르겠다',\n",
       " '단순하면서 은은한 매력의 영화',\n",
       " \"'다 알바생인가 내용도 없고 무서운거도 없고 웃긴거도 하나도 없음 완전 별싱거운 영화.ㅇ.ㅇ내ㅇ시간 넘 아까움 .. . 완전 낚임\",\n",
       " '오게두어라! 서리한이 굶주렸다!',\n",
       " '정말 맘에 들어요. 그래서 또 보고싶은데 또 보는 방법이 없네? >.. ㅜㅡ',\n",
       " '윤제문이라는 멋진 배우를 발견하게 됐어요. 소소한 일탈이 잔잔한 미소를 머금게 합니다. 음악은 조금 아쉽네요ㅠㅠ 8점 주고 싶은데 평점 올리고 싶어 10점 줄게요^^',\n",
       " '평점에속지마시길시간낭비 돈낭비임',\n",
       " '리얼리티가 뛰어나긴 한데 큰 공감은 안간다. 이민기캐릭터는 정신의학상 분노조절장애 초기 증상일거다. 툭하면 사람패고 욕하고 물건 파손하고.. 조금 오바였음. 극 초반엔 신선했는데 가면 갈수록 이민기 정신상태 공감불가.',\n",
       " '마이너스는 왜없냐 ㅋ 뮤비 보고 영화수준 딱 알만하더군 ㅉㅉ 북한에서 이런거 만들라고 돈 대주던?',\n",
       " '난 우리영화를 사랑합니다....^^;',\n",
       " '데너리스 타르 가르엔...나도 용의주인이 되고 싶다...누이랑,근친상간이나 하고 다닐지라도,소설 속에선 제일 멋진 놈이 자이메 라니스터였는데,드라마속에선,드래곤(용)이 제일 멋지네(웃음)감독님 토르-2 다크 월드는 말아 잡수셨을지라도,기본 선방은 했음',\n",
       " '영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영화에 행복했네요',\n",
       " '야 세르게이! 작은고추의 매운맛을 보여주마! 포퐁저그 콩진호가 간다',\n",
       " '이렇게 가슴시리게 본 드라마가 또 있을까? 감동 그 자체!',\n",
       " '난또 저 꼬마애가 무슨 원한이 깊길래.,. 했더니 OO 그냥 혼자 나대다 OO걸 어쩌라고.',\n",
       " '재미있어요',\n",
       " '전 좋아요',\n",
       " '최고',\n",
       " '너무 충격적이엇다. 기분을 완전히 푹 꺼지게 하는 느낌... 활력이라고는 하나도 없는 너무나도 무거운....지독하고 차갑고 무자비하다. 그저 일본인들의 상상력은 정말 대단한거 같다는 생각이 든다.',\n",
       " '심심한영화.',\n",
       " '백봉기 언제나오나요?',\n",
       " '보는내내 그대로 들어맞는 예측 카리스마 없는 악역',\n",
       " '불알이 나와서 당황...아무튼 영화가 중간에 끝나는 느낌',\n",
       " '평범함속에 녹아든 평범한 일상. 조금 밋밋한게 흠.',\n",
       " '보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에 짜증이 ㅜㅜ 맨날 언제끝나나 기대만하고있어요 전개좀 빨리빨리 ㅜㅜ',\n",
       " '사랑하고싶게하는,가슴속온감정을헤집어놓는영화예요정말최고.',\n",
       " '많은 사람들이 이 다큐를 보고 우리나라 슬픈 현대사의 한 단면에 대해 깊이 생각하고 사죄하고 바로 잡기 위해 노력했으면 합니다. 말로만 듣던 보도연맹, 그 민간인 학살이 이정도 일 줄이야. 이건 명백한 살인입니다. 살인자들은 다 어디있나요?',\n",
       " '예전 작품 캐릭터, 에피소드 재탕 삼탕 사골우려먹듯 우리고 내용은 산으로 가고 시청률은 아예안나오고 이제 70회중반인데 120부작이라니 ...',\n",
       " '김남길의 백점짜리 연기력과 초반 몰입도에도 불구하고 지루하고 손예진 ㅈㅈ',\n",
       " '재밌네 비슷한 영화를 안보신 분들한테는 재미있을 듯',\n",
       " '노래실력으로뽑는게 맞냐? 박시환이 mama나가면 진짜 망신이다',\n",
       " '아 일본영화 다이런건가?? 유치하다',\n",
       " '이틀만에 다 봤어요 재밌어요 근데 차 안에 물건 넣어 조작하려고 하면 차 안이 열려있다던지 집 안이 활짝 열려서 아무나 들어간다던가 문자를 조작하려고하면 비번이 안 걸려있고 ㅋㅋㅋ 그런 건 억지스러웠는데 그래도 내용 자체는 좋았어요',\n",
       " '졸작',\n",
       " '재밋네요 달팽이가 빨라서 더 재밌었어요',\n",
       " '어설픈 전개 어이없는 결말',\n",
       " '부패한 로마노프 왕조를 기리는 뭣같은 영화... 온몸으로 항거했던 러시아 민중들이 그저 폭도냐',\n",
       " '내용전개는 무난한 편이였구 잘 보았습니다 ^^',\n",
       " '매우 실망.....',\n",
       " '한국영화 흥행코드: 갈등-갈등-계~에속 갈등-화해-감동- 평점 10점 남발- 흥행 뻔하지 뭐...',\n",
       " '아햏햏 아햏햏 아햏햏.',\n",
       " '뭐냐..시작하고 3분만에 나왔다. 리플릿 사진 보며 불안하더니만..',\n",
       " '단연 최고라고 할수있지',\n",
       " '감독이 럼먹고 영화를 만들었나보다.. 관객에게 뭘 말하는지도 모르겠고, 엉망진창 개진창이다.',\n",
       " '이건 뭐냐? 우뢰매냐? ;;;',\n",
       " '정말쓰레기영화입니다',\n",
       " '진정 위대한 영화 최고임',\n",
       " '별루 였다..',\n",
       " '내일이 기대되는 `',\n",
       " '근데 조미가 막문위 좋아한건가요??',\n",
       " 'ㅋㅋㅋ 진짜 골깜..ㅋㅋ 눈 부라릴때 쓰러짐..ㅋㅋ',\n",
       " '성룡영화중 최악인듯 ㅋㅋ',\n",
       " '골때리네ㅋㅋㅋㅋ 걸스데이 이혜리 잘 되라!',\n",
       " '서기가이뻐서',\n",
       " '완전 재밌어요ㅋㅋㅋㅋㅋ백인공주귀여움ㅋㅋㅋㅋㅋㅋ',\n",
       " '인상적인 영화였다',\n",
       " '어내스트와 셀레스틴 완전 강추에요~ 정말 재밌습니다^^',\n",
       " '재미있는영화입니다.',\n",
       " '클라라볼라고화신본거아닌데',\n",
       " '진짜 보면서 너무 슬펐던 영화다',\n",
       " '설정이 재밌고 새로운 에피소드 내에서 메인 스토리도 차차 나오는게 재밌음']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['document'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713a1a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bbf5f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9976970',\n",
       " '3819312',\n",
       " '10265843',\n",
       " '9045019',\n",
       " '6483659',\n",
       " '5403919',\n",
       " '7797314',\n",
       " '9443947',\n",
       " '7156791',\n",
       " '5912145']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5141067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b2a42",
   "metadata": {},
   "source": [
    "# STEP 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64e703a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# distilbert-base-uncased\n",
    "\n",
    "#huggingface_model = AutoModel.from_pretrained(\"klue/bert-base\")\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)\n",
    "#huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-small', num_labels = 2)\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b43c1b",
   "metadata": {},
   "source": [
    "# STEP 3.  tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8383162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = True,\n",
    "        max_length=41, # nsmc 데이터 93%가 lenth 41이내\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f86dc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-aa04a674437bedfa.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a0e1585694cf16af.arrow\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = huggingface_nsmc_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "608375eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
       " '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.',\n",
       " '원작의 긴장감을 제대로 살려내지못했다.',\n",
       " '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네',\n",
       " '액션이 없는데도 재미 있는 몇안되는 영화',\n",
       " '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['document'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95c98941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf_dataset['train']['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9753c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e9699",
   "metadata": {},
   "source": [
    "### validation set 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f90ba02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Dateset split 적용\n",
    "spilt_dataset = hf_dataset['train'].train_test_split(test_size=0.2)\n",
    "spilt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6b60eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 120000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 30000\n",
       " }))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. spilt한 데이터 저장\n",
    "train_dataset, validation_dataset = spilt_dataset['train'], spilt_dataset['test']\n",
    "train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a902652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-7267e811ee508baf.arrow\n"
     ]
    }
   ],
   "source": [
    "# 3. train & validation & test split.\n",
    "hf_train_dataset = train_dataset.shuffle(seed=42).select(range(5000))\n",
    "hf_val_dataset = validation_dataset.shuffle(seed=42).select(range(1000))\n",
    "hf_test_dataset = hf_dataset['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96acb075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 1000\n",
       " }))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_dataset, hf_val_dataset, hf_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234cf14",
   "metadata": {},
   "source": [
    "### ㄴ 문제해결회고 \n",
    ": 과도한 학습 시간(8시간 30분)으로 인하여 데이터 샘플링 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828d04a",
   "metadata": {},
   "source": [
    "### Trainer를 활용한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9de3dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/Natural_Language_Processing/10_huggingface_project/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                            # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           # evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                  # learning_rate\n",
    "    per_device_train_batch_size = 8,       # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,        # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                 # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                   # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "060074af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# # 정확도(Accuracy)와 F1 점수(F1 Score)를 계산하는 compute_metrics 함수\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average='weighted')\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "341ac342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue/mrpc의 acc, f1 지표 활용\n",
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2192a81",
   "metadata": {},
   "source": [
    "Trainer에 model, arguments, train_dataset, eval_dataset, compute_metrics를 넣고 train을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4249fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 04:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.431659</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.840459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.659398</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.832998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.841176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.2597946756998698, metrics={'train_runtime': 256.4822, 'train_samples_per_second': 58.484, 'train_steps_per_second': 7.31, 'total_flos': 316041599700000.0, 'train_loss': 0.2597946756998698, 'epoch': 3.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1fc9",
   "metadata": {},
   "source": [
    "마지막으로 test 데이터셋으로 평가를 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78e3ffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7093942761421204,\n",
       " 'eval_accuracy': 0.854,\n",
       " 'eval_f1': 0.8522267206477733,\n",
       " 'eval_runtime': 3.061,\n",
       " 'eval_samples_per_second': 326.695,\n",
       " 'eval_steps_per_second': 40.837,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6736c5a",
   "metadata": {},
   "source": [
    "### 결과\n",
    ": 다양한 시도를 했을 때 베이스라인 모델의 최고 validation accuracy는 0.888로 나타났다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaeaca5",
   "metadata": {},
   "source": [
    "# STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6232f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리를 비워줍니다.\n",
    "del huggingface_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53094f0",
   "metadata": {},
   "source": [
    "### monologg/koelectra-base-v3 모델을 활용한 fine-tuning 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d77fe1b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmp4z3icby0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48df90731a04cf9921b270ad4cfd6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer_config.json in cache at /aiffel/.cache/huggingface/transformers/6c420e1e4e8a4bb8cef587982266920738d38d77d6210cc0fa20f4e50e24f04c.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/6c420e1e4e8a4bb8cef587982266920738d38d77d6210cc0fa20f4e50e24f04c.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
      "https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpt8naqg02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2756171e816347a39500421ae7817378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json in cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpiuwe2g4y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949b050d70d44cf6a6930707ad7d3557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/273k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/vocab.txt in cache at /aiffel/.cache/huggingface/transformers/bfe61bb60a4fddad9575ec468f02c708afd9458e51599f876b2164ae6870f128.42b747a439e42193b1b54614a2e0c4608d8ec136b7405c799d29f667ccdbb27d\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/bfe61bb60a4fddad9575ec468f02c708afd9458e51599f876b2164ae6870f128.42b747a439e42193b1b54614a2e0c4608d8ec136b7405c799d29f667ccdbb27d\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/bfe61bb60a4fddad9575ec468f02c708afd9458e51599f876b2164ae6870f128.42b747a439e42193b1b54614a2e0c4608d8ec136b7405c799d29f667ccdbb27d\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/6c420e1e4e8a4bb8cef587982266920738d38d77d6210cc0fa20f4e50e24f04c.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpqokycmoi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e308acb80047427396b61bccb25d5ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/pytorch_model.bin in cache at /aiffel/.cache/huggingface/transformers/8a0cc1df8fdcf3fba5608bcd23344e15a1a41fff3b41ead3ff1e9fe22a0e78c0.a42adf6aa236918d3389c3e8eceaad08f1354410aa9f353cb469fd0472e6e426\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/8a0cc1df8fdcf3fba5608bcd23344e15a1a41fff3b41ead3ff1e9fe22a0e78c0.a42adf6aa236918d3389c3e8eceaad08f1354410aa9f353cb469fd0472e6e426\n",
      "loading weights file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/8a0cc1df8fdcf3fba5608bcd23344e15a1a41fff3b41ead3ff1e9fe22a0e78c0.a42adf6aa236918d3389c3e8eceaad08f1354410aa9f353cb469fd0472e6e426\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-finetuned-sentiment were not used when initializing ElectraForSequenceClassification: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-finetuned-sentiment and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# monologg/koelectra-base-v3-discriminator\n",
    "# huggingface_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "# huggingface_model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels = 2)\n",
    "\n",
    "# monologg/koelectra-base-finetuned-sentiment\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\")\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\", num_labels = 2)\n",
    "\n",
    "# monologg/koelectra-base-discriminator\n",
    "# huggingface_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "# huggingface_model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-discriminator\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca089280",
   "metadata": {},
   "source": [
    "### monologg/koelectra-base-v3 토크나이져 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45ed11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        #max_length=41, # nsmc 데이터 93%가 lenth 41이내\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b2b98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ec3fc9092c4443ac297d29d9eeef4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f69809c2d14f3aa491ec025f80668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = huggingface_nsmc_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e5c36",
   "metadata": {},
   "source": [
    "### validation set 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d64639fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Dateset split 적용\n",
    "spilt_dataset = hf_dataset['train'].train_test_split(test_size=0.2)\n",
    "spilt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80dadf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 120000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 30000\n",
       " }))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. spilt한 데이터 저장\n",
    "train_dataset, validation_dataset = spilt_dataset['train'], spilt_dataset['test']\n",
    "train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b66916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train & validation & test split.\n",
    "hf_train_dataset = train_dataset.shuffle(seed=42).select(range(5000))\n",
    "hf_val_dataset = validation_dataset.shuffle(seed=42).select(range(1000))\n",
    "hf_test_dataset = hf_dataset['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47a98114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "     num_rows: 1000\n",
       " }))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_dataset, hf_val_dataset, hf_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423d375",
   "metadata": {},
   "source": [
    "### Trainer를 활용한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7adc55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/Natural_Language_Processing/10_huggingface_project/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                            # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           # evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                  # learning_rate\n",
    "    per_device_train_batch_size = 8,       # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,        # evaluation 시에 batch size\n",
    "    num_train_epochs = 1,                  # train 시킬 총 epochs\n",
    "    weight_decay = 0.1,                    # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff6ebb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue/mrpc의 acc, f1 지표 활용\n",
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf308c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 08:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.090472</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.980119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.12469301452636719, metrics={'train_runtime': 533.7338, 'train_samples_per_second': 9.368, 'train_steps_per_second': 1.171, 'total_flos': 1315555276800000.0, 'train_loss': 0.12469301452636719, 'epoch': 1.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4578a",
   "metadata": {},
   "source": [
    "### Test Set 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3eb95783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5503530502319336,\n",
       " 'eval_accuracy': 0.898,\n",
       " 'eval_f1': 0.8969696969696969,\n",
       " 'eval_runtime': 36.7177,\n",
       " 'eval_samples_per_second': 27.235,\n",
       " 'eval_steps_per_second': 3.404,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c54af8",
   "metadata": {},
   "source": [
    "### 결과\n",
    "1. 'monologg/koelectra-base-finetuned-sentiment' pre-trained model을 활용했을 때 validation accuracy 0.98, test accuracy 0.898로 가장 양호한 결과를 보였다.\n",
    "- 모델 출처 : https://huggingface.co/monologg/koelectra-base-finetuned-sentiment\n",
    "2. padding은 'max_len'로 변경하여 수행하였고, weight_decay는 0.1로 설정하였다.\n",
    "3. learning rate scheduler는 linear, cosine, polynomial을 각각 시도했지만 학습결과가 불안정하게 나타나면서 validation loss는 지속적으로 증가하였다. 기본값으로 학습하는 것이 가장 양호하였다.\n",
    "4. batch size는 'out of memory' 문제로 인하여 8로 설정하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e6069",
   "metadata": {},
   "source": [
    "# STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8bda172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5435280799865723,\n",
       " 'eval_accuracy': 0.898,\n",
       " 'eval_f1': 0.89738430583501,\n",
       " 'eval_runtime': 37.0446,\n",
       " 'eval_samples_per_second': 26.994,\n",
       " 'eval_steps_per_second': 3.374,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd0a9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TrainingArguments, TFTrainer\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer, max_length=None, padding=True, pad_to_multiple_of=None, bucketing=True):\n",
    "        super().__init__(\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            pad_to_multiple_of=pad_to_multiple_of\n",
    "        )\n",
    "        self.bucketing = bucketing\n",
    "\n",
    "    def collate_batch(self, features):\n",
    "        if self.bucketing:\n",
    "            # 버킷 내에서 가장 긴 시퀀스의 길이를 찾습니다.\n",
    "            max_length = max(len(feature[\"input_ids\"]) for feature in features)\n",
    "            # 버킷을 만듭니다. 길이가 10부터 110까지 10씩 증가하는 버킷을 만듭니다.\n",
    "            buckets = {length: [] for length in range(10, 111, 10)}\n",
    "            for feature in features:\n",
    "                input_ids = feature[\"input_ids\"]\n",
    "                for bucket_size in buckets.keys():\n",
    "                    if len(input_ids) <= bucket_size:\n",
    "                        buckets[bucket_size].append(feature)\n",
    "                        break\n",
    "            \n",
    "            batch = []\n",
    "            # 각 버킷에 대해 동적 패딩을 적용합니다.\n",
    "            for bucket_size, bucket in buckets.items():\n",
    "                if not bucket:\n",
    "                    continue\n",
    "                batch += self.dynamic_padding(bucket, max_length)\n",
    "        else:\n",
    "            batch = self.dynamic_padding(features, self.max_length)\n",
    "        \n",
    "        return tf.data.Dataset.from_tensor_slices(batch)\n",
    "\n",
    "    def dynamic_padding(self, features, max_length):\n",
    "        # 가장 긴 시퀀스의 길이에 따라 패딩합니다.\n",
    "        if self.padding:\n",
    "            max_length = min(max_length, self.max_length)\n",
    "            for feature in features:\n",
    "                padding_length = max_length - len(feature[\"input_ids\"])\n",
    "                feature[\"input_ids\"] = feature[\"input_ids\"] + [self.tokenizer.pad_token_id] * padding_length\n",
    "                feature[\"attention_mask\"] = feature[\"attention_mask\"] + [0] * padding_length\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48edd5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/8a0cc1df8fdcf3fba5608bcd23344e15a1a41fff3b41ead3ff1e9fe22a0e78c0.a42adf6aa236918d3389c3e8eceaad08f1354410aa9f353cb469fd0472e6e426\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-finetuned-sentiment were not used when initializing ElectraForSequenceClassification: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-finetuned-sentiment and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/bfe61bb60a4fddad9575ec468f02c708afd9458e51599f876b2164ae6870f128.42b747a439e42193b1b54614a2e0c4608d8ec136b7405c799d29f667ccdbb27d\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/6c420e1e4e8a4bb8cef587982266920738d38d77d6210cc0fa20f4e50e24f04c.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-f86dd97f46b54d91.arrow\n",
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-59c171f980671803.arrow\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 08:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.523911</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.904137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.184204931640625, metrics={'train_runtime': 534.4432, 'train_samples_per_second': 9.356, 'train_steps_per_second': 1.169, 'total_flos': 1315555276800000.0, 'train_loss': 0.184204931640625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFTrainer\n",
    "\n",
    "huggingface_model_2 = AutoModelForSequenceClassification.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\", num_labels = 2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\")\n",
    "\n",
    "data_collator = CustomDataCollator(tokenizer=tokenizer, bucketing=True)\n",
    "\n",
    "# TrainingArguments 설정\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay = 0.1,               \n",
    ")\n",
    "\n",
    "#dataset = data_collator.collate_batch(hf_dataset['train'].shuffle(seed=42).select(range(6000)))\n",
    "\n",
    "# 트레이너 초기화\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model_2,\n",
    "    args=training_arguments,\n",
    "    train_dataset=hf_dataset['train'].shuffle(seed=42).select(range(5000)),\n",
    "    eval_dataset=hf_dataset['test'].shuffle(seed=42).select(range(1000)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bcd73",
   "metadata": {},
   "source": [
    "## Bucketing과 Dynamic Padding에 대한 설명\n",
    "#### Bucketing\n",
    "1. Bucketing은 주어진 데이터의 시퀀스 길이에 따라 데이터를 그룹화하는 것이다. NLP모델은 보통 고정된 크기의 입력 데이터를 받지만, 실제 데이터는 길이가 다양하다. 이러한 상황에서 길이가 다른 데이터를 처리하기 위해 사용하는 방법이 Bucketing이다.\n",
    "\n",
    "2. Bucketing 방식을 예로 들면, 문장의 길이가 10 이하인 데이터를 하나의 그룹에 할당하고, 길이가 10 ~ 20 인 데이터를 다른 그룹에 할당하는 것이다.\n",
    "\n",
    "3. 이러한 Bucketing 방법으로 배치를 만들 때 padding을 최소화하고 메모리를 효율적으로 사용할 수 있고 학습 속도를 향상시킨다.\n",
    "\n",
    "#### Dynamic Padding\n",
    "1. Dynamic Padding은 각 배치에 포함된 시퀀스들의 길이에 따라 실제로 필요한 패딩의 양을 조절하는 것을 의미한다. Dynamic Padding을 사용하면 각 배치의 시퀀스 길이에 따라 실제로 필요한 패딩의 양이 달라진다. \n",
    "\n",
    "2. Dynamic Padding은 길이가 가장 긴 시퀀스에 맞춰 다른 시퀀스들을 패딩하는 것이 아니라, 각 배치에 있는 시퀀스 중 가장 길이가 긴 시퀀스의 길이만큼만 패딩을 적용한다. \n",
    "\n",
    "3. 이렇게 함으로써, 각 시퀀스의 실제 길이만큼만 연상을 수행하게 되어 계산 효율을 높일 수 있다.\n",
    "\n",
    "Bucketing과 Dynamic Padding은 모두 자연어 처리 모델의 성능과 효율성을 향상시키는 데 도움이 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b5c23",
   "metadata": {},
   "source": [
    "## Step 4. 와 Step 5. 결과 비교\n",
    "1. 시간 단축 - step4 에서 모델 학습에 걸린 시간은 08분 52초인 반면, Bucketing과 Dynamic Padding을 적용했을 때는 약 8분 17초가 소요되었다. Bucketing과 Dynamic Padding을 적용했을 때 시간 단축의 효과가 있는 것으로 보인다.\n",
    "\n",
    "2. 성능 - 성능면에서는 step4의 validation이 0.98로, step5의 0.9보다 높게 나왔다. 시간 및 메모리 제한으로 인해 epoch을 1로 설정했기 때문에 epoch 수를 늘린 추가적인 실험을 시도해서 결과를 확인해야 할 필요가 있다. \n",
    "\n",
    "결과적으로, Bucketing task를 수행할 때 연산 속도와 모델 성능 간의 trade-off 관계가 있다면, 위 비교결과가 적절하게 나온 것으로 생각할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440de24a",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 부족했던 점, 추가적으로 시도 해볼만 한 것들\n",
    "1. 'CUDA out of memory' 문제와 시간의 제한으로 인해 데이터와 모델, 하이퍼파라미터를 최소화할 수밖에 없었다. 만일 충분한 컴퓨팅 자원이 갖춰져 있다면, 샘플의 크기와 다양한 하이퍼파라미터 튜닝을 통해 일반적인 성능결과를 비교대조하면서 확인해 보고 싶다.\n",
    "\n",
    "- 문제해결\n",
    "1. huggingface의 다양한 라이브러리를 이번 프로젝트를 통해 처음 접했다. 'Dataset' 타입으로 생성된 데이터셋은 지금까지 다뤘던 csv 파일을 전처리하던 것과 같이 handling할 수 없었다. 단시간에 huggingface의 라이브러리를 세부적이게 구현하는 것은 분명 어려움이 있었다.\n",
    "\n",
    "2. 따라서 fine-tuning을 통한 성능 향상에 어려움이 있었다. csv파일과 pandas 도구를 활용한 한국어 전처리를 수행하지 못하기 때문에 TrainingArguments, Trainer의 파라미터를 조정하면서 많은 시도를 해보았다. 결론부터 말하면 기존 klue base 모델로는 validation accuracy 0.888을 넘지 못했다. 추가적으로 조사해본 결과, 기존 존재하던 대부분의 pre-trained model도 89%를 거의 대부분 넘지 못했고(nsmc 태스크), 일부 Hanbret, koelectra 모델만 90%를 살짝 넘은 leaderboard를 확인하였다.\n",
    "\n",
    "3. 결과적으로 모델을 'monologg/koelectra-base-finetuned-sentiment' pre-traind 모델로 바꿨을 때, nsmc 데이터 이진분류 태스크에서 validation accuracy가 90% 넘는 결과를 확인할 수 있었다.\n",
    "\n",
    "- 아쉬운 점\n",
    "1. Bucketing과 Dynamic Padding에 대해 설명된 학습자료가 없는 것이 아쉬웠다. 공식문서에서는 구체적인 설명을 찾을 수 없었다. 구글링과 ChatGPT를 활용해서 어찌됬든 코드를 구현하긴 했지만 제대로 된 것인지 사실 확인할 길이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bd40372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/bfe61bb60a4fddad9575ec468f02c708afd9458e51599f876b2164ae6870f128.42b747a439e42193b1b54614a2e0c4608d8ec136b7405c799d29f667ccdbb27d\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/6c420e1e4e8a4bb8cef587982266920738d38d77d6210cc0fa20f4e50e24f04c.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a0a010ec774eac100a824caba3e4730b7d8d74824d5fd9a63fbf3e910354d3f8.235694219f1bd11aa2574b58d596fa99ecdcab54e7cd4eb291e2186b33bd124b\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-base-finetuned-sentiment/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/8a0cc1df8fdcf3fba5608bcd23344e15a1a41fff3b41ead3ff1e9fe22a0e78c0.a42adf6aa236918d3389c3e8eceaad08f1354410aa9f353cb469fd0472e6e426\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-finetuned-sentiment were not used when initializing ElectraForSequenceClassification: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-finetuned-sentiment and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 08:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.097968</td>\n",
       "      <td>0.979000</td>\n",
       "      <td>0.979146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/Natural_Language_Processing/10_huggingface_project/transformers/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.13212779541015626, metrics={'train_runtime': 533.4271, 'train_samples_per_second': 9.373, 'train_steps_per_second': 1.172, 'total_flos': 1315555276800000.0, 'train_loss': 0.13212779541015626, 'epoch': 1.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bucketing과 Dynamic Padding 구현 코드 2\n",
    "\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\")\n",
    "# huggingface_model_2 = AutoModelForSequenceClassification.from_pretrained(\"monologg/koelectra-base-finetuned-sentiment\", num_labels = 2)\n",
    "\n",
    "# # 데이터를 토큰화하고 패딩을 추가하는 DataCollatorWithPadding 인스턴스 생성\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# training_arguments = TrainingArguments(\n",
    "#     output_dir,                            # output이 저장될 경로\n",
    "#     evaluation_strategy=\"epoch\",           # evaluation하는 빈도\n",
    "#     learning_rate = 2e-5,                  # learning_rate\n",
    "#     per_device_train_batch_size = 8,       # 각 device 당 batch size\n",
    "#     per_device_eval_batch_size = 8,        # evaluation 시에 batch size\n",
    "#     num_train_epochs = 1,                  # train 시킬 총 epochs\n",
    "#     weight_decay = 0.1,                    # weight decay\n",
    "#     group_by_length=True\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=huggingface_model_2,           # 학습시킬 model\n",
    "#     args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "#     train_dataset=hf_dataset['train'].shuffle(seed=42).select(range(5000)),\n",
    "#     eval_dataset=hf_dataset['test'].shuffle(seed=42).select(range(1000)),\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=data_collator,       # 데이터 콜래이터 설정\n",
    "#     #tokenizer=tokenizer\n",
    "# )\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
